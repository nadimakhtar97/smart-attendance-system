{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n// inspired by https://github.com/maxogden/filereader-stream\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { ByteChunkIterator } from './byte_chunk_iterator';\n/**\n * Provide a stream of chunks from a File, Blob, or Uint8Array.\n * @param file The source File, Blob or Uint8Array.\n * @param options Optional settings controlling file reading.\n * @returns a lazy Iterator of Uint8Arrays containing sequential chunks of the\n *   input File, Blob or Uint8Array.\n */\n\nexport class FileChunkIterator extends ByteChunkIterator {\n  constructor(file) {\n    let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    super();\n    this.file = file;\n    this.options = options;\n    util.assert(file instanceof Uint8Array || (env().get('IS_BROWSER') ? file instanceof File || file instanceof Blob : false), () => 'FileChunkIterator only supports File, Blob and Uint8Array ' + 'right now.');\n    this.offset = options.offset || 0; // default 1MB chunk has tolerable perf on large files\n\n    this.chunkSize = options.chunkSize || 1024 * 1024;\n  }\n\n  summary() {\n    return `FileChunks ${this.file}`;\n  }\n\n  async next() {\n    if (this.offset >= (this.file instanceof Uint8Array ? this.file.byteLength : this.file.size)) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n\n    const chunk = new Promise((resolve, reject) => {\n      const end = this.offset + this.chunkSize;\n\n      if (this.file instanceof Uint8Array) {\n        // Note if end > this.uint8Array.byteLength, we just get a small last\n        // chunk.\n        resolve(new Uint8Array(this.file.slice(this.offset, end)));\n      } else {\n        // This branch assumes that this.file type is File or Blob, which\n        // means it is in the browser environment.\n        // TODO(soergel): is this a performance issue?\n        const fileReader = new FileReader();\n\n        fileReader.onload = event => {\n          let data = fileReader.result; // Not sure we can trust the return type of\n          // FileReader.readAsArrayBuffer See e.g.\n          // https://github.com/node-file-api/FileReader/issues/2\n\n          if (data instanceof ArrayBuffer) {\n            data = new Uint8Array(data);\n          }\n\n          if (!(data instanceof Uint8Array)) {\n            return reject(new TypeError('FileReader returned unknown type.'));\n          }\n\n          resolve(data);\n        };\n\n        fileReader.onabort = event => {\n          return reject(new Error('Aborted'));\n        };\n\n        fileReader.onerror = event => {\n          return reject(new Error(event.type));\n        }; // TODO(soergel): better handle onabort, onerror\n        // Note if end > this.file.size, we just get a small last chunk.\n\n\n        const slice = this.file.slice(this.offset, end); // We can't use readAsText here (even if we know the file is text)\n        // because the slice boundary may fall within a multi-byte character.\n\n        fileReader.readAsArrayBuffer(slice);\n      }\n\n      this.offset = end;\n    });\n    return {\n      value: await chunk,\n      done: false\n    };\n  }\n\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;;AAkBA;AACA,SAAQA,GAAR,EAAaC,IAAb,QAAwB,uBAAxB;AAEA,SAAQC,iBAAR,QAAgC,uBAAhC;AASA;;;;;;;;AAOA,OAAM,MAAOC,iBAAP,SAAiCD,iBAAjC,CAAkD;AAItDE,cACcC,IADd,EAEoD;AAAA,QAAtCC,OAAsC,uEAAF,EAAE;AAClD;AAFY;AACA;AAEZL,QAAI,CAACM,MAAL,CACKF,IAAI,YAAYG,UAAjB,KACKR,GAAG,GAAGS,GAAN,CAAU,YAAV,IACKJ,IAAI,YAAYK,IAAhB,IAAwBL,IAAI,YAAYM,IAD7C,GAEI,KAHT,CADJ,EAKI,MAAM,+DACF,YANR;AAOA,SAAKC,MAAL,GAAcN,OAAO,CAACM,MAAR,IAAkB,CAAhC,CATkD,CAUlD;;AACA,SAAKC,SAAL,GAAiBP,OAAO,CAACO,SAAR,IAAqB,OAAO,IAA7C;AACD;;AAEDC,SAAO;AACL,WAAO,cAAc,KAAKT,IAAI,EAA9B;AACD;;AAES,QAAJU,IAAI;AACR,QAAI,KAAKH,MAAL,KAAiB,KAAKP,IAAL,YAAqBG,UAAtB,GACI,KAAKH,IAAL,CAAUW,UADd,GAEI,KAAKX,IAAL,CAAUY,IAF9B,CAAJ,EAEyC;AACvC,aAAO;AAACC,aAAK,EAAE,IAAR;AAAcC,YAAI,EAAE;AAApB,OAAP;AACD;;AACD,UAAMC,KAAK,GAAG,IAAIC,OAAJ,CAAwB,CAACC,OAAD,EAAUC,MAAV,KAAoB;AACxD,YAAMC,GAAG,GAAG,KAAKZ,MAAL,GAAc,KAAKC,SAA/B;;AACA,UAAI,KAAKR,IAAL,YAAqBG,UAAzB,EAAqC;AACnC;AACA;AACAc,eAAO,CAAC,IAAId,UAAJ,CAAe,KAAKH,IAAL,CAAUoB,KAAV,CAAgB,KAAKb,MAArB,EAA6BY,GAA7B,CAAf,CAAD,CAAP;AACD,OAJD,MAIO;AACL;AACA;AAEA;AACA,cAAME,UAAU,GAAG,IAAIC,UAAJ,EAAnB;;AACAD,kBAAU,CAACE,MAAX,GAAqBC,KAAD,IAAU;AAC5B,cAAIC,IAAI,GAAkCJ,UAAU,CAACK,MAArD,CAD4B,CAE5B;AACA;AACA;;AACA,cAAID,IAAI,YAAYE,WAApB,EAAiC;AAC/BF,gBAAI,GAAG,IAAItB,UAAJ,CAAesB,IAAf,CAAP;AACD;;AACD,cAAI,EAAEA,IAAI,YAAYtB,UAAlB,CAAJ,EAAmC;AACjC,mBAAOe,MAAM,CAAC,IAAIU,SAAJ,CAAc,mCAAd,CAAD,CAAb;AACD;;AACDX,iBAAO,CAACQ,IAAD,CAAP;AACD,SAZD;;AAaAJ,kBAAU,CAACQ,OAAX,GAAsBL,KAAD,IAAU;AAC7B,iBAAON,MAAM,CAAC,IAAIY,KAAJ,CAAU,SAAV,CAAD,CAAb;AACD,SAFD;;AAGAT,kBAAU,CAACU,OAAX,GAAsBP,KAAD,IAAU;AAC7B,iBAAON,MAAM,CAAC,IAAIY,KAAJ,CAAUN,KAAK,CAACQ,IAAhB,CAAD,CAAb;AACD,SAFD,CAtBK,CAyBL;AACA;;;AACA,cAAMZ,KAAK,GAAG,KAAKpB,IAAL,CAAUoB,KAAV,CAAgB,KAAKb,MAArB,EAA6BY,GAA7B,CAAd,CA3BK,CA4BL;AACA;;AACAE,kBAAU,CAACY,iBAAX,CAA6Bb,KAA7B;AACD;;AACD,WAAKb,MAAL,GAAcY,GAAd;AACD,KAvCa,CAAd;AAwCA,WAAO;AAACN,WAAK,EAAG,MAAME,KAAf;AAAuBD,UAAI,EAAE;AAA7B,KAAP;AACD;;AAvEqD","names":["env","util","ByteChunkIterator","FileChunkIterator","constructor","file","options","assert","Uint8Array","get","File","Blob","offset","chunkSize","summary","next","byteLength","size","value","done","chunk","Promise","resolve","reject","end","slice","fileReader","FileReader","onload","event","data","result","ArrayBuffer","TypeError","onabort","Error","onerror","type","readAsArrayBuffer"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-data/src/iterators/file_chunk_iterator.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\n// inspired by https://github.com/maxogden/filereader-stream\nimport {env, util} from '@tensorflow/tfjs-core';\nimport {FileElement} from '../types';\nimport {ByteChunkIterator} from './byte_chunk_iterator';\n\nexport interface FileChunkIteratorOptions {\n  /** The byte offset at which to begin reading the File or Blob. Default 0. */\n  offset?: number;\n  /** The number of bytes to read at a time. Default 1MB. */\n  chunkSize?: number;\n}\n\n/**\n * Provide a stream of chunks from a File, Blob, or Uint8Array.\n * @param file The source File, Blob or Uint8Array.\n * @param options Optional settings controlling file reading.\n * @returns a lazy Iterator of Uint8Arrays containing sequential chunks of the\n *   input File, Blob or Uint8Array.\n */\nexport class FileChunkIterator extends ByteChunkIterator {\n  offset: number;\n  chunkSize: number;\n\n  constructor(\n      protected file: FileElement,\n      protected options: FileChunkIteratorOptions = {}) {\n    super();\n    util.assert(\n        (file instanceof Uint8Array) ||\n            (env().get('IS_BROWSER') ?\n                 (file instanceof File || file instanceof Blob) :\n                 false),\n        () => 'FileChunkIterator only supports File, Blob and Uint8Array ' +\n            'right now.');\n    this.offset = options.offset || 0;\n    // default 1MB chunk has tolerable perf on large files\n    this.chunkSize = options.chunkSize || 1024 * 1024;\n  }\n\n  summary() {\n    return `FileChunks ${this.file}`;\n  }\n\n  async next(): Promise<IteratorResult<Uint8Array>> {\n    if (this.offset >= ((this.file instanceof Uint8Array) ?\n                            this.file.byteLength :\n                            this.file.size)) {\n      return {value: null, done: true};\n    }\n    const chunk = new Promise<Uint8Array>((resolve, reject) => {\n      const end = this.offset + this.chunkSize;\n      if (this.file instanceof Uint8Array) {\n        // Note if end > this.uint8Array.byteLength, we just get a small last\n        // chunk.\n        resolve(new Uint8Array(this.file.slice(this.offset, end)));\n      } else {\n        // This branch assumes that this.file type is File or Blob, which\n        // means it is in the browser environment.\n\n        // TODO(soergel): is this a performance issue?\n        const fileReader = new FileReader();\n        fileReader.onload = (event) => {\n          let data: string|ArrayBuffer|Uint8Array = fileReader.result;\n          // Not sure we can trust the return type of\n          // FileReader.readAsArrayBuffer See e.g.\n          // https://github.com/node-file-api/FileReader/issues/2\n          if (data instanceof ArrayBuffer) {\n            data = new Uint8Array(data);\n          }\n          if (!(data instanceof Uint8Array)) {\n            return reject(new TypeError('FileReader returned unknown type.'));\n          }\n          resolve(data);\n        };\n        fileReader.onabort = (event) => {\n          return reject(new Error('Aborted'));\n        };\n        fileReader.onerror = (event) => {\n          return reject(new Error(event.type));\n        };\n        // TODO(soergel): better handle onabort, onerror\n        // Note if end > this.file.size, we just get a small last chunk.\n        const slice = this.file.slice(this.offset, end);\n        // We can't use readAsText here (even if we know the file is text)\n        // because the slice boundary may fall within a multi-byte character.\n        fileReader.readAsArrayBuffer(slice);\n      }\n      this.offset = end;\n    });\n    return {value: (await chunk), done: false};\n  }\n}\n"]},"metadata":{},"sourceType":"module"}