{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\n\nfunction assertFeedCompatibility(key, val) {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(`The dtype of the feed (${val.dtype}) can not be cast to the dtype ` + `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\n\n\nexport class FeedDict {\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds) {\n    this.id2Value = {};\n    this.id2Mask = {};\n    this.name2Id = {};\n\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n\n\n  add(key, value, mask) {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n\n    return this;\n  }\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n\n\n  addFeed(feed) {\n    this.add(feed.key, feed.value);\n  }\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n\n\n  hasKey(key) {\n    return this.id2Value[key.id] != null;\n  }\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n\n\n  names() {\n    return Object.keys(this.name2Id);\n  }\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n\n\n  getValue(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n\n      return this.id2Value[id];\n    }\n  }\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n\n\n  getMask(key) {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n\n      return this.id2Mask[id];\n    }\n  }\n  /** Dispose all mask Tensors held by this object. */\n\n\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n\n} // Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\n\nconst cachedSorted = {}; // Cache for recipient count maps for given execution targets (i.e., fetches).\n\nconst cachedRecipientCounts = {};\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\n\nexport function execute(fetches, feedDict, kwargs, probe) {\n  const training = kwargs == null ? false : kwargs['training'];\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray = arrayFetches ? fetches : [fetches];\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs = [];\n  const feedNames = feedDict.names();\n\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  } // Check cache.\n\n\n  const fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().join(',');\n  let sorted;\n  let recipientCounts;\n\n  if (cachedSorted[fetchAndFeedKey] == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts; // Store results in cache for future use.\n\n    cachedSorted[fetchAndFeedKey] = sorted;\n    cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n  }\n\n  sorted = cachedSorted[fetchAndFeedKey];\n  recipientCounts = {};\n\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n  }\n\n  const internalFeedDict = new FeedDict(feedDict); // Start iterative execution on the topologically-sorted SymbolicTensors.\n\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n\n    const inputValues = [];\n    const inputMasks = [];\n    const tensorsToDispose = [];\n    let maskExists = false;\n\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n\n      if (mask != null) {\n        maskExists = true;\n      }\n\n      if (!training) {\n        recipientCounts[input.name]--;\n\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) && outputNames.indexOf(input.name) === -1 && !value.isDisposed && input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n\n    const outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n    let outputMask = null;\n\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(outputSymbolicTensors[i], outputTensors[i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  } // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n\n\n  internalFeedDict.disposeMasks();\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\n\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n  util.assert(fetches != null && fetches.length > 0, () => `Expected at least one fetch, got none`);\n  let finalSorted = [];\n  let finalRecipientMap = {};\n\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set();\n\n    for (const fetch of fetches) {\n      const {\n        sorted,\n        recipientMap\n      } = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict); // Merge sorted SymbolicTensor Arrays.\n\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      } // Merge recipient maps.\n\n\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set();\n        }\n\n        recipientMap[name].forEach(recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap) {\n  const recipientCounts = {};\n\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n\n  return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\n\n\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n  const visited = new Set();\n  const sorted = [];\n  const recipientMap = {}; // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n\n  const stack = [];\n  const marks = []; // Initial population of stack and marks.\n\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set();\n        }\n\n        recipientMap[input.name].add(top.name);\n\n        if (visited.has(input.name)) {\n          continue; // Avoid repeated visits to the same SymbolicTensor.\n        }\n\n        stack.push(input);\n      }\n    }\n  }\n\n  return {\n    sorted,\n    recipientMap\n  };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\n\nfunction getNodeOutputs(fetch) {\n  let layerOutputs;\n\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex = null;\n\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i].outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n\n  return layerOutputs;\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,SAAQA,IAAR,EAAcC,OAAd,EAAuBC,MAAvB,EAAuCC,IAAvC,QAAkD,uBAAlD;AAEA,SAAQC,UAAR,QAAyB,WAAzB;AAEA,SAAQC,MAAR,QAAqB,wBAArB;AAEA,SAAQC,UAAR,QAAyB,eAAzB;AACA,SAAQC,cAAR,QAA6B,YAA7B;AAEA;;;;AAGA,SAASC,uBAAT,CAAiCC,GAAjC,EAAsDC,GAAtD,EAAiE;AAC/D;AACA,MAAID,GAAG,CAACE,KAAJ,IAAa,IAAb,IAAqBF,GAAG,CAACE,KAAJ,KAAcD,GAAG,CAACC,KAA3C,EAAkD;AAChD;AACA,WAAOD,GAAP;AACD;;AACD,MAAI;AACF;AACA,WAAOV,IAAI,CAACU,GAAD,EAAMD,GAAG,CAACE,KAAV,CAAX;AACD,GAHD,CAGE,OAAOC,GAAP,EAAY;AACZ;AACA,UAAM,IAAIR,UAAJ,CACF,0BAA0BM,GAAG,CAACC,KAAK,iCAAnC,GACA,eAAeF,GAAG,CAACI,IAAI,MAAMJ,GAAG,CAACE,KAAK,IAFpC,CAAN;AAGD;AACF;AAUD;;;;;;AAIA,OAAM,MAAOG,QAAP,CAAe;AAKnB;;;;;AAKAC,cAAYC,KAAZ,EAAmC;AAT3B,oBAAmC,EAAnC;AACA,mBAAkC,EAAlC;AACA,mBAAoC,EAApC;;AAQN,QAAIA,KAAK,YAAYF,QAArB,EAA+B;AAC7B,WAAK,MAAMG,EAAX,IAAiBD,KAAK,CAACE,QAAvB,EAAiC;AAC/B,aAAKA,QAAL,CAAcD,EAAd,IAAoBD,KAAK,CAACE,QAAN,CAAeD,EAAf,CAApB;;AACA,YAAIA,EAAE,IAAID,KAAK,CAACG,OAAhB,EAAyB;AACvB,eAAKA,OAAL,CAAaF,EAAb,IAAmBD,KAAK,CAACG,OAAN,CAAcF,EAAd,CAAnB;AACD;AACF;AACF,KAPD,MAOO;AACL,UAAID,KAAK,IAAI,IAAb,EAAmB;AACjB;AACD;;AACD,WAAK,MAAMI,IAAX,IAAmBJ,KAAnB,EAA0B;AACxB,aAAKK,GAAL,CAASD,IAAI,CAACX,GAAd,EAAmBW,IAAI,CAACE,KAAxB;AACD;AACF;AACF;AAED;;;;;;;;;;;;AAUAD,KAAG,CAACZ,GAAD,EAAsBa,KAAtB,EAAqCC,IAArC,EAAkD;AACnD,QAAI,KAAKL,QAAL,CAAcT,GAAG,CAACQ,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,WAAKC,QAAL,CAAcT,GAAG,CAACQ,EAAlB,IAAwBT,uBAAuB,CAACC,GAAD,EAAMa,KAAN,CAA/C;AACA,WAAKE,OAAL,CAAaf,GAAG,CAACI,IAAjB,IAAyBJ,GAAG,CAACQ,EAA7B;;AACA,UAAIM,IAAI,IAAI,IAAZ,EAAkB;AAChB,aAAKJ,OAAL,CAAaV,GAAG,CAACQ,EAAjB,IAAuBM,IAAvB;AACD;AACF,KAND,MAMO;AACL,YAAM,IAAInB,UAAJ,CAAe,uBAAuBK,GAAG,CAACI,IAAI,QAAQJ,GAAG,CAACQ,EAAE,EAA5D,CAAN;AACD;;AACD,WAAO,IAAP;AACD;AAED;;;;;;;AAKAQ,SAAO,CAACL,IAAD,EAAW;AAChB,SAAKC,GAAL,CAASD,IAAI,CAACX,GAAd,EAAmBW,IAAI,CAACE,KAAxB;AACD;AAED;;;;;;AAIAI,QAAM,CAACjB,GAAD,EAAoB;AACxB,WAAO,KAAKS,QAAL,CAAcT,GAAG,CAACQ,EAAlB,KAAyB,IAAhC;AACD;AAED;;;;;AAGAU,OAAK;AACH,WAAOC,MAAM,CAACC,IAAP,CAAY,KAAKL,OAAjB,CAAP;AACD;AAED;;;;;;;;;AAOAM,UAAQ,CAACrB,GAAD,EAA2B;AACjC,QAAIA,GAAG,YAAYF,cAAnB,EAAmC;AACjC,UAAI,KAAKW,QAAL,CAAcT,GAAG,CAACQ,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,cAAM,IAAIb,UAAJ,CAAe,oBAAoBK,GAAG,CAACI,IAAI,EAA3C,CAAN;AACD,OAFD,MAEO;AACL,eAAO,KAAKK,QAAL,CAAcT,GAAG,CAACQ,EAAlB,CAAP;AACD;AACF,KAND,MAMO;AACL,YAAMA,EAAE,GAAG,KAAKO,OAAL,CAAaf,GAAb,CAAX;;AACA,UAAIQ,EAAE,IAAI,IAAV,EAAgB;AACd,cAAM,IAAIb,UAAJ,CAAe,yCAAyCK,GAAG,EAA3D,CAAN;AACD;;AACD,aAAO,KAAKS,QAAL,CAAcD,EAAd,CAAP;AACD;AACF;AAED;;;;;;;;;AAOAc,SAAO,CAACtB,GAAD,EAA2B;AAChC,QAAIA,GAAG,YAAYF,cAAnB,EAAmC;AACjC,UAAI,KAAKW,QAAL,CAAcT,GAAG,CAACQ,EAAlB,KAAyB,IAA7B,EAAmC;AACjC,cAAM,IAAIb,UAAJ,CAAe,oBAAoBK,GAAG,CAACI,IAAI,EAA3C,CAAN;AACD,OAFD,MAEO;AACL,eAAO,KAAKM,OAAL,CAAaV,GAAG,CAACQ,EAAjB,CAAP;AACD;AACF,KAND,MAMO;AACL,YAAMA,EAAE,GAAG,KAAKO,OAAL,CAAaf,GAAb,CAAX;;AACA,UAAIQ,EAAE,IAAI,IAAV,EAAgB;AACd,cAAM,IAAIb,UAAJ,CAAe,yCAAyCK,GAAG,EAA3D,CAAN;AACD;;AACD,aAAO,KAAKU,OAAL,CAAaF,EAAb,CAAP;AACD;AACF;AAED;;;AACAe,cAAY;AACV,QAAI,KAAKb,OAAL,IAAgB,IAApB,EAA0B;AACxBlB,aAAO,CAAC,KAAKkB,OAAN,CAAP;AACD;AACF;;AA9HkB,C,CAiIrB;AACA;;AACA,MAAMc,YAAY,GAAmD,EAArE,C,CAEA;;AACA,MAAMC,qBAAqB,GACuC,EADlE;AAuBA;;;;;;;;;;;;;;;;;;;;;;AAqBA,OAAM,SAAUC,OAAV,CACFC,OADE,EACwCC,QADxC,EAEFC,MAFE,EAEeC,KAFf,EAEqC;AAEzC,QAAMC,QAAQ,GAAYF,MAAM,IAAI,IAAV,GAAiB,KAAjB,GAAyBA,MAAM,CAAC,UAAD,CAAzD;AAEA,QAAMG,YAAY,GAAGC,KAAK,CAACC,OAAN,CAAcP,OAAd,CAArB;AACA,QAAMQ,UAAU,GACZH,YAAY,GAAGL,OAAH,GAAiC,CAACA,OAAD,CADjD;AAGA,QAAMS,WAAW,GAAGD,UAAU,CAACE,GAAX,CAAeC,CAAC,IAAIA,CAAC,CAAClC,IAAtB,CAApB;AACA,QAAMmC,YAAY,GAAa,EAA/B;AACA,QAAMC,SAAS,GAAGZ,QAAQ,CAACV,KAAT,EAAlB;;AACA,OAAK,MAAMuB,UAAX,IAAyBL,WAAzB,EAAsC;AACpC,QAAII,SAAS,CAACE,OAAV,CAAkBD,UAAlB,MAAkC,CAAC,CAAvC,EAA0C;AACxCF,kBAAY,CAACI,IAAb,CAAkBf,QAAQ,CAACP,QAAT,CAAkBoB,UAAlB,CAAlB;AACD,KAFD,MAEO;AACLF,kBAAY,CAACI,IAAb,CAAkB,IAAlB;AACD;AACF;;AAED,MAAIb,KAAK,IAAI,IAAb,EAAmB;AACjB;AACAA,SAAK,CAACc,aAAN,GAAsB,CAACC,QAAvB;AACAf,SAAK,CAACgB,aAAN,GAAsBD,QAAtB;AACD,GAvBwC,CAyBzC;;;AACA,QAAME,eAAe,GACjBX,WAAW,CAACY,IAAZ,CAAiB,GAAjB,IAAwB,GAAxB,GAA8BpB,QAAQ,CAACV,KAAT,GAAiB8B,IAAjB,CAAsB,GAAtB,CADlC;AAEA,MAAIC,MAAJ;AACA,MAAIC,eAAJ;;AACA,MAAI1B,YAAY,CAACuB,eAAD,CAAZ,IAAiC,IAArC,EAA2C;AACzC;AACA;AACA,UAAMI,GAAG,GAAGC,oCAAoC,CAACjB,UAAD,EAAaP,QAAb,CAAhD;AACAqB,UAAM,GAAGE,GAAG,CAACF,MAAb;AACAC,mBAAe,GAAGC,GAAG,CAACD,eAAtB,CALyC,CAOzC;;AACA1B,gBAAY,CAACuB,eAAD,CAAZ,GAAgCE,MAAhC;AACAxB,yBAAqB,CAACsB,eAAD,CAArB,GAAyCG,eAAzC;AACD;;AACDD,QAAM,GAAGzB,YAAY,CAACuB,eAAD,CAArB;AACAG,iBAAe,GAAG,EAAlB;;AACA,MAAI,CAACnB,QAAL,EAAe;AACbZ,UAAM,CAACkC,MAAP,CAAcH,eAAd,EAA+BzB,qBAAqB,CAACsB,eAAD,CAApD;AACD;;AAED,QAAMO,gBAAgB,GAAG,IAAIjD,QAAJ,CAAauB,QAAb,CAAzB,CA/CyC,CAiDzC;;AACA,OAAK,IAAI2B,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,MAAM,CAACO,MAA3B,EAAmC,EAAED,CAArC,EAAwC;AACtC,QAAIzB,KAAK,IAAI,IAAb,EAAmB;AACjB;AACA,YAAM2B,UAAU,GAAGhE,MAAM,GAAGgE,UAA5B;;AACA,UAAIA,UAAU,GAAG3B,KAAK,CAACc,aAAvB,EAAsC;AACpCd,aAAK,CAACc,aAAN,GAAsBa,UAAtB;AACD;;AACD,UAAIA,UAAU,GAAG3B,KAAK,CAACgB,aAAvB,EAAsC;AACpChB,aAAK,CAACgB,aAAN,GAAsBW,UAAtB;AACD;AACF;;AAED,UAAMC,QAAQ,GAAGT,MAAM,CAACM,CAAD,CAAvB;AACA,UAAMI,QAAQ,GAAGD,QAAQ,CAACE,WAA1B;;AACA,QAAID,QAAQ,YAAY9D,UAAxB,EAAoC;AAClC;AACD;;AACD,UAAMgE,WAAW,GAAa,EAA9B;AACA,UAAMC,UAAU,GAAa,EAA7B;AACA,UAAMC,gBAAgB,GAAa,EAAnC;AAEA,QAAIC,UAAU,GAAG,KAAjB;;AACA,SAAK,MAAMC,KAAX,IAAoBP,QAAQ,CAACQ,MAA7B,EAAqC;AACnC,YAAMrD,KAAK,GAAGyC,gBAAgB,CAACjC,QAAjB,CAA0B4C,KAA1B,CAAd;AACA,YAAMnD,IAAI,GAAGwC,gBAAgB,CAAChC,OAAjB,CAAyB2C,KAAzB,CAAb;AACAJ,iBAAW,CAAClB,IAAZ,CAAiB9B,KAAjB;AACAiD,gBAAU,CAACnB,IAAX,CAAgB7B,IAAhB;;AACA,UAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBkD,kBAAU,GAAG,IAAb;AACD;;AACD,UAAI,CAACjC,QAAL,EAAe;AACbmB,uBAAe,CAACe,KAAK,CAAC7D,IAAP,CAAf;;AACA,YAAI8C,eAAe,CAACe,KAAK,CAAC7D,IAAP,CAAf,KAAgC,CAAhC,IAAqC,CAACwB,QAAQ,CAACX,MAAT,CAAgBgD,KAAhB,CAAtC,IACA7B,WAAW,CAACM,OAAZ,CAAoBuB,KAAK,CAAC7D,IAA1B,MAAoC,CAAC,CADrC,IAC0C,CAACS,KAAK,CAACsD,UADjD,IAEAF,KAAK,CAACL,WAAN,CAAkBQ,QAAlB,KAA+B,IAFnC,EAEyC;AACvCL,0BAAgB,CAACpB,IAAjB,CAAsB9B,KAAtB;AACD;AACF;AACF;;AAED,QAAImD,UAAJ,EAAgB;AACdnC,YAAM,GAAGA,MAAM,IAAI,EAAnB;AACAA,YAAM,CAAC,MAAD,CAAN,GAAiBiC,UAAU,CAAC,CAAD,CAA3B;AACD;;AACD,UAAMO,aAAa,GACfzE,MAAM,CAAC+D,QAAQ,CAACW,KAAT,CAAeT,WAAf,EAA4BhC,MAA5B,CAAD,CADV;AAEA,QAAI0C,UAAU,GAAoB,IAAlC;;AACA,QAAIZ,QAAQ,CAACa,eAAb,EAA8B;AAC5BD,gBAAU,GAAGZ,QAAQ,CAACc,WAAT,CAAqBZ,WAArB,EAAkCC,UAAlC,CAAb;AACD;;AACD,UAAMY,YAAY,GAAGC,cAAc,CAACjB,QAAD,CAAnC;AACA,UAAMkB,qBAAqB,GACvB3C,KAAK,CAACC,OAAN,CAAcwC,YAAd,IAA8BA,YAA9B,GAA6C,CAACA,YAAD,CADjD;;AAEA,SAAK,IAAInB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGqB,qBAAqB,CAACpB,MAA1C,EAAkD,EAAED,CAApD,EAAuD;AACrD,UAAI,CAACD,gBAAgB,CAACrC,MAAjB,CAAwB2D,qBAAqB,CAACrB,CAAD,CAA7C,CAAL,EAAwD;AACtDD,wBAAgB,CAAC1C,GAAjB,CACIgE,qBAAqB,CAACrB,CAAD,CADzB,EAC8Bc,aAAa,CAACd,CAAD,CAD3C,EAEItB,KAAK,CAACC,OAAN,CAAcqC,UAAd,IAA4BA,UAAU,CAAC,CAAD,CAAtC,GAA4CA,UAFhD;AAGD;;AACD,YAAMM,KAAK,GAAGzC,WAAW,CAACM,OAAZ,CAAoBkC,qBAAqB,CAACrB,CAAD,CAArB,CAAyBnD,IAA7C,CAAd;;AACA,UAAIyE,KAAK,KAAK,CAAC,CAAf,EAAkB;AAChBtC,oBAAY,CAACsC,KAAD,CAAZ,GAAsBR,aAAa,CAACd,CAAD,CAAnC;AACD;AACF;;AAED,QAAI,CAACxB,QAAL,EAAe;AACb;AACAvC,aAAO,CAACuE,gBAAD,CAAP;AACD;AACF,GAvHwC,CAwHzC;AACA;AACA;AACA;AACA;;;AACAT,kBAAgB,CAAC/B,YAAjB;AAEA,SAAOS,YAAY,GAAGO,YAAH,GAAkBA,YAAY,CAAC,CAAD,CAAjD;AACD;AAUD;;;;;;;;;;;;AAWA,SAASa,oCAAT,CACIzB,OADJ,EAC+BC,QAD/B,EACiD;AAE/ClC,MAAI,CAACoF,MAAL,CACInD,OAAO,IAAI,IAAX,IAAmBA,OAAO,CAAC6B,MAAR,GAAiB,CADxC,EAEI,MAAM,uCAFV;AAIA,MAAIuB,WAAW,GAAqB,EAApC;AACA,MAAIC,iBAAiB,GAAiB,EAAtC;;AACA,MAAIrD,OAAO,CAAC6B,MAAR,KAAmB,CAAvB,EAA0B;AACxB;AACA,UAAML,GAAG,GACL8B,+CAA+C,CAACtD,OAAO,CAAC,CAAD,CAAR,EAAaC,QAAb,CADnD;AAEAmD,eAAW,GAAG5B,GAAG,CAACF,MAAlB;AACA+B,qBAAiB,GAAG7B,GAAG,CAAC+B,YAAxB;AACD,GAND,MAMO;AACL,UAAMC,OAAO,GAAG,IAAIC,GAAJ,EAAhB;;AACA,SAAK,MAAMC,KAAX,IAAoB1D,OAApB,EAA6B;AAC3B,YAAM;AAACsB,cAAD;AAASiC;AAAT,UACFD,+CAA+C,CAACI,KAAD,EAAQzD,QAAR,CADnD,CAD2B,CAI3B;;AACA,WAAK,MAAM0D,cAAX,IAA6BrC,MAA7B,EAAqC;AACnC,YAAI,CAACkC,OAAO,CAACI,GAAR,CAAYD,cAAc,CAAClF,IAA3B,CAAL,EAAuC;AACrC2E,qBAAW,CAACpC,IAAZ,CAAiB2C,cAAjB;AACAH,iBAAO,CAACvE,GAAR,CAAY0E,cAAc,CAAClF,IAA3B;AACD;AACF,OAV0B,CAY3B;;;AACA,WAAK,MAAMA,IAAX,IAAmB8E,YAAnB,EAAiC;AAC/B,YAAIF,iBAAiB,CAAC5E,IAAD,CAAjB,IAA2B,IAA/B,EAAqC;AACnC4E,2BAAiB,CAAC5E,IAAD,CAAjB,GAA0B,IAAIgF,GAAJ,EAA1B;AACD;;AACDF,oBAAY,CAAC9E,IAAD,CAAZ,CAAmBoF,OAAnB,CACIC,SAAS,IAAIT,iBAAiB,CAAC5E,IAAD,CAAjB,CAAwBQ,GAAxB,CAA4B6E,SAA5B,CADjB;AAED;AACF;AACF;;AACD,SAAO;AACLxC,UAAM,EAAE8B,WADH;AAEL7B,mBAAe,EAAEwC,mBAAmB,CAACV,iBAAD;AAF/B,GAAP;AAID;;AAED,SAASU,mBAAT,CAA6BR,YAA7B,EAAuD;AACrD,QAAMhC,eAAe,GAAoB,EAAzC;;AACA,OAAK,MAAM9C,IAAX,IAAmB8E,YAAnB,EAAiC;AAC/BhC,mBAAe,CAAC9C,IAAD,CAAf,GAAwB8E,YAAY,CAAC9E,IAAD,CAAZ,CAAmBuF,IAA3C;AACD;;AACD,SAAOzC,eAAP;AACD;AAED;;;;;;;;;;;;;AAWA,OAAM,SAAU+B,+CAAV,CACFI,KADE,EACqBzD,QADrB,EACuC;AAE3C,QAAMuD,OAAO,GAAG,IAAIC,GAAJ,EAAhB;AACA,QAAMnC,MAAM,GAAqB,EAAjC;AACA,QAAMiC,YAAY,GAAiB,EAAnC,CAJ2C,CAM3C;AACA;AACA;;AACA,OAAK,MAAMlF,GAAX,IAAkB4B,QAAQ,CAACV,KAAT,EAAlB,EAAoC;AAClCiE,WAAO,CAACvE,GAAR,CAAYZ,GAAZ;AACD;;AAED,QAAM4F,KAAK,GAAqB,EAAhC;AACA,QAAMC,KAAK,GAAa,EAAxB,CAd2C,CAgB3C;;AACAD,OAAK,CAACjD,IAAN,CAAW0C,KAAX;;AAEA,SAAOO,KAAK,CAACpC,MAAN,GAAe,CAAtB,EAAyB;AACvB,UAAMsC,GAAG,GAAGF,KAAK,CAACA,KAAK,CAACpC,MAAN,GAAe,CAAhB,CAAjB;;AACA,QAAI2B,OAAO,CAACI,GAAR,CAAYO,GAAG,CAAC1F,IAAhB,CAAJ,EAA2B;AACzBwF,WAAK,CAACG,GAAN;AACA;AACD;;AACD,UAAMC,WAAW,GAAGH,KAAK,CAACA,KAAK,CAACrC,MAAN,GAAe,CAAhB,CAAL,KAA4BoC,KAAK,CAACpC,MAAN,GAAe,CAA/D;;AACA,QAAIsC,GAAG,CAAC5B,MAAJ,CAAWV,MAAX,KAAsB,CAAtB,IAA2BwC,WAA/B,EAA4C;AAC1C;AACAJ,WAAK,CAACG,GAAN;AACA9C,YAAM,CAACN,IAAP,CAAYmD,GAAZ;AACAX,aAAO,CAACvE,GAAR,CAAYkF,GAAG,CAAC1F,IAAhB;;AACA,UAAI4F,WAAJ,EAAiB;AACfH,aAAK,CAACE,GAAN;AACD;AACF,KARD,MAQO;AACL;AACA;AACAF,WAAK,CAAClD,IAAN,CAAWiD,KAAK,CAACpC,MAAN,GAAe,CAA1B;;AACA,WAAK,MAAMS,KAAX,IAAoB6B,GAAG,CAAC5B,MAAxB,EAAgC;AAC9B;AACA;AACA,YAAIgB,YAAY,CAACjB,KAAK,CAAC7D,IAAP,CAAZ,IAA4B,IAAhC,EAAsC;AACpC8E,sBAAY,CAACjB,KAAK,CAAC7D,IAAP,CAAZ,GAA2B,IAAIgF,GAAJ,EAA3B;AACD;;AACDF,oBAAY,CAACjB,KAAK,CAAC7D,IAAP,CAAZ,CAAyBQ,GAAzB,CAA6BkF,GAAG,CAAC1F,IAAjC;;AAEA,YAAI+E,OAAO,CAACI,GAAR,CAAYtB,KAAK,CAAC7D,IAAlB,CAAJ,EAA6B;AAC3B,mBAD2B,CAChB;AACZ;;AACDwF,aAAK,CAACjD,IAAN,CAAWsB,KAAX;AACD;AACF;AACF;;AACD,SAAO;AAAChB,UAAD;AAASiC;AAAT,GAAP;AACD;AAED;;;;;;;AAMA,SAASP,cAAT,CAAwBU,KAAxB,EAA6C;AAE3C,MAAIX,YAAJ;;AACA,MAAIW,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+BzC,MAA/B,KAA0C,CAA9C,EAAiD;AAC/CkB,gBAAY,GAAGW,KAAK,CAACzB,WAAN,CAAkBsC,MAAjC;AACD,GAFD,MAEO;AACL,QAAIC,SAAS,GAAW,IAAxB;;AACA,SAAK,IAAI5C,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG8B,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+BzC,MAAnD,EAA2D,EAAED,CAA7D,EAAgE;AAC9D,WAAK,MAAM6C,YAAX,IAA2Bf,KAAK,CAACzB,WAAN,CAAkBqC,YAAlB,CAA+B1C,CAA/B,EACjBc,aADV,EACyB;AACvB,YAAI+B,YAAY,CAAC5F,EAAb,KAAoB6E,KAAK,CAAC7E,EAA9B,EAAkC;AAChC2F,mBAAS,GAAG5C,CAAZ;AACA;AACD;AACF;AACF;;AACDmB,gBAAY,GAAGW,KAAK,CAACzB,WAAN,CAAkByC,WAAlB,CAA8BF,SAA9B,CAAf;AACD;;AACD,SAAOzB,YAAP;AACD","names":["cast","dispose","memory","util","ValueError","toList","InputLayer","SymbolicTensor","assertFeedCompatibility","key","val","dtype","err","name","FeedDict","constructor","feeds","id","id2Value","id2Mask","feed","add","value","mask","name2Id","addFeed","hasKey","names","Object","keys","getValue","getMask","disposeMasks","cachedSorted","cachedRecipientCounts","execute","fetches","feedDict","kwargs","probe","training","arrayFetches","Array","isArray","fetchArray","outputNames","map","t","finalOutputs","feedNames","outputName","indexOf","push","maxNumTensors","Infinity","minNumTensors","fetchAndFeedKey","join","sorted","recipientCounts","out","getTopologicalSortAndRecipientCounts","assign","internalFeedDict","i","length","numTensors","symbolic","srcLayer","sourceLayer","inputValues","inputMasks","tensorsToDispose","maskExists","input","inputs","isDisposed","stateful","outputTensors","apply","outputMask","supportsMasking","computeMask","layerOutputs","getNodeOutputs","outputSymbolicTensors","index","assert","finalSorted","finalRecipientMap","getTopologicalSortAndRecipientCountsForOneFetch","recipientMap","visited","Set","fetch","symbolicTensor","has","forEach","recipient","recipientMap2Counts","size","stack","marks","top","pop","topIsMarked","inboundNodes","output","nodeIndex","outputTensor","getOutputAt"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/engine/executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\n\nimport {cast, dispose, memory, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport {ValueError} from '../errors';\nimport {Kwargs} from '../types';\nimport {toList} from '../utils/generic_utils';\n\nimport {InputLayer} from './input_layer';\nimport {SymbolicTensor} from './topology';\n\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key: SymbolicTensor, val: Tensor): Tensor {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(\n        `The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n        `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n\n/**\n * A concrete Tensor value for a symbolic tensor as the key.\n */\nexport interface Feed {\n  key: SymbolicTensor;\n  value: Tensor;\n}\n\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n  private id2Value: {[id: number]: Tensor} = {};\n  private id2Mask: {[id: number]: Tensor} = {};\n  private name2Id: {[name: string]: number} = {};\n\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds?: Feed[]|FeedDict) {\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n  add(key: SymbolicTensor, value: Tensor, mask?: Tensor): FeedDict {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n    return this;\n  }\n\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n  addFeed(feed: Feed) {\n    this.add(feed.key, feed.value);\n  }\n\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n  hasKey(key: SymbolicTensor): boolean {\n    return this.id2Value[key.id] != null;\n  }\n\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n  names(): string[] {\n    return Object.keys(this.name2Id);\n  }\n\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getValue(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Value[id];\n    }\n  }\n\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getMask(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Mask[id];\n    }\n  }\n\n  /** Dispose all mask Tensors held by this object. */\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n}\n\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nconst cachedSorted: {[concatFetchNames: string]: SymbolicTensor[]} = {};\n\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nconst cachedRecipientCounts:\n    {[concatFetchNames: string]: {[fetchName: string]: number}} = {};\n\n/**\n * Interface for the optional object used for probing the memory\n * usage and other statistics during execution.\n */\nexport interface ExecutionProbe {\n  /**\n   * Maximum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  maxNumTensors?: number;\n\n  /**\n   * Minimum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  minNumTensors?: number;\n}\n\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(\n    fetches: SymbolicTensor|SymbolicTensor[], feedDict: FeedDict,\n    kwargs?: Kwargs, probe?: ExecutionProbe): Tensor|\n    Tensor[]|[Tensor | Tensor[]] {\n  const training: boolean = kwargs == null ? false : kwargs['training'];\n\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray: SymbolicTensor[] =\n      arrayFetches ? fetches as SymbolicTensor[] : [fetches as SymbolicTensor];\n\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs: Tensor[] = [];\n  const feedNames = feedDict.names();\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  }\n\n  // Check cache.\n  const fetchAndFeedKey =\n      outputNames.join(',') + '|' + feedDict.names().join(',');\n  let sorted: SymbolicTensor[];\n  let recipientCounts: {[fetchName: string]: number};\n  if (cachedSorted[fetchAndFeedKey] == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts;\n\n    // Store results in cache for future use.\n    cachedSorted[fetchAndFeedKey] = sorted;\n    cachedRecipientCounts[fetchAndFeedKey] = recipientCounts;\n  }\n  sorted = cachedSorted[fetchAndFeedKey];\n  recipientCounts = {};\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts[fetchAndFeedKey]);\n  }\n\n  const internalFeedDict = new FeedDict(feedDict);\n\n  // Start iterative execution on the topologically-sorted SymbolicTensors.\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n    const inputValues: Tensor[] = [];\n    const inputMasks: Tensor[] = [];\n    const tensorsToDispose: Tensor[] = [];\n\n    let maskExists = false;\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n      if (mask != null) {\n        maskExists = true;\n      }\n      if (!training) {\n        recipientCounts[input.name]--;\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n            outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n            input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n    const outputTensors =\n        toList(srcLayer.apply(inputValues, kwargs)) as Tensor[];\n    let outputMask: Tensor|Tensor[] = null;\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors =\n        Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(\n            outputSymbolicTensors[i], outputTensors[i],\n            Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  }\n  // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n  internalFeedDict.disposeMasks();\n\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n\ntype RecipientCounts = {\n  [fetchName: string]: number\n};\n\nexport type RecipientMap = {\n  [fetchName: string]: Set<string>;\n};\n\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(\n    fetches: SymbolicTensor[], feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientCounts: RecipientCounts} {\n  util.assert(\n      fetches != null && fetches.length > 0,\n      () => `Expected at least one fetch, got none`);\n\n  let finalSorted: SymbolicTensor[] = [];\n  let finalRecipientMap: RecipientMap = {};\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out =\n        getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set<string>();\n    for (const fetch of fetches) {\n      const {sorted, recipientMap} =\n          getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n\n      // Merge sorted SymbolicTensor Arrays.\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      }\n\n      // Merge recipient maps.\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set<string>();\n        }\n        recipientMap[name].forEach(\n            recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap: RecipientMap): RecipientCounts {\n  const recipientCounts: RecipientCounts = {};\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n  return recipientCounts;\n}\n\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(\n    fetch: SymbolicTensor, feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientMap: RecipientMap} {\n  const visited = new Set<string>();\n  const sorted: SymbolicTensor[] = [];\n  const recipientMap: RecipientMap = {};\n\n  // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n\n  const stack: SymbolicTensor[] = [];\n  const marks: number[] = [];\n\n  // Initial population of stack and marks.\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set<string>();\n        }\n        recipientMap[input.name].add(top.name);\n\n        if (visited.has(input.name)) {\n          continue;  // Avoid repeated visits to the same SymbolicTensor.\n        }\n        stack.push(input);\n      }\n    }\n  }\n  return {sorted, recipientMap};\n}\n\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch: SymbolicTensor): SymbolicTensor|\n    SymbolicTensor[] {\n  let layerOutputs: SymbolicTensor|SymbolicTensor[];\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex: number = null;\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n               .outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n  return layerOutputs;\n}\n"]},"metadata":{},"sourceType":"module"}