{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// base.ts is tfjs-core without auto registration of things like flags,\n// gradients, chained ops or the opHandler. See base_side_effects.ts for parts\n// tfjs core that are required side effects.\n\n/**\n * @fileoverview\n * @suppress {partialAlias} Optimization disabled due to passing the module\n * object into a function below:\n *\n *   import * as ops from './ops/ops';\n *   setOpHandler(ops);\n */\n// Serialization.\nimport * as io from './io/io';\nimport * as math from './math';\nimport * as broadcast_util from './ops/broadcast_util';\nimport * as browser from './ops/browser';\nimport * as gather_util from './ops/gather_nd_util';\nimport * as scatter_util from './ops/scatter_nd_util';\nimport * as slice_util from './ops/slice_util';\nimport * as serialization from './serialization';\nimport * as tensor_util from './tensor_util';\nimport * as test_util from './test_util';\nimport * as util from './util';\nimport { version } from './version';\nexport { AdadeltaOptimizer } from './optimizers/adadelta_optimizer';\nexport { AdagradOptimizer } from './optimizers/adagrad_optimizer';\nexport { AdamOptimizer } from './optimizers/adam_optimizer';\nexport { AdamaxOptimizer } from './optimizers/adamax_optimizer';\nexport { MomentumOptimizer } from './optimizers/momentum_optimizer';\nexport { Optimizer } from './optimizers/optimizer'; // Optimizers.\n\nexport { OptimizerConstructors } from './optimizers/optimizer_constructors';\nexport { RMSPropOptimizer } from './optimizers/rmsprop_optimizer';\nexport { SGDOptimizer } from './optimizers/sgd_optimizer';\nexport { Tensor, TensorBuffer, Variable } from './tensor';\nexport { Rank, sumOutType, upcastType } from './types';\nexport * from './ops/ops';\nexport { Reduction } from './ops/loss_ops_utils';\nexport * from './train';\nexport * from './globals';\nexport * from './kernel_registry';\nexport { customGrad, grad, grads, valueAndGrad, valueAndGrads, variableGrads } from './gradients';\nexport { Environment, env, ENV } from './environment';\nexport { version as version_core }; // Top-level method exports.\n\nexport { nextFrame } from './browser_util'; // Second level exports.\n\nimport * as backend_util from './backends/backend_util';\nimport * as device_util from './device_util';\nexport { browser, io, math, serialization, test_util, util, backend_util, broadcast_util, tensor_util, slice_util, gather_util, scatter_util, device_util };\nimport * as kernel_impls from './backends/kernel_impls';\nexport { kernel_impls }; // Backend specific.\n\nexport { KernelBackend, DataStorage } from './backends/backend'; // Export all kernel names / info.\n\nexport * from './kernel_names';","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA;AACA;AACA;;AAEA;;;;;;;;AASA;AACA,OAAO,KAAKA,EAAZ,MAAoB,SAApB;AACA,OAAO,KAAKC,IAAZ,MAAsB,QAAtB;AACA,OAAO,KAAKC,cAAZ,MAAgC,sBAAhC;AACA,OAAO,KAAKC,OAAZ,MAAyB,eAAzB;AACA,OAAO,KAAKC,WAAZ,MAA6B,sBAA7B;AACA,OAAO,KAAKC,YAAZ,MAA8B,uBAA9B;AACA,OAAO,KAAKC,UAAZ,MAA4B,kBAA5B;AACA,OAAO,KAAKC,aAAZ,MAA+B,iBAA/B;AACA,OAAO,KAAKC,WAAZ,MAA6B,eAA7B;AACA,OAAO,KAAKC,SAAZ,MAA2B,aAA3B;AACA,OAAO,KAAKC,IAAZ,MAAsB,QAAtB;AACA,SAAQC,OAAR,QAAsB,WAAtB;AAGA,SAAQC,iBAAR,QAAgC,iCAAhC;AACA,SAAQC,gBAAR,QAA+B,gCAA/B;AACA,SAAQC,aAAR,QAA4B,6BAA5B;AACA,SAAQC,eAAR,QAA8B,+BAA9B;AACA,SAAQC,iBAAR,QAAgC,iCAAhC;AACA,SAAQC,SAAR,QAAwB,wBAAxB,C,CACA;;AACA,SAAQC,qBAAR,QAAoC,qCAApC;AACA,SAAQC,gBAAR,QAA+B,gCAA/B;AACA,SAAQC,YAAR,QAA2B,4BAA3B;AACA,SAAiEC,MAAjE,EAA2HC,YAA3H,EAAyIC,QAAzI,QAAwJ,UAAxJ;AAEA,SAAsFC,IAAtF,EAAkIC,UAAlI,EAAsKC,UAAtK,QAAuL,SAAvL;AAEA,cAAc,WAAd;AACA,SAAQC,SAAR,QAAwB,sBAAxB;AAEA,cAAc,SAAd;AACA,cAAc,WAAd;AACA,cAAc,mBAAd;AACA,SAAQC,UAAR,EAAoBC,IAApB,EAA0BC,KAA1B,EAAiCC,YAAjC,EAA+CC,aAA/C,EAA8DC,aAA9D,QAAkF,aAAlF;AAGA,SAAQC,WAAR,EAAqBC,GAArB,EAA0BC,GAA1B,QAAoC,eAApC;AAGA,SAAQzB,OAAO,IAAI0B,YAAnB,G,CAEA;;AACA,SAAQC,SAAR,QAAwB,gBAAxB,C,CAEA;;AACA,OAAO,KAAKC,YAAZ,MAA8B,yBAA9B;AACA,OAAO,KAAKC,WAAZ,MAA6B,eAA7B;AACA,SACErC,OADF,EAEEH,EAFF,EAGEC,IAHF,EAIEM,aAJF,EAKEE,SALF,EAMEC,IANF,EAOE6B,YAPF,EAQErC,cARF,EASEM,WATF,EAUEF,UAVF,EAWEF,WAXF,EAYEC,YAZF,EAaEmC,WAbF;AAgBA,OAAO,KAAKC,YAAZ,MAA8B,yBAA9B;AACA,SAAQA,YAAR,G,CACA;;AACA,SAAQC,aAAR,EAAqDC,WAArD,QAAuE,oBAAvE,C,CAEA;;AACA,cAAc,gBAAd","names":["io","math","broadcast_util","browser","gather_util","scatter_util","slice_util","serialization","tensor_util","test_util","util","version","AdadeltaOptimizer","AdagradOptimizer","AdamOptimizer","AdamaxOptimizer","MomentumOptimizer","Optimizer","OptimizerConstructors","RMSPropOptimizer","SGDOptimizer","Tensor","TensorBuffer","Variable","Rank","sumOutType","upcastType","Reduction","customGrad","grad","grads","valueAndGrad","valueAndGrads","variableGrads","Environment","env","ENV","version_core","nextFrame","backend_util","device_util","kernel_impls","KernelBackend","DataStorage"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/base.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n// base.ts is tfjs-core without auto registration of things like flags,\n// gradients, chained ops or the opHandler. See base_side_effects.ts for parts\n// tfjs core that are required side effects.\n\n/**\n * @fileoverview\n * @suppress {partialAlias} Optimization disabled due to passing the module\n * object into a function below:\n *\n *   import * as ops from './ops/ops';\n *   setOpHandler(ops);\n */\n\n// Serialization.\nimport * as io from './io/io';\nimport * as math from './math';\nimport * as broadcast_util from './ops/broadcast_util';\nimport * as browser from './ops/browser';\nimport * as gather_util from './ops/gather_nd_util';\nimport * as scatter_util from './ops/scatter_nd_util';\nimport * as slice_util from './ops/slice_util';\nimport * as serialization from './serialization';\nimport * as tensor_util from './tensor_util';\nimport * as test_util from './test_util';\nimport * as util from './util';\nimport {version} from './version';\n\nexport {InferenceModel, MetaGraph, MetaGraphInfo, ModelPredictConfig, ModelTensorInfo, SavedModelTensorInfo, SignatureDef, SignatureDefEntry, SignatureDefInfo} from './model_types';\nexport {AdadeltaOptimizer} from './optimizers/adadelta_optimizer';\nexport {AdagradOptimizer} from './optimizers/adagrad_optimizer';\nexport {AdamOptimizer} from './optimizers/adam_optimizer';\nexport {AdamaxOptimizer} from './optimizers/adamax_optimizer';\nexport {MomentumOptimizer} from './optimizers/momentum_optimizer';\nexport {Optimizer} from './optimizers/optimizer';\n// Optimizers.\nexport {OptimizerConstructors} from './optimizers/optimizer_constructors';\nexport {RMSPropOptimizer} from './optimizers/rmsprop_optimizer';\nexport {SGDOptimizer} from './optimizers/sgd_optimizer';\nexport {DataToGPUOptions, DataToGPUWebGLOption, GPUData, Scalar, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, Tensor5D, TensorBuffer, Variable} from './tensor';\nexport {GradSaveFunc, NamedTensorMap, TensorContainer, TensorContainerArray, TensorContainerObject} from './tensor_types';\nexport {BackendValues, DataType, DataTypeMap, DataValues, NumericDataType, PixelData, Rank, RecursiveArray, ScalarLike, ShapeMap, sumOutType, TensorLike, TypedArray, upcastType} from './types';\n\nexport * from './ops/ops';\nexport {Reduction} from './ops/loss_ops_utils';\n\nexport * from './train';\nexport * from './globals';\nexport * from './kernel_registry';\nexport {customGrad, grad, grads, valueAndGrad, valueAndGrads, variableGrads} from './gradients';\n\nexport {TimingInfo, MemoryInfo, ForwardFunc} from './engine';\nexport {Environment, env, ENV} from './environment';\nexport {Platform} from './platforms/platform';\n\nexport {version as version_core};\n\n// Top-level method exports.\nexport {nextFrame} from './browser_util';\n\n// Second level exports.\nimport * as backend_util from './backends/backend_util';\nimport * as device_util from './device_util';\nexport {\n  browser,\n  io,\n  math,\n  serialization,\n  test_util,\n  util,\n  backend_util,\n  broadcast_util,\n  tensor_util,\n  slice_util,\n  gather_util,\n  scatter_util,\n  device_util\n};\n\nimport * as kernel_impls from './backends/kernel_impls';\nexport {kernel_impls};\n// Backend specific.\nexport {KernelBackend, BackendTimingInfo, DataMover, DataStorage} from './backends/backend';\n\n// Export all kernel names / info.\nexport * from './kernel_names';\n"]},"metadata":{},"sourceType":"module"}