{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport class Wrapper extends Layer {\n  constructor(args) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape) {\n    this.built = true;\n  } // TODO(cais): Implement activityRegularizer getter.\n\n\n  get trainable() {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  } // TODO(cais): Implement setter for trainableWeights.\n\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  } // TODO(cais): Implement setter for nonTrainableWeights.\n\n\n  get updates() {\n    // tslint:disable-next-line:no-any\n    return this.layer._updates;\n  } // TODO(cais): Implement getUpdatesFor().\n\n\n  get losses() {\n    return this.layer.losses;\n  } // TODO(cais): Implement getLossesFor().\n\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights) {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig() {\n    const config = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig()\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const layerConfig = config['layer'];\n    const layer = deserialize(layerConfig, customObjects);\n    delete config['layer'];\n    const newConfig = {\n      layer\n    };\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n\n}\nexport class TimeDistributed extends Wrapper {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n\n    if (inputShape.length < 3) {\n      throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` + `input shape ${JSON.stringify(inputShape)}`);\n    }\n\n    this.inputSpec = [{\n      shape: inputShape\n    }];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape = this.layer.computeOutputShape(childInputShape);\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n\n      const step = (inputs, states) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n\n      const rnnOutputs = rnn(step, inputs, [], false\n      /* goBackwards */\n      , null\n      /* mask */\n      , null\n      /* constants */\n      , false\n      /* unroll */\n      , true\n      /* needPerStepOutputs */\n      );\n      const y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n\n      return y;\n    });\n  }\n\n}\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n  constructor(args) {\n    super(args); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    const layerConfig = args.layer.getConfig();\n    const forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    const backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict);\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n    this.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(value) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights) {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape) {\n    let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes];\n    }\n\n    layerShapes = layerShapes;\n    let outputShape;\n    let outputShapes;\n    let stateShape;\n\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n\n    outputShape = outputShape;\n\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(inputs, kwargs) {\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = inputs.slice(1);\n      inputs = inputs[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) && constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n\n    const additionalInputs = [];\n    const additionalSpecs = [];\n\n    if (initialState != null) {\n      const numStates = initialState.length;\n\n      if (numStates % 2 > 0) {\n        throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n      }\n\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = initialState.map(state => new InputSpec({\n        shape: state.shape\n      }));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n\n    if (constants != null) {\n      throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n      let y;\n      let yRev;\n\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: forwardState\n        }));\n        yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, {\n          initialState: backwardState\n        }));\n      }\n\n      let states;\n\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat(yRev.slice(1));\n        } else {}\n\n        y = y[0];\n        yRev = yRev[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev, 1);\n      }\n\n      let output;\n\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y, yRev]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y, yRev);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y, yRev));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y, yRev);\n      } else if (this.mergeMode == null) {\n        output = [y, yRev];\n      } // TODO(cais): Properly set learning phase.\n\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return output.concat(states);\n        }\n\n        return [output].concat(states);\n      }\n\n      return output;\n    });\n  }\n\n  resetStates(states) {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs, mask) {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n\n    let outputMask;\n\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask = states.map(state => null);\n\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  } // TODO(cais): Implement constraints().\n\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const config = {\n      'mergeMode': this.mergeMode\n    }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    const rnnLayer = deserialize(config['layer']);\n    delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` + `present is not supported yet.`);\n    } // tslint:disable-next-line:no-any\n\n\n    const newConfig = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n\n}\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAQC,aAAR,EAA+BC,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAAQC,SAAR,QAAwB,WAAxB;AACA,SAAQC,SAAR,EAAmBC,KAAnB,EAAqCC,cAArC,QAA0D,oBAA1D;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AACA,SAAuCC,+BAAvC,QAA6E,wBAA7E;AAGA,OAAO,KAAKC,aAAZ,MAA+B,wBAA/B;AACA,SAAQC,kBAAR,EAA4BC,mBAA5B,QAAsD,sBAAtD;AAGA,SAAQC,GAAR,EAAkBC,eAAlB,QAAwC,aAAxC;AACA,SAAQC,WAAR,QAA0B,iBAA1B;AASA;;;;;;;;AAOA,OAAM,MAAgBC,OAAhB,SAAgCX,KAAhC,CAAqC;AAGzCY,cAAYC,IAAZ,EAAkC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAMA,IAAN;AACA,SAAKC,KAAL,GAAaD,IAAI,CAACC,KAAlB;AACD;;AAEDC,OAAK,CAACC,UAAD,EAA0B;AAC7B,SAAKC,KAAL,GAAa,IAAb;AACD,GAjBwC,CAmBzC;;;AAEa,MAATC,SAAS;AACX;AACA;AACA;AACA,QAAI,KAAKJ,KAAL,IAAc,IAAlB,EAAwB;AACtB,aAAO,KAAKA,KAAL,CAAWI,SAAlB;AACD,KAFD,MAEO;AACL,aAAO,KAAP;AACD;AACF;;AAEY,MAATA,SAAS,CAACC,KAAD,EAAe;AAC1B;AACA;AACA;AACA,QAAI,KAAKL,KAAL,IAAc,IAAlB,EAAwB;AACtB,WAAKA,KAAL,CAAWI,SAAX,GAAuBC,KAAvB;AACD;AACF;;AAEmB,MAAhBC,gBAAgB;AAClB,WAAO,KAAKN,KAAL,CAAWM,gBAAlB;AACD,GA3CwC,CA4CzC;;;AAEuB,MAAnBC,mBAAmB;AACrB,WAAO,KAAKP,KAAL,CAAWO,mBAAlB;AACD,GAhDwC,CAiDzC;;;AAEW,MAAPC,OAAO;AACT;AACA,WAAQ,KAAKR,KAAL,CAAmBS,QAA3B;AACD,GAtDwC,CAwDzC;;;AAEU,MAANC,MAAM;AACR,WAAO,KAAKV,KAAL,CAAWU,MAAlB;AACD,GA5DwC,CA8DzC;;;AAEAC,YAAU;AACR,WAAO,KAAKX,KAAL,CAAWW,UAAX,EAAP;AACD;;AAEDC,YAAU,CAACC,OAAD,EAAkB;AAC1B,SAAKb,KAAL,CAAWY,UAAX,CAAsBC,OAAtB;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AACvC,eAAS;AACP,qBAAa,KAAKf,KAAL,CAAWgB,YAAX,EADN;AAEP,kBAAU,KAAKhB,KAAL,CAAWc,SAAX;AAFH;AAD8B,KAAzC;AAMA,UAAMG,UAAU,GAAG,MAAMH,SAAN,EAAnB;AACAI,UAAM,CAACC,MAAP,CAAcJ,MAAd,EAAsBE,UAAtB;AACA,WAAOF,MAAP;AACD;;AAEDK,8BAA4B,CAACf,KAAD,EAAe;AACzC,UAAMe,4BAAN,CAAmCf,KAAnC;;AACA,QAAI,KAAKL,KAAL,IAAc,IAAlB,EAAwB;AACtB,WAAKA,KAAL,CAAWoB,4BAAX,CAAwCf,KAAxC;AACD;AACF;AAED;;;AACiB,SAAVgB,UAAU,CACbC,GADa,EAEbP,MAFa,EAGiC;AAAA,QAA9CQ,aAA8C,uEAA9B,EAA8B;AAChD,UAAMC,WAAW,GAAGT,MAAM,CAAC,OAAD,CAA1B;AACA,UAAMf,KAAK,GAAGJ,WAAW,CAAC4B,WAAD,EAAcD,aAAd,CAAzB;AACA,WAAOR,MAAM,CAAC,OAAD,CAAb;AACA,UAAMU,SAAS,GAAG;AAACzB;AAAD,KAAlB;AACAkB,UAAM,CAACC,MAAP,CAAcM,SAAd,EAAyBV,MAAzB;AACA,WAAO,IAAIO,GAAJ,CAAQG,SAAR,CAAP;AACD;;AAtGwC;AAyG3C,OAAM,MAAOC,eAAP,SAA+B7B,OAA/B,CAAsC;AAG1CC,cAAYC,IAAZ,EAAkC;AAChC,UAAMA,IAAN;AACA,SAAK4B,eAAL,GAAuB,IAAvB;AACD;;AAED1B,OAAK,CAACC,UAAD,EAA0B;AAC7BA,cAAU,GAAGV,kBAAkB,CAACU,UAAD,CAA/B;;AACA,QAAIA,UAAU,CAAC0B,MAAX,GAAoB,CAAxB,EAA2B;AACzB,YAAM,IAAIvC,UAAJ,CACF,sEACA,eAAewC,IAAI,CAACC,SAAL,CAAe5B,UAAf,CAA0B,EAFvC,CAAN;AAGD;;AACD,SAAK6B,SAAL,GAAiB,CAAC;AAACC,WAAK,EAAE9B;AAAR,KAAD,CAAjB;AACA,UAAM+B,eAAe,GAAG,CAAC/B,UAAU,CAAC,CAAD,CAAX,EAAgBgC,MAAhB,CAAuBhC,UAAU,CAACiC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;AACA,QAAI,CAAC,KAAKnC,KAAL,CAAWG,KAAhB,EAAuB;AACrB,WAAKH,KAAL,CAAWC,KAAX,CAAiBgC,eAAjB;AACA,WAAKjC,KAAL,CAAWG,KAAX,GAAmB,IAAnB;AACD;;AACD,UAAMF,KAAN,CAAYC,UAAZ;AACD;;AAEDkC,oBAAkB,CAAClC,UAAD,EAA0B;AAC1CA,cAAU,GAAGV,kBAAkB,CAACU,UAAD,CAA/B;AACA,UAAM+B,eAAe,GAAG,CAAC/B,UAAU,CAAC,CAAD,CAAX,EAAgBgC,MAAhB,CAAuBhC,UAAU,CAACiC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;AACA,UAAME,gBAAgB,GAClB,KAAKrC,KAAL,CAAWoC,kBAAX,CAA8BH,eAA9B,CADJ;AAEA,UAAMK,SAAS,GAAGpC,UAAU,CAAC,CAAD,CAA5B;AACA,WAAO,CAACmC,gBAAgB,CAAC,CAAD,CAAjB,EAAsBC,SAAtB,EAAiCJ,MAAjC,CAAwCG,gBAAgB,CAACF,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;AACD;;AAEDI,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAO3D,IAAI,CAAC,MAAK;AACf;AACA0D,YAAM,GAAG/C,mBAAmB,CAAC+C,MAAD,CAA5B,CAFe,CAGf;AACA;AACA;;AACA,YAAME,IAAI,GAAoB,CAACF,MAAD,EAAiBG,MAAjB,KAAqC;AACjE;AACA;AACA;AACA;AACA,cAAMC,MAAM,GAAGnD,mBAAmB,CAAC,KAAKO,KAAL,CAAWuC,IAAX,CAAgBC,MAAhB,EAAwBC,MAAxB,CAAD,CAAlC;AACA,eAAO,CAACG,MAAD,EAAS,EAAT,CAAP;AACD,OAPD;;AAQA,YAAMC,UAAU,GACZnD,GAAG,CAACgD,IAAD,EAAOF,MAAP,EAAe,EAAf,EAAmB;AAAM;AAAzB,QAA4C;AAAK;AAAjD,QACC;AAAK;AADN,QACuB;AAAM;AAD7B,QAEC;AAAK;AAFN,OADP;AAIA,YAAMM,CAAC,GAAGD,UAAU,CAAC,CAAD,CAApB,CAlBe,CAmBf;AACA;;AACA,aAAOC,CAAP;AACD,KAtBU,CAAX;AAuBD;;AAzDyC;AAC1C;;AACOpB,4BAAY,iBAAZ;AA2DT7C,aAAa,CAACkE,aAAd,CAA4BrB,eAA5B;AAEA,OAAM,SAAUsB,2BAAV,CAAsC3C,KAAtC,EAAoD;AACxDd,eAAa,CAAC0D,yBAAd,CACI3D,+BADJ,EACqC,wBADrC,EAC+De,KAD/D;AAED;AAkBD,MAAM6C,gCAAgC,GAA2B,QAAjE;AAEA,OAAM,MAAOC,aAAP,SAA6BtD,OAA7B,CAAoC;AAWxCC,cAAYC,IAAZ,EAAwC;AACtC,UAAMA,IAAN,EADsC,CAGtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,UAAMyB,WAAW,GAAGzB,IAAI,CAACC,KAAL,CAAWc,SAAX,EAApB;AACA,UAAMsC,QAAQ,GAA6B,EAA3C;AACAA,YAAQ,CAAC,WAAD,CAAR,GAAwBrD,IAAI,CAACC,KAAL,CAAWgB,YAAX,EAAxB;AACAoC,YAAQ,CAAC,QAAD,CAAR,GAAqB5B,WAArB;AACA,SAAK6B,YAAL,GAAoBzD,WAAW,CAACwD,QAAD,CAA/B;AACA5B,eAAW,CAAC,aAAD,CAAX,GACIA,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;AAEA,UAAM8B,QAAQ,GAA6B,EAA3C;AACAA,YAAQ,CAAC,WAAD,CAAR,GAAwBvD,IAAI,CAACC,KAAL,CAAWgB,YAAX,EAAxB;AACAsC,YAAQ,CAAC,QAAD,CAAR,GAAqB9B,WAArB;AACA,SAAK+B,aAAL,GAAqB3D,WAAW,CAAC0D,QAAD,CAAhC;AACA,SAAKD,YAAL,CAAkBG,IAAlB,GAAyB,aAAa,KAAKH,YAAL,CAAkBG,IAAxD;AACA,SAAKD,aAAL,CAAmBC,IAAnB,GAA0B,cAAc,KAAKD,aAAL,CAAmBC,IAA3D;AAEA,SAAKC,SAAL,GAAiB1D,IAAI,CAAC0D,SAAL,KAAmBC,SAAnB,GACbR,gCADa,GAEbnD,IAAI,CAAC0D,SAFT;AAGAT,+BAA2B,CAAC,KAAKS,SAAN,CAA3B;;AACA,QAAI1D,IAAI,CAACc,OAAT,EAAkB;AAChB,YAAM,IAAIzB,mBAAJ,CACF,iEADE,CAAN;AAED;;AACD,SAAKuE,SAAL,GAAiB5D,IAAI,CAACC,KAAL,CAAW4D,QAA5B;AACA,SAAKC,eAAL,GAAuB9D,IAAI,CAACC,KAAL,CAAW6D,eAAlC;AACA,SAAKC,WAAL,GAAmB/D,IAAI,CAACC,KAAL,CAAW8D,WAA9B;AACA,SAAKnC,eAAL,GAAuB,IAAvB;AACA,SAAKoC,UAAL,GAAkB,IAAlB;AACA,SAAKhC,SAAL,GAAiBhC,IAAI,CAACC,KAAL,CAAW+B,SAA5B;AACA,SAAKiC,YAAL,GAAoB,IAApB;AACD;;AAEY,MAAT5D,SAAS;AACX,WAAO,KAAK2D,UAAZ;AACD;;AAEY,MAAT3D,SAAS,CAACC,KAAD,EAAe;AAC1B;AACA;AACA;AACA,SAAK0D,UAAL,GAAkB1D,KAAlB;;AACA,QAAI,KAAKgD,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,WAAKA,YAAL,CAAkBjD,SAAlB,GAA8BC,KAA9B;AACD;;AACD,QAAI,KAAKkD,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,WAAKA,aAAL,CAAmBnD,SAAnB,GAA+BC,KAA/B;AACD;AACF;;AAEDM,YAAU;AACR,WAAO,KAAK0C,YAAL,CAAkB1C,UAAlB,GAA+BuB,MAA/B,CACH,KAAKqB,aAAL,CAAmB5C,UAAnB,EADG,CAAP;AAED;;AAEDC,YAAU,CAACC,OAAD,EAAkB;AAC1B,UAAMoD,UAAU,GAAGpD,OAAO,CAACe,MAA3B;AACA,UAAMsC,cAAc,GAAGC,IAAI,CAACC,KAAL,CAAWH,UAAU,GAAG,CAAxB,CAAvB;AACA,SAAKZ,YAAL,CAAkBzC,UAAlB,CAA6BC,OAAO,CAACsB,KAAR,CAAc,CAAd,EAAiB+B,cAAjB,CAA7B;AACA,SAAKX,aAAL,CAAmB3C,UAAnB,CAA8BC,OAAO,CAACsB,KAAR,CAAc+B,cAAd,CAA9B;AACD;;AAED9B,oBAAkB,CAAClC,UAAD,EAA0B;AAC1C,QAAImE,WAAW,GACX,KAAKhB,YAAL,CAAkBjB,kBAAlB,CAAqClC,UAArC,CADJ;;AAEA,QAAI,EAAEoE,KAAK,CAACC,OAAN,CAAcF,WAAd,KAA8BC,KAAK,CAACC,OAAN,CAAcF,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;AAClEA,iBAAW,GAAG,CAACA,WAAD,CAAd;AACD;;AACDA,eAAW,GAAGA,WAAd;AAEA,QAAIG,WAAJ;AACA,QAAIC,YAAJ;AACA,QAAIC,UAAJ;;AACA,QAAI,KAAKZ,WAAT,EAAsB;AACpBY,gBAAU,GAAGL,WAAW,CAAClC,KAAZ,CAAkB,CAAlB,CAAb;AACAqC,iBAAW,GAAGH,WAAW,CAAC,CAAD,CAAzB;AACD,KAHD,MAGO;AACLG,iBAAW,GAAGH,WAAW,CAAC,CAAD,CAAzB;AACD;;AACDG,eAAW,GAAGA,WAAd;;AACA,QAAI,KAAKf,SAAL,KAAmB,QAAvB,EAAiC;AAC/Be,iBAAW,CAACA,WAAW,CAAC5C,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;AACA6C,kBAAY,GAAG,CAACD,WAAD,CAAf;AACD,KAHD,MAGO,IAAI,KAAKf,SAAL,IAAkB,IAAtB,EAA4B;AACjCgB,kBAAY,GAAG,CAACD,WAAD,EAAcA,WAAW,CAACrC,KAAZ,EAAd,CAAf;AACD,KAFM,MAEA;AACLsC,kBAAY,GAAG,CAACD,WAAD,CAAf;AACD;;AAED,QAAI,KAAKV,WAAT,EAAsB;AACpB,UAAI,KAAKL,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,eAAOgB,YAAY,CAACvC,MAAb,CAAoBwC,UAApB,EAAgCxC,MAAhC,CAAuCwC,UAAU,CAACvC,KAAX,EAAvC,CAAP;AACD;;AACD,aAAO,CAACqC,WAAD,EAActC,MAAd,CAAqBwC,UAArB,EAAiCxC,MAAjC,CAAwCwC,UAAU,CAACvC,KAAX,EAAxC,CAAP;AACD;;AACD,WAAO5C,aAAa,CAACoF,gBAAd,CAA+BF,YAA/B,CAAP;AACD;;AAEDG,OAAK,CACDpC,MADC,EAEDC,MAFC,EAEc;AACjB,QAAIoC,YAAY,GACZpC,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAEA,QAAIqC,SAAS,GACTrC,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,WAAD,CADlC;;AAEA,QAAIA,MAAM,IAAI,IAAd,EAAoB;AAClBA,YAAM,GAAG,EAAT;AACD;;AACD,UAAMsC,YAAY,GACdpF,eAAe,CAAC6C,MAAD,EAASqC,YAAT,EAAuBC,SAAvB,EAAkC,KAAKd,YAAvC,CADnB;AAEAxB,UAAM,GAAGuC,YAAY,CAACvC,MAAtB;AACAqC,gBAAY,GAAGE,YAAY,CAACF,YAA5B;AACAC,aAAS,GAAGC,YAAY,CAACD,SAAzB;;AAEA,QAAIR,KAAK,CAACC,OAAN,CAAc/B,MAAd,CAAJ,EAA2B;AACzBqC,kBAAY,GAAIrC,MAAsC,CAACL,KAAvC,CAA6C,CAA7C,CAAhB;AACAK,YAAM,GAAIA,MAAsC,CAAC,CAAD,CAAhD;AACD;;AAED,QAAI,CAACqC,YAAY,IAAI,IAAhB,IAAwBA,YAAY,CAACjD,MAAb,KAAwB,CAAjD,KACAkD,SAAS,IAAI,IADjB,EACuB;AACrB,aAAO,MAAMF,KAAN,CAAYpC,MAAZ,EAAoBC,MAApB,CAAP;AACD;;AACD,UAAMuC,gBAAgB,GAAiC,EAAvD;AACA,UAAMC,eAAe,GAAgB,EAArC;;AACA,QAAIJ,YAAY,IAAI,IAApB,EAA0B;AACxB,YAAMK,SAAS,GAAGL,YAAY,CAACjD,MAA/B;;AACA,UAAIsD,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;AACrB,cAAM,IAAI7F,UAAJ,CACF,wDACA,wDADA,GAEA,sBAHE,CAAN;AAID;;AACDoD,YAAM,CAAC,cAAD,CAAN,GAAyBoC,YAAzB;AACAG,sBAAgB,CAACG,IAAjB,CAAsB,GAAGN,YAAzB;AACA,YAAMO,UAAU,GAAIP,YAA6C,CACzCQ,GADJ,CACQC,KAAK,IAAI,IAAIrG,SAAJ,CAAc;AAAC+C,aAAK,EAAEsD,KAAK,CAACtD;AAAd,OAAd,CADjB,CAApB;AAEA,WAAKqB,YAAL,CAAkBkC,SAAlB,GAA8BH,UAAU,CAACjD,KAAX,CAAiB,CAAjB,EAAoB+C,SAAS,GAAG,CAAhC,CAA9B;AACA,WAAK3B,aAAL,CAAmBgC,SAAnB,GAA+BH,UAAU,CAACjD,KAAX,CAAiB+C,SAAS,GAAG,CAA7B,CAA/B;AACAD,qBAAe,CAACE,IAAhB,CAAqB,GAAGC,UAAxB;AACD;;AACD,QAAIN,SAAS,IAAI,IAAjB,EAAuB;AACrB,YAAM,IAAI1F,mBAAJ,CACF,0DACA,kBAFE,CAAN;AAGD;;AAED,UAAMoG,gBAAgB,GAAGR,gBAAgB,CAAC,CAAD,CAAhB,YAA+B7F,cAAxD;;AACA,SAAK,MAAMsG,MAAX,IAAqBT,gBAArB,EAAuC;AACrC,UAAIS,MAAM,YAAYtG,cAAlB,KAAqCqG,gBAAzC,EAA2D;AACzD,cAAM,IAAInG,UAAJ,CACF,0DACA,yDAFE,CAAN;AAGD;AACF;;AAED,QAAImG,gBAAJ,EAAsB;AACpB;AACA,YAAME,SAAS,GAAG,CAAClD,MAAD,EAASN,MAAT,CAAgB8C,gBAAhB,CAAlB;AACA,YAAMW,aAAa,GAAG,KAAK5D,SAAL,CAAeG,MAAf,CAAsB+C,eAAtB,CAAtB,CAHoB,CAIpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,YAAMW,iBAAiB,GAAG,KAAK7D,SAA/B;AACA,WAAKA,SAAL,GAAiB4D,aAAjB;AACA,YAAM/C,MAAM,GACR,MAAMgC,KAAN,CAAYc,SAAZ,EAAsDjD,MAAtD,CADJ;AAEA,WAAKV,SAAL,GAAiB6D,iBAAjB;AACA,aAAOhD,MAAP;AACD,KAnBD,MAmBO;AACL,aAAO,MAAMgC,KAAN,CAAYpC,MAAZ,EAAoBC,MAApB,CAAP;AACD;AACF;;AAEDF,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAO3D,IAAI,CAAC,MAAK;AACf,YAAM+F,YAAY,GAAGpC,MAAM,CAAC,cAAD,CAA3B;AAEA,UAAIK,CAAJ;AACA,UAAI+C,IAAJ;;AACA,UAAIhB,YAAY,IAAI,IAApB,EAA0B;AACxB/B,SAAC,GAAG,KAAKO,YAAL,CAAkBd,IAAlB,CAAuBC,MAAvB,EAA+BC,MAA/B,CAAJ;AACAoD,YAAI,GAAG,KAAKtC,aAAL,CAAmBhB,IAAnB,CAAwBC,MAAxB,EAAgCC,MAAhC,CAAP;AACD,OAHD,MAGO;AACL,cAAMqD,YAAY,GAAGjB,YAAY,CAAC1C,KAAb,CAAmB,CAAnB,EAAsB0C,YAAY,CAACjD,MAAb,GAAsB,CAA5C,CAArB;AACA,cAAMmE,aAAa,GAAGlB,YAAY,CAAC1C,KAAb,CAAmB0C,YAAY,CAACjD,MAAb,GAAsB,CAAzC,CAAtB;AACAkB,SAAC,GAAG,KAAKO,YAAL,CAAkBd,IAAlB,CACAC,MADA,EACQtB,MAAM,CAACC,MAAP,CAAcsB,MAAd,EAAsB;AAACoC,sBAAY,EAAEiB;AAAf,SAAtB,CADR,CAAJ;AAEAD,YAAI,GAAG,KAAKtC,aAAL,CAAmBhB,IAAnB,CACHC,MADG,EACKtB,MAAM,CAACC,MAAP,CAAcsB,MAAd,EAAsB;AAACoC,sBAAY,EAAEkB;AAAf,SAAtB,CADL,CAAP;AAED;;AAED,UAAIpD,MAAJ;;AACA,UAAI,KAAKmB,WAAT,EAAsB;AACpB,YAAIQ,KAAK,CAACC,OAAN,CAAczB,CAAd,CAAJ,EAAsB;AACpBH,gBAAM,GAAGG,CAAC,CAACX,KAAF,CAAQ,CAAR,EAAWD,MAAX,CAAmB2D,IAAiB,CAAC1D,KAAlB,CAAwB,CAAxB,CAAnB,CAAT;AACD,SAFD,MAEO,CACN;;AACDW,SAAC,GAAIA,CAAc,CAAC,CAAD,CAAnB;AACA+C,YAAI,GAAIA,IAAiB,CAAC,CAAD,CAAzB;AACD;;AAED,UAAI,KAAKhC,eAAT,EAA0B;AACxBgC,YAAI,GAAGjH,GAAG,CAACoH,OAAJ,CAAYH,IAAZ,EAA4B,CAA5B,CAAP;AACD;;AAED,UAAIjD,MAAJ;;AACA,UAAI,KAAKa,SAAL,KAAmB,QAAvB,EAAiC;AAC/Bb,cAAM,GAAG7D,CAAC,CAACkH,WAAF,CAAc,CAACnD,CAAD,EAAc+C,IAAd,CAAd,CAAT;AACD,OAFD,MAEO,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;AACnCb,cAAM,GAAGhE,GAAG,CAACsH,GAAJ,CAAQpD,CAAR,EAAqB+C,IAArB,CAAT;AACD,OAFM,MAEA,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;AACnCb,cAAM,GAAGhE,GAAG,CAACuH,GAAJ,CAAQ,EAAR,EAAYvH,GAAG,CAACsH,GAAJ,CAAQpD,CAAR,EAAqB+C,IAArB,CAAZ,CAAT;AACD,OAFM,MAEA,IAAI,KAAKpC,SAAL,KAAmB,KAAvB,EAA8B;AACnCb,cAAM,GAAGhE,GAAG,CAACuH,GAAJ,CAAQrD,CAAR,EAAqB+C,IAArB,CAAT;AACD,OAFM,MAEA,IAAI,KAAKpC,SAAL,IAAkB,IAAtB,EAA4B;AACjCb,cAAM,GAAG,CAACE,CAAD,EAAc+C,IAAd,CAAT;AACD,OA1Cc,CA4Cf;;;AACA,UAAI,KAAK/B,WAAT,EAAsB;AACpB,YAAI,KAAKL,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,iBAAQb,MAAmB,CAACV,MAApB,CAA2BS,MAA3B,CAAR;AACD;;AACD,eAAO,CAACC,MAAD,EAAmBV,MAAnB,CAA0BS,MAA1B,CAAP;AACD;;AACD,aAAOC,MAAP;AACD,KApDU,CAAX;AAqDD;;AAEDwD,aAAW,CAACzD,MAAD,EAAyB;AAClC,SAAKU,YAAL,CAAkB+C,WAAlB;AACA,SAAK7C,aAAL,CAAmB6C,WAAnB;AACD;;AAEDnG,OAAK,CAACC,UAAD,EAA0B;AAC7BlB,aAAS,CAAC,KAAKqE,YAAL,CAAkBG,IAAnB,EAAyB,MAAK;AACrC,WAAKH,YAAL,CAAkBpD,KAAlB,CAAwBC,UAAxB;AACD,KAFQ,CAAT;AAGAlB,aAAS,CAAC,KAAKuE,aAAL,CAAmBC,IAApB,EAA0B,MAAK;AACtC,WAAKD,aAAL,CAAmBtD,KAAnB,CAAyBC,UAAzB;AACD,KAFQ,CAAT;AAGA,SAAKC,KAAL,GAAa,IAAb;AACD;;AAEDkG,aAAW,CAAC7D,MAAD,EAA0B8D,IAA1B,EAAgD;AAEzD,QAAIhC,KAAK,CAACC,OAAN,CAAc+B,IAAd,CAAJ,EAAyB;AACvBA,UAAI,GAAGA,IAAI,CAAC,CAAD,CAAX;AACD;;AACD,QAAIC,UAAJ;;AACA,QAAI,KAAK1C,eAAT,EAA0B;AACxB,UAAI,KAAKJ,SAAL,IAAkB,IAAtB,EAA4B;AAC1B8C,kBAAU,GAAG,CAACD,IAAD,EAAOA,IAAP,CAAb;AACD,OAFD,MAEO;AACLC,kBAAU,GAAGD,IAAb;AACD;AACF,KAND,MAMO;AACL,UAAI,KAAK7C,SAAL,IAAkB,IAAtB,EAA4B;AAC1B8C,kBAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;AACD,OAFD,MAEO;AACLA,kBAAU,GAAG,IAAb;AACD;AACF;;AACD,QAAI,KAAKzC,WAAT,EAAsB;AACpB,YAAMnB,MAAM,GAAG,KAAKU,YAAL,CAAkBV,MAAjC;AACA,YAAM6D,SAAS,GAAa7D,MAAM,CAAC0C,GAAP,CAAWC,KAAK,IAAI,IAApB,CAA5B;;AACA,UAAIhB,KAAK,CAACC,OAAN,CAAcgC,UAAd,CAAJ,EAA+B;AAC7B,eAAOA,UAAU,CAACrE,MAAX,CAAkBsE,SAAlB,EAA6BtE,MAA7B,CAAoCsE,SAApC,CAAP;AACD,OAFD,MAEO;AACL,eAAO,CAACD,UAAD,EAAarE,MAAb,CAAoBsE,SAApB,EAA+BtE,MAA/B,CAAsCsE,SAAtC,CAAP;AACD;AACF,KARD,MAQO;AACL,aAAOD,UAAP;AACD;AACF;;AAEmB,MAAhBjG,gBAAgB;AAClB,WAAO,KAAK+C,YAAL,CAAkB/C,gBAAlB,CAAmC4B,MAAnC,CACH,KAAKqB,aAAL,CAAmBjD,gBADhB,CAAP;AAED;;AAEsB,MAAnBC,mBAAmB;AACrB,WAAO,KAAK8C,YAAL,CAAkB9C,mBAAlB,CAAsC2B,MAAtC,CACH,KAAKqB,aAAL,CAAmBhD,mBADhB,CAAP;AAED,GAvTuC,CAyTxC;;;AAEAa,8BAA4B,CAACf,KAAD,EAAe;AACzC,UAAMe,4BAAN,CAAmCf,KAAnC;;AACA,QAAI,KAAKgD,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,WAAKA,YAAL,CAAkBjC,4BAAlB,CAA+Cf,KAA/C;AACD;;AACD,QAAI,KAAKkD,aAAL,IAAsB,IAA1B,EAAgC;AAC9B,WAAKA,aAAL,CAAmBnC,4BAAnB,CAAgDf,KAAhD;AACD;AACF;;AAEDS,WAAS;AACP,UAAMC,MAAM,GAA6B;AACvC,mBAAa,KAAK0C;AADqB,KAAzC,CADO,CAIP;;AACA,UAAMxC,UAAU,GAAG,MAAMH,SAAN,EAAnB;AACAI,UAAM,CAACC,MAAP,CAAcJ,MAAd,EAAsBE,UAAtB;AACA,WAAOF,MAAP;AACD;AAED;;;AACiB,SAAVM,UAAU,CACbC,GADa,EAEbP,MAFa,EAEmB;AAClC,UAAM0F,QAAQ,GACV7G,WAAW,CAACmB,MAAM,CAAC,OAAD,CAAP,CADf;AAEA,WAAOA,MAAM,CAAC,OAAD,CAAb,CAHkC,CAIlC;;AACA,QAAIA,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;AAClC,YAAM,IAAI3B,mBAAJ,CACF,gEACA,+BAFE,CAAN;AAGD,KATiC,CAUlC;;;AACA,UAAMqC,SAAS,GAAyBV,MAAxC;AACAU,aAAS,CAAC,OAAD,CAAT,GAAqBgF,QAArB;AACA,WAAO,IAAInF,GAAJ,CAAQG,SAAR,CAAP;AACD;;AAhWuC;AACxC;;AACO0B,0BAAY,eAAZ;AAgWTtE,aAAa,CAACkE,aAAd,CAA4BI,aAA5B","names":["tfc","serialization","tidy","K","nameScope","InputSpec","Layer","SymbolicTensor","NotImplementedError","ValueError","VALID_BIDIRECTIONAL_MERGE_MODES","generic_utils","getExactlyOneShape","getExactlyOneTensor","rnn","standardizeArgs","deserialize","Wrapper","constructor","args","layer","build","inputShape","built","trainable","value","trainableWeights","nonTrainableWeights","updates","_updates","losses","getWeights","setWeights","weights","getConfig","config","getClassName","baseConfig","Object","assign","setFastWeightInitDuringBuild","fromConfig","cls","customObjects","layerConfig","newConfig","TimeDistributed","supportsMasking","length","JSON","stringify","inputSpec","shape","childInputShape","concat","slice","computeOutputShape","childOutputShape","timesteps","call","inputs","kwargs","step","states","output","rnnOutputs","y","registerClass","checkBidirectionalMergeMode","checkStringTypeUnionValue","DEFAULT_BIDIRECTIONAL_MERGE_MODE","Bidirectional","forwDict","forwardLayer","backDict","backwardLayer","name","mergeMode","undefined","_stateful","stateful","returnSequences","returnState","_trainable","numConstants","numWeights","numeightsOver2","Math","floor","layerShapes","Array","isArray","outputShape","outputShapes","stateShape","singletonOrArray","apply","initialState","constants","standardized","additionalInputs","additionalSpecs","numStates","push","stateSpecs","map","state","stateSpec","isSymbolicTensor","tensor","fullInput","fullInputSpec","originalInputSpec","yRev","forwardState","backwardState","reverse","concatenate","add","mul","resetStates","computeMask","mask","outputMask","stateMask","rnnLayer"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/layers/wrappers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {InputSpec, Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {BidirectionalMergeMode, Shape, VALID_BIDIRECTIONAL_MERGE_MODES} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {RegularizerFn, RnnStepFunction} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nimport {rnn, RNN, standardizeArgs} from './recurrent';\nimport {deserialize} from './serialization';\n\nexport declare interface WrapperLayerArgs extends LayerArgs {\n  /**\n   * The layer to be wrapped.\n   */\n  layer: Layer;\n}\n\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport abstract class Wrapper extends Layer {\n  readonly layer: Layer;\n\n  constructor(args: WrapperLayerArgs) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    this.built = true;\n  }\n\n  // TODO(cais): Implement activityRegularizer getter.\n\n  get trainable(): boolean {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n\n  get updates(): Tensor[] {\n    // tslint:disable-next-line:no-any\n    return (this.layer as any)._updates;\n  }\n\n  // TODO(cais): Implement getUpdatesFor().\n\n  get losses(): RegularizerFn[] {\n    return this.layer.losses;\n  }\n\n  // TODO(cais): Implement getLossesFor().\n\n  getWeights(): Tensor[] {\n    return this.layer.getWeights();\n  }\n\n  setWeights(weights: Tensor[]): void {\n    this.layer.setWeights(weights);\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig(),\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const layerConfig = config['layer'] as serialization.ConfigDict;\n    const layer = deserialize(layerConfig, customObjects) as Layer;\n    delete config['layer'];\n    const newConfig = {layer};\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\n\nexport class TimeDistributed extends Wrapper {\n  /** @nocollapse */\n  static className = 'TimeDistributed';\n  constructor(args: WrapperLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(\n          `TimeDistributed layer expects an input shape >= 3D, but received ` +\n          `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{shape: inputShape}];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape =\n        this.layer.computeOutputShape(childInputShape) as Shape;\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step: RnnStepFunction = (inputs: Tensor, states: Tensor[]) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs =\n          rnn(step, inputs, [], false /* goBackwards */, null /* mask */,\n              null /* constants */, false /* unroll */,\n              true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n\n  // TODO(cais): Implement detailed computeMask() logic.\n}\nserialization.registerClass(TimeDistributed);\n\nexport function checkBidirectionalMergeMode(value?: string): void {\n  generic_utils.checkStringTypeUnionValue(\n      VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\n\nexport declare interface BidirectionalLayerArgs extends WrapperLayerArgs {\n  /**\n   * The instance of an `RNN` layer to be wrapped.\n   */\n  layer: RNN;\n\n  /**\n   * Mode by which outputs of the forward and backward RNNs are\n   * combined. If `null` or `undefined`, the output will not be\n   * combined, they will be returned as an `Array`.\n   *\n   * If `undefined` (i.e., not provided), defaults to `'concat'`.\n   */\n  mergeMode?: BidirectionalMergeMode;\n}\n\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE: BidirectionalMergeMode = 'concat';\n\nexport class Bidirectional extends Wrapper {\n  /** @nocollapse */\n  static className = 'Bidirectional';\n  mergeMode: BidirectionalMergeMode;\n  private forwardLayer: RNN;\n  private backwardLayer: RNN;\n  private returnSequences: boolean;\n  private returnState: boolean;\n  private numConstants?: number;\n  private _trainable: boolean;\n\n  constructor(args: BidirectionalLayerArgs) {\n    super(args);\n\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict: serialization.ConfigDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict) as RNN;\n    layerConfig['goBackwards'] =\n        layerConfig['goBackwards'] === true ? false : true;\n    const backDict: serialization.ConfigDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict) as RNN;\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n\n    this.mergeMode = args.mergeMode === undefined ?\n        DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n        args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError(\n          'weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  get trainable(): boolean {\n    return this._trainable;\n  }\n\n  set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  getWeights(): Tensor[] {\n    return this.forwardLayer.getWeights().concat(\n        this.backwardLayer.getWeights());\n  }\n\n  setWeights(weights: Tensor[]): void {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    let layerShapes: Shape|Shape[] =\n        this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes as Shape];\n    }\n    layerShapes = layerShapes as Shape[];\n\n    let outputShape: Shape;\n    let outputShapes: Shape[];\n    let stateShape: Shape[];\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = (inputs as Tensor[] | SymbolicTensor[]).slice(1);\n      inputs = (inputs as Tensor[] | SymbolicTensor[])[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) &&\n        constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    const additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError(\n            'When passing `initialState` to a Bidrectional RNN, ' +\n            'the state should be an Array containing the states of ' +\n            'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = (initialState as Array<Tensor|SymbolicTensor>)\n                             .map(state => new InputSpec({shape: state.shape}));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError(\n          'Support for constants in Bidirectional layers is not ' +\n          'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError(\n            'The initial state of a Bidirectional layer cannot be ' +\n            'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output =\n          super.apply(fullInput as Tensor[] | SymbolicTensor[], kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n\n      let y: Tensor|Tensor[];\n      let yRev: Tensor|Tensor[];\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: forwardState}));\n        yRev = this.backwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: backwardState}));\n      }\n\n      let states: Tensor[];\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat((yRev as Tensor[]).slice(1));\n        } else {\n        }\n        y = (y as Tensor[])[0];\n        yRev = (yRev as Tensor[])[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev as Tensor, 1);\n      }\n\n      let output: Tensor|Tensor[];\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y as Tensor, yRev as Tensor]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y as Tensor, yRev as Tensor));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode == null) {\n        output = [y as Tensor, yRev as Tensor];\n      }\n\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return (output as Tensor[]).concat(states);\n        }\n        return [output as Tensor].concat(states);\n      }\n      return output;\n    });\n  }\n\n  resetStates(states?: Tensor|Tensor[]): void {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask: Tensor|Tensor[];\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask: Tensor[] = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    return this.forwardLayer.trainableWeights.concat(\n        this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    return this.forwardLayer.nonTrainableWeights.concat(\n        this.backwardLayer.nonTrainableWeights);\n  }\n\n  // TODO(cais): Implement constraints().\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'mergeMode': this.mergeMode,\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    const rnnLayer =\n        deserialize(config['layer'] as serialization.ConfigDict) as RNN;\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\n          `Deserialization of a Bidirectional layer with numConstants ` +\n          `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig: {[key: string]: any} = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\nserialization.registerClass(Bidirectional);\n"]},"metadata":{},"sourceType":"module"}