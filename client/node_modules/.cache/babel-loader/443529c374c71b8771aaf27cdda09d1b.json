{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { dispose } from '../../globals';\nimport { assert } from '../../util';\nimport { clone } from '../clone';\nimport { concat } from '../concat';\nimport { div } from '../div';\nimport { eye } from '../eye';\nimport { greater } from '../greater';\nimport { matMul } from '../mat_mul';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { slice } from '../slice';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { tensor2d } from '../tensor2d';\nimport { transpose } from '../transpose';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n *\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\n\nfunction qr_(x) {\n  let fullMatrices = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n  assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n\n  if (x.rank === 2) {\n    return qr2d(x, fullMatrices);\n  } else {\n    // Rank > 2.\n    // TODO(cais): Below we split the input into individual 2D tensors,\n    //   perform QR decomposition on them and then stack the results back\n    //   together. We should explore whether this can be parallelized.\n    const outerDimsProd = x.shape.slice(0, x.shape.length - 2).reduce((value, prev) => value * prev);\n    const x2ds = unstack(reshape(x, [outerDimsProd, x.shape[x.shape.length - 2], x.shape[x.shape.length - 1]]), 0);\n    const q2ds = [];\n    const r2ds = [];\n    x2ds.forEach(x2d => {\n      const [q2d, r2d] = qr2d(x2d, fullMatrices);\n      q2ds.push(q2d);\n      r2ds.push(r2d);\n    });\n    const q = reshape(stack(q2ds, 0), x.shape);\n    const r = reshape(stack(r2ds, 0), x.shape);\n    return [q, r];\n  }\n}\n\nfunction qr2d(x) {\n  let fullMatrices = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n  return ENGINE.tidy(() => {\n    assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n    const m = x.shape[0];\n    const n = x.shape[1];\n    let q = eye(m); // Orthogonal transform so far.\n\n    let r = clone(x); // Transformed matrix so far.\n\n    const one2D = tensor2d([[1]], [1, 1]);\n    let w = clone(one2D);\n    const iters = m >= n ? n : m;\n\n    for (let j = 0; j < iters; ++j) {\n      // This tidy within the for-loop ensures we clean up temporary\n      // tensors as soon as they are no longer needed.\n      const rTemp = r;\n      const wTemp = w;\n      const qTemp = q;\n      [w, r, q] = ENGINE.tidy(() => {\n        // Find H = I - tau * w * w', to put zeros below R(j, j).\n        const rjEnd1 = slice(r, [j, j], [m - j, 1]);\n        const normX = norm(rjEnd1);\n        const rjj = slice(r, [j, j], [1, 1]); // The sign() function returns 0 on 0, which causes division by zero.\n\n        const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));\n        const u1 = sub(rjj, mul(s, normX));\n        const wPre = div(rjEnd1, u1);\n\n        if (wPre.shape[0] === 1) {\n          w = clone(one2D);\n        } else {\n          w = concat([one2D, slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])], 0);\n        }\n\n        const tau = neg(div(matMul(s, u1), normX)); // -- R := HR, Q := QH.\n\n        const rjEndAll = slice(r, [j, 0], [m - j, n]);\n        const tauTimesW = mul(tau, w);\n        const wT = transpose(w);\n\n        if (j === 0) {\n          r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n        } else {\n          const rTimesTau = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n          r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);\n        }\n\n        const tawTimesWT = transpose(tauTimesW);\n        const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);\n\n        if (j === 0) {\n          q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n        } else {\n          const qTimesTau = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n          q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);\n        }\n\n        return [w, r, q];\n      });\n      dispose([rTemp, wTemp, qTemp]);\n    }\n\n    if (!fullMatrices && m > n) {\n      q = slice(q, [0, 0], [m, n]);\n      r = slice(r, [0, 0], [n, n]);\n    }\n\n    return [q, r];\n  });\n}\n\nexport const qr = op({\n  qr_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAgBA,SAAQA,MAAR,QAAqB,cAArB;AACA,SAAQC,OAAR,QAAsB,eAAtB;AAEA,SAAQC,MAAR,QAAqB,YAArB;AAEA,SAAQC,KAAR,QAAoB,UAApB;AACA,SAAQC,MAAR,QAAqB,WAArB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,SAAQC,OAAR,QAAsB,YAAtB;AACA,SAAQC,MAAR,QAAqB,YAArB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,SAAQC,IAAR,QAAmB,SAAnB;AACA,SAAQC,EAAR,QAAiB,cAAjB;AACA,SAAQC,OAAR,QAAsB,YAAtB;AACA,SAAQC,KAAR,QAAoB,UAApB;AACA,SAAQC,KAAR,QAAoB,UAApB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,SAAQC,QAAR,QAAuB,aAAvB;AACA,SAAQC,SAAR,QAAwB,cAAxB;AACA,SAAQC,OAAR,QAAsB,YAAtB;AACA,SAAQC,KAAR,QAAoB,UAApB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2CA,SAASC,GAAT,CAAaC,CAAb,EAA4C;AAAA,MAApBC,YAAoB,uEAAL,KAAK;AAC1CrB,QAAM,CACFoB,CAAC,CAACE,IAAF,IAAU,CADR,EAEF,MAAM,gEACFF,CAAC,CAACE,IAAI,EAHR,CAAN;;AAKA,MAAIF,CAAC,CAACE,IAAF,KAAW,CAAf,EAAkB;AAChB,WAAOC,IAAI,CAACH,CAAD,EAAgBC,YAAhB,CAAX;AACD,GAFD,MAEO;AACL;AACA;AACA;AACA;AACA,UAAMG,aAAa,GAAGJ,CAAC,CAACK,KAAF,CAAQb,KAAR,CAAc,CAAd,EAAiBQ,CAAC,CAACK,KAAF,CAAQC,MAAR,GAAiB,CAAlC,EACKC,MADL,CACY,CAACC,KAAD,EAAQC,IAAR,KAAiBD,KAAK,GAAGC,IADrC,CAAtB;AAEA,UAAMC,IAAI,GAAGb,OAAO,CAChBN,OAAO,CACHS,CADG,EAEH,CACEI,aADF,EACiBJ,CAAC,CAACK,KAAF,CAAQL,CAAC,CAACK,KAAF,CAAQC,MAAR,GAAiB,CAAzB,CADjB,EAEEN,CAAC,CAACK,KAAF,CAAQL,CAAC,CAACK,KAAF,CAAQC,MAAR,GAAiB,CAAzB,CAFF,CAFG,CADS,EAOhB,CAPgB,CAApB;AAQA,UAAMK,IAAI,GAAe,EAAzB;AACA,UAAMC,IAAI,GAAe,EAAzB;AACAF,QAAI,CAACG,OAAL,CAAaC,GAAG,IAAG;AACjB,YAAM,CAACC,GAAD,EAAMC,GAAN,IAAab,IAAI,CAACW,GAAD,EAAkBb,YAAlB,CAAvB;AACAU,UAAI,CAACM,IAAL,CAAUF,GAAV;AACAH,UAAI,CAACK,IAAL,CAAUD,GAAV;AACD,KAJD;AAKA,UAAME,CAAC,GAAG3B,OAAO,CAACE,KAAK,CAACkB,IAAD,EAAO,CAAP,CAAN,EAAiBX,CAAC,CAACK,KAAnB,CAAjB;AACA,UAAMc,CAAC,GAAG5B,OAAO,CAACE,KAAK,CAACmB,IAAD,EAAO,CAAP,CAAN,EAAiBZ,CAAC,CAACK,KAAnB,CAAjB;AACA,WAAO,CAACa,CAAD,EAAIC,CAAJ,CAAP;AACD;AACF;;AAED,SAAShB,IAAT,CAAcH,CAAd,EAA+C;AAAA,MAApBC,YAAoB,uEAAL,KAAK;AAC7C,SAAOvB,MAAM,CAAC0C,IAAP,CAAY,MAAK;AACtBxC,UAAM,CACFoB,CAAC,CAACK,KAAF,CAAQC,MAAR,KAAmB,CADjB,EAEF,MAAM,0CACFN,CAAC,CAACK,KAAF,CAAQC,MAAM,WAHhB,CAAN;AAKA,UAAMe,CAAC,GAAGrB,CAAC,CAACK,KAAF,CAAQ,CAAR,CAAV;AACA,UAAMiB,CAAC,GAAGtB,CAAC,CAACK,KAAF,CAAQ,CAAR,CAAV;AAEA,QAAIa,CAAC,GAAGlC,GAAG,CAACqC,CAAD,CAAX,CATsB,CASH;;AACnB,QAAIF,CAAC,GAAGtC,KAAK,CAACmB,CAAD,CAAb,CAVsB,CAUH;;AAEnB,UAAMuB,KAAK,GAAG5B,QAAQ,CAAC,CAAC,CAAC,CAAD,CAAD,CAAD,EAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,CAAtB;AACA,QAAI6B,CAAC,GAAa3C,KAAK,CAAC0C,KAAD,CAAvB;AAEA,UAAME,KAAK,GAAGJ,CAAC,IAAIC,CAAL,GAASA,CAAT,GAAaD,CAA3B;;AACA,SAAK,IAAIK,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGD,KAApB,EAA2B,EAAEC,CAA7B,EAAgC;AAC9B;AACA;AACA,YAAMC,KAAK,GAAGR,CAAd;AACA,YAAMS,KAAK,GAAGJ,CAAd;AACA,YAAMK,KAAK,GAAGX,CAAd;AACA,OAACM,CAAD,EAAIL,CAAJ,EAAOD,CAAP,IAAYxC,MAAM,CAAC0C,IAAP,CAAY,MAAqC;AAC3D;AACA,cAAMU,MAAM,GAAGtC,KAAK,CAAC2B,CAAD,EAAI,CAACO,CAAD,EAAIA,CAAJ,CAAJ,EAAY,CAACL,CAAC,GAAGK,CAAL,EAAQ,CAAR,CAAZ,CAApB;AACA,cAAMK,KAAK,GAAG1C,IAAI,CAACyC,MAAD,CAAlB;AACA,cAAME,GAAG,GAAGxC,KAAK,CAAC2B,CAAD,EAAI,CAACO,CAAD,EAAIA,CAAJ,CAAJ,EAAY,CAAC,CAAD,EAAI,CAAJ,CAAZ,CAAjB,CAJ2D,CAM3D;;AACA,cAAMO,CAAC,GAAGnC,KAAK,CAACb,OAAO,CAAC+C,GAAD,EAAM,CAAN,CAAR,EAAkBrC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAF,CAAD,CAAD,CAA1B,EAAoCA,QAAQ,CAAC,CAAC,CAAC,CAAD,CAAD,CAAD,CAA5C,CAAf;AAEA,cAAMuC,EAAE,GAAGxC,GAAG,CAACsC,GAAD,EAAM7C,GAAG,CAAC8C,CAAD,EAAIF,KAAJ,CAAT,CAAd;AACA,cAAMI,IAAI,GAAGpD,GAAG,CAAC+C,MAAD,EAASI,EAAT,CAAhB;;AACA,YAAIC,IAAI,CAAC9B,KAAL,CAAW,CAAX,MAAkB,CAAtB,EAAyB;AACvBmB,WAAC,GAAG3C,KAAK,CAAC0C,KAAD,CAAT;AACD,SAFD,MAEO;AACLC,WAAC,GAAG1C,MAAM,CACN,CACEyC,KADF,EAEE/B,KAAK,CAAC2C,IAAD,EAAO,CAAC,CAAD,EAAI,CAAJ,CAAP,EAAe,CAACA,IAAI,CAAC9B,KAAL,CAAW,CAAX,IAAgB,CAAjB,EAAoB8B,IAAI,CAAC9B,KAAL,CAAW,CAAX,CAApB,CAAf,CAFP,CADM,EAMN,CANM,CAAV;AAOD;;AACD,cAAM+B,GAAG,GAAGhD,GAAG,CAACL,GAAG,CAACG,MAAM,CAAC+C,CAAD,EAAIC,EAAJ,CAAP,EAAgBH,KAAhB,CAAJ,CAAf,CAtB2D,CAwB3D;;AACA,cAAMM,QAAQ,GAAG7C,KAAK,CAAC2B,CAAD,EAAI,CAACO,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAACL,CAAC,GAAGK,CAAL,EAAQJ,CAAR,CAAZ,CAAtB;AACA,cAAMgB,SAAS,GAAanD,GAAG,CAACiD,GAAD,EAAMZ,CAAN,CAA/B;AACA,cAAMe,EAAE,GAAa3C,SAAS,CAAC4B,CAAD,CAA9B;;AACA,YAAIE,CAAC,KAAK,CAAV,EAAa;AACXP,WAAC,GAAGzB,GAAG,CAAC2C,QAAD,EAAWnD,MAAM,CAACoD,SAAD,EAAYpD,MAAM,CAACqD,EAAD,EAAKF,QAAL,CAAlB,CAAjB,CAAP;AACD,SAFD,MAEO;AACL,gBAAMG,SAAS,GACX9C,GAAG,CAAC2C,QAAD,EAAWnD,MAAM,CAACoD,SAAD,EAAYpD,MAAM,CAACqD,EAAD,EAAKF,QAAL,CAAlB,CAAjB,CADP;AAEAlB,WAAC,GAAGrC,MAAM,CAAC,CAACU,KAAK,CAAC2B,CAAD,EAAI,CAAC,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAACO,CAAD,EAAIJ,CAAJ,CAAZ,CAAN,EAA2BkB,SAA3B,CAAD,EAAwC,CAAxC,CAAV;AACD;;AACD,cAAMC,UAAU,GAAa7C,SAAS,CAAC0C,SAAD,CAAtC;AACA,cAAMI,QAAQ,GAAGlD,KAAK,CAAC0B,CAAD,EAAI,CAAC,CAAD,EAAIQ,CAAJ,CAAJ,EAAY,CAACL,CAAD,EAAIH,CAAC,CAACb,KAAF,CAAQ,CAAR,IAAaqB,CAAjB,CAAZ,CAAtB;;AACA,YAAIA,CAAC,KAAK,CAAV,EAAa;AACXR,WAAC,GAAGxB,GAAG,CAACgD,QAAD,EAAWxD,MAAM,CAACA,MAAM,CAACwD,QAAD,EAAWlB,CAAX,CAAP,EAAsBiB,UAAtB,CAAjB,CAAP;AACD,SAFD,MAEO;AACL,gBAAME,SAAS,GACXjD,GAAG,CAACgD,QAAD,EAAWxD,MAAM,CAACA,MAAM,CAACwD,QAAD,EAAWlB,CAAX,CAAP,EAAsBiB,UAAtB,CAAjB,CADP;AAEAvB,WAAC,GAAGpC,MAAM,CAAC,CAACU,KAAK,CAAC0B,CAAD,EAAI,CAAC,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAACG,CAAD,EAAIK,CAAJ,CAAZ,CAAN,EAA2BiB,SAA3B,CAAD,EAAwC,CAAxC,CAAV;AACD;;AACD,eAAO,CAACnB,CAAD,EAAIL,CAAJ,EAAOD,CAAP,CAAP;AACD,OA7CW,CAAZ;AA8CAvC,aAAO,CAAC,CAACgD,KAAD,EAAQC,KAAR,EAAeC,KAAf,CAAD,CAAP;AACD;;AAED,QAAI,CAAC5B,YAAD,IAAiBoB,CAAC,GAAGC,CAAzB,EAA4B;AAC1BJ,OAAC,GAAG1B,KAAK,CAAC0B,CAAD,EAAI,CAAC,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAACG,CAAD,EAAIC,CAAJ,CAAZ,CAAT;AACAH,OAAC,GAAG3B,KAAK,CAAC2B,CAAD,EAAI,CAAC,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAACG,CAAD,EAAIA,CAAJ,CAAZ,CAAT;AACD;;AAED,WAAO,CAACJ,CAAD,EAAIC,CAAJ,CAAP;AACD,GA7EM,CAAP;AA8ED;;AAED,OAAO,MAAMyB,EAAE,GAAGtD,EAAE,CAAC;AAACS;AAAD,CAAD,CAAb","names":["ENGINE","dispose","assert","clone","concat","div","eye","greater","matMul","mul","neg","norm","op","reshape","slice","stack","sub","tensor2d","transpose","unstack","where","qr_","x","fullMatrices","rank","qr2d","outerDimsProd","shape","length","reduce","value","prev","x2ds","q2ds","r2ds","forEach","x2d","q2d","r2d","push","q","r","tidy","m","n","one2D","w","iters","j","rTemp","wTemp","qTemp","rjEnd1","normX","rjj","s","u1","wPre","tau","rjEndAll","tauTimesW","wT","rTimesTau","tawTimesWT","qAllJEnd","qTimesTau","qr"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/linalg/qr.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {ENGINE} from '../../engine';\nimport {dispose} from '../../globals';\nimport {Tensor, Tensor2D} from '../../tensor';\nimport {assert} from '../../util';\n\nimport {clone} from '../clone';\nimport {concat} from '../concat';\nimport {div} from '../div';\nimport {eye} from '../eye';\nimport {greater} from '../greater';\nimport {matMul} from '../mat_mul';\nimport {mul} from '../mul';\nimport {neg} from '../neg';\nimport {norm} from '../norm';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\nimport {slice} from '../slice';\nimport {stack} from '../stack';\nimport {sub} from '../sub';\nimport {tensor2d} from '../tensor2d';\nimport {transpose} from '../transpose';\nimport {unstack} from '../unstack';\nimport {where} from '../where';\n\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n *\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction qr_(x: Tensor, fullMatrices = false): [Tensor, Tensor] {\n  assert(\n      x.rank >= 2,\n      () => `qr() requires input tensor to have a rank >= 2, but got rank ${\n          x.rank}`);\n\n  if (x.rank === 2) {\n    return qr2d(x as Tensor2D, fullMatrices);\n  } else {\n    // Rank > 2.\n    // TODO(cais): Below we split the input into individual 2D tensors,\n    //   perform QR decomposition on them and then stack the results back\n    //   together. We should explore whether this can be parallelized.\n    const outerDimsProd = x.shape.slice(0, x.shape.length - 2)\n                              .reduce((value, prev) => value * prev);\n    const x2ds = unstack(\n        reshape(\n            x,\n            [\n              outerDimsProd, x.shape[x.shape.length - 2],\n              x.shape[x.shape.length - 1]\n            ]),\n        0);\n    const q2ds: Tensor2D[] = [];\n    const r2ds: Tensor2D[] = [];\n    x2ds.forEach(x2d => {\n      const [q2d, r2d] = qr2d(x2d as Tensor2D, fullMatrices);\n      q2ds.push(q2d);\n      r2ds.push(r2d);\n    });\n    const q = reshape(stack(q2ds, 0), x.shape);\n    const r = reshape(stack(r2ds, 0), x.shape);\n    return [q, r];\n  }\n}\n\nfunction qr2d(x: Tensor2D, fullMatrices = false): [Tensor2D, Tensor2D] {\n  return ENGINE.tidy(() => {\n    assert(\n        x.shape.length === 2,\n        () => `qr2d() requires a 2D Tensor, but got a ${\n            x.shape.length}D Tensor.`);\n\n    const m = x.shape[0];\n    const n = x.shape[1];\n\n    let q = eye(m);    // Orthogonal transform so far.\n    let r = clone(x);  // Transformed matrix so far.\n\n    const one2D = tensor2d([[1]], [1, 1]);\n    let w: Tensor2D = clone(one2D);\n\n    const iters = m >= n ? n : m;\n    for (let j = 0; j < iters; ++j) {\n      // This tidy within the for-loop ensures we clean up temporary\n      // tensors as soon as they are no longer needed.\n      const rTemp = r;\n      const wTemp = w;\n      const qTemp = q;\n      [w, r, q] = ENGINE.tidy((): [Tensor2D, Tensor2D, Tensor2D] => {\n        // Find H = I - tau * w * w', to put zeros below R(j, j).\n        const rjEnd1 = slice(r, [j, j], [m - j, 1]);\n        const normX = norm(rjEnd1);\n        const rjj = slice(r, [j, j], [1, 1]);\n\n        // The sign() function returns 0 on 0, which causes division by zero.\n        const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));\n\n        const u1 = sub(rjj, mul(s, normX));\n        const wPre = div(rjEnd1, u1);\n        if (wPre.shape[0] === 1) {\n          w = clone(one2D);\n        } else {\n          w = concat(\n              [\n                one2D,\n                slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]]) as\n                    Tensor2D\n              ],\n              0);\n        }\n        const tau = neg(div(matMul(s, u1), normX)) as Tensor2D;\n\n        // -- R := HR, Q := QH.\n        const rjEndAll = slice(r, [j, 0], [m - j, n]);\n        const tauTimesW: Tensor2D = mul(tau, w);\n        const wT: Tensor2D = transpose(w);\n        if (j === 0) {\n          r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n        } else {\n          const rTimesTau: Tensor2D =\n              sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n          r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);\n        }\n        const tawTimesWT: Tensor2D = transpose(tauTimesW);\n        const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);\n        if (j === 0) {\n          q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n        } else {\n          const qTimesTau: Tensor2D =\n              sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n          q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);\n        }\n        return [w, r, q];\n      });\n      dispose([rTemp, wTemp, qTemp]);\n    }\n\n    if (!fullMatrices && m > n) {\n      q = slice(q, [0, 0], [m, n]);\n      r = slice(r, [0, 0], [n, n]);\n    }\n\n    return [q, r];\n  }) as [Tensor2D, Tensor2D];\n}\n\nexport const qr = op({qr_});\n"]},"metadata":{},"sourceType":"module"}