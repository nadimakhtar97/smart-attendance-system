{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * deeplearn.js backend.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from '../common';\nimport { NotImplementedError, ValueError } from '../errors';\nimport * as math_utils from '../utils/math_utils';\nimport { imageDataFormat } from './common'; // tslint:enable\n\n/* Setting and getting backend from deeplearn.js. */\n// Default deeplearn.js backend is WebGL (GPU).\n\nlet backend = 'webgl';\nexport function setBackend(requestedBackend) {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\nexport function getBackend() {\n  return backend;\n}\n/**\n * Indicates whether the backend is operating symbolically.\n *\n * This function will be used to determine how to interpret user code. If\n * it returns true, calls to the backend construct a symbolic graph; if\n * it returns false, calls to the backend execute immediately.\n */\n\nexport function isBackendSymbolic() {\n  return false;\n}\n/**\n * Get the number of elements in a Tensor.\n * @param x The Tensor.\n * @return Number of elements in `x`.\n */\n\nexport function countParams(x) {\n  const shape = x.shape;\n\n  if (shape.length > 0) {\n    return shape.reduce((a, b) => a * b);\n  } else {\n    // Scalar.\n    return 1;\n  }\n}\n/**\n * Casts a tensor to a different dtype and returns it.\n * @param x Input tensor.\n * @param dtype String: 'float32'|'int32'|'bool'.\n * @returns Tensor of the specified `dtype`.\n */\n\nexport function cast(x, dtype) {\n  return tfc.cast(x, dtype);\n}\n/**\n * Adds a 1-sized dimension at index \"axis\".\n * @param x Input tensor.\n * @param axis Position where to add the new axis.\n * @returns Result of the dimension expansion.\n */\n\nexport function expandDims(x) {\n  let axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  const outShape = x.shape.slice();\n\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n\n  outShape.splice(axis, 0, 1);\n  return tfc.reshape(x, outShape);\n}\n/**\n * Repeats a 2D tensor.\n *\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\n * will have shape `[samples, 2, dim]`.\n *\n * @param x Input tensor.\n * @param n Integer, number of times to repeat.\n * @returns The result of the repeat operation.\n * @throws ValueError: If input tensor is not 2D.\n */\n\nexport function repeat(x, n) {\n  return tidy(() => {\n    if (x.shape.length !== 2) {\n      throw new ValueError(`repeat() expects a rank-2 tensor, but received a ` + `rank-${x.shape.length} tensor.`);\n    }\n\n    const y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\n/**\n * Flatten a Tensor into 1D.\n * @param x Input tensor.\n * @return The result of the flattening `x`.\n */\n\nexport function flatten(x) {\n  const newShape = [math_utils.arrayProd(x.shape)];\n  return tfc.reshape(x, newShape);\n}\n/**\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\n * In other words, it flattens each data samples of a batch.\n *\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\n *   or higher.\n * @return The result of the flattening.\n */\n\nexport function batchFlatten(x) {\n  if (x.rank <= 1) {\n    throw new ValueError(`batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n  }\n\n  const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return tfc.reshape(x, newShape);\n}\n/**\n * Do slicing along the first axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the first axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\n\nexport function sliceAlongFirstAxis(array, start, size) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n\n      case 3:\n        return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n\n      case 4:\n        return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n\n      case 5:\n        return tfc.slice(array, [start, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]]);\n\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4], array.shape[5]]);\n\n      default:\n        throw new ValueError(`sliceAlongFirstAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\n * Do slicing along the last axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the last axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\n\nexport function sliceAlongLastAxis(array, start, size) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n\n      case 3:\n        return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n\n      case 4:\n        return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n\n      default:\n        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\n * Do slicing along the sepcified axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size of the slice along the chosen axis.\n * @param choose an axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\n\nexport function sliceAlongAxis(array, start, size, axis) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n\n          case 3:\n            return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n\n      default:\n        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\n * Concatenates a list of tensors alongside the specified axis.\n * @param tensors `Array` of tensors to concatenate.\n * @param axis Concatenation axis.\n * @returns The result of the concatenation.\n */\n\nexport function concatenate(tensors) {\n  let axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  let rank;\n\n  if (axis < 0) {\n    rank = tensors[0].rank;\n\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n\n  if (axis === tensors[0].rank) {\n    // Porting Note: This is necessary because tfc.concat() requires axis to be\n    //   in the interval [-rank, rank).\n    axis = -1;\n  } // Porting Note: Sparse concat is not supported yet.\n\n\n  return tfc.concat(tensors, axis);\n}\n/**\n * Concatenate two arrays along the first dimension.\n * @param a The 1st `tf.Tensor` to concatenate.\n * @param b The 2nd `tf.Tensor` to concatenate.\n * @returns Result of the concatenation.\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\n */\n\nexport function concatAlongFirstAxis(a, b) {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a, b]);\n\n    case 2:\n      return tfc.concat2d([a, b], 0);\n\n    case 3:\n      return tfc.concat3d([a, b], 0);\n\n    case 4:\n      return tfc.concat4d([a, b], 0);\n\n    default:\n      throw new ValueError(`concatAlongFirstAxis() received an unsupported ` + `tensor rank: ${a.rank}`);\n  }\n}\n/**\n * Creates a tensor by tiling `x` by `n`.\n * @param x A tensor.\n * @param n An Array of integers or a single integer. If an Array, the length\n *   must be the same as the number of dimensions in `x`. If a single integer,\n *   it will be treated as an Array of length 1.\n */\n\nexport function tile(x, n) {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n\n  if (x.rank !== n.length) {\n    throw new ValueError(`The length of input n (${n.length}) does not match ` + `the number of dimensions in input x (${x.rank})`);\n  }\n\n  return tfc.tile(x, n);\n}\n/* Creation of random tensors. */\n\n/**\n * Get a tensor with normal distribution of values.\n *\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @return The normal tensor.\n */\n\nexport function randomNormal(shape) {\n  let mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  let stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  let dtype = arguments.length > 3 ? arguments[3] : undefined;\n  let seed = arguments.length > 4 ? arguments[4] : undefined;\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n/* Linear Algebra */\n\n/**\n * Multiply two tensors and returns the result as a tensor.\n *\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\n * For tensors of higher ranks, it follows the Theano behavior,\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\n *\n * For N dimensions it is a sum product over the last axis of x and the\n * second-to-last of y:\n *\n * @param a A tensor of at least rank 2.\n * @param b A tensor of at least rank 2.\n * @param activation (optional) A string identifying the activation\n *   function.\n * @return Result of the dot operation.\n */\n\nexport function dot(a, b, activation, bias) {\n  if (a.rank < 2 || b.rank < 2) {\n    throw new NotImplementedError(`dot requires both inputs to be rank >= 2` + ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n  }\n\n  if (b.rank >= 3) {\n    const xLastDim = a.shape.slice(-1)[0];\n    const ySecondLastDim = b.shape.slice(-2)[0];\n\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(`If rank y >= 3, then the second last dim` + ` of y must equal the last dim of x but got x shape = ${a.shape} and ` + ` y shape = ${b.shape}`);\n    }\n  } // Handle basic 2D x 2D case.\n\n\n  if (a.rank === 2 && b.rank === 2) {\n    const transposeA = false;\n    const transposeB = false; // tfc.fused.matMul only fuses certain activation functions. Unsupported\n    // activation functions are treated as 'linear' activations, which is\n    // equivalent to a no-op.\n\n    return tfc.fused.matMul({\n      a,\n      b: b,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    });\n  } else {\n    // Reshape x into the analogous 2D Tensor.\n    const aFirstDims = a.shape.slice(); // Holds all but the last dim of x.\n\n    const aLastDim = aFirstDims.pop();\n    a = tfc.reshape(a, [-1, aLastDim]); // Reshape y into the analogous 2D Tensor, and keep track of the\n    // required dimensions to reproduce the output shape.\n\n    const bShape = b.shape.slice();\n    const bLastDim = bShape.pop();\n    const ySecondLastDim = bShape.pop();\n    const yOtherDims = [...bShape, bLastDim]; // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n    // where r is the rank of y.\n\n    const perm = Array.from({\n      length: b.rank\n    }, (_, i) => {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n\n      return i;\n    });\n    b = tfc.reshape(tfc.transpose(b, perm), [ySecondLastDim, -1]); // Multiply x and y as 2D Tensors, and then reshape back to original.\n\n    const outputShape = [...aFirstDims, ...yOtherDims];\n    const transposeA = false;\n    const transposeB = false;\n    return tfc.reshape(tfc.fused.matMul({\n      a,\n      b,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    }), outputShape);\n  }\n}\n/**\n * Compute the sign Tensor of an input Tensor.\n *\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\n *\n * @param x Input `tf.Tensor`.\n * @return The sign `tf.Tensor`.\n */\n\nexport function sign(x) {\n  // TODO(cais): Move to the core.\n  return tidy(() => {\n    const zerosLikeX = coreZerosLike(x);\n    const onesLikeX = coreOnesLike(x);\n    return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n  });\n}\n/**\n * Computes the one-hot representation of an integer tensor.\n * @param indices nD integer tensor of shape\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\n * @param numClasses Integer, number of classes to consider.\n * @returns (n + 1)D one hot representation of the input\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n */\n\nexport function oneHot(indices, numClasses) {\n  return tidy(() => {\n    if (indices.rank !== 1) {\n      throw new Error('Only 1D one-hot tensors are supported in the ' + 'deeplearn backend, at present.');\n    }\n\n    indices = tfc.cast(indices, 'int32');\n    return tfc.cast(tfc.oneHot(indices, numClasses), 'float32');\n  });\n}\n/* Elementary math functions. */\n\n/**\n * Retrieves the elements of indices `indices` in the tensor `reference`.\n * @param reference A tensor.\n * @param indices An integer tensor of indices or an `Array` of integers.\n * @param axis Axis along which to perform the gather operation.\n * @returns The result of the gathering as a tensor.\n */\n\nexport function gather(reference, indices, axis) {\n  return tidy(() => {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = tfc.cast(indices, 'int32');\n    }\n\n    return tfc.gather(reference, indices, axis);\n  });\n}\n/**\n * Element-wise square.\n * @param x Input tensor.\n * @return element-wise x^2\n */\n\nexport function square(x) {\n  return tfc.mul(x, x);\n}\n/**\n * Element-wise exponentiation.\n *\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\n *\n * @param x The base tensor.\n * @param a The exponent, tensor or number. If a number, it is rounded to the\n *   nearest integer and converted to a tensor.\n * @returns A tensor of the same shape as `x`.\n */\n\nexport function pow(x, a) {\n  return tidy(() => {\n    if (typeof a === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(`Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n    }\n\n    return tfc.pow(x, a);\n  });\n}\n/**\n * Reshapes bias tensor according to rank of x.\n */\n\nfunction reshapeBias(xRank, bias, dataFormat) {\n  const biasShape = bias.shape;\n\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(`Unexpected bias dimensions: ${bias.rank}` + `; expected it to be 1 or ${xRank}`);\n  }\n\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n\n  throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n/* Neural-network operations. */\n\n/**\n * Add a bias to a tensor.\n *\n * @param x The tensor to add the bias to.\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\n * @return Result of the bias adding.\n * @throws ValueError: If the rank of `bias` is incorrect.\n */\n\n\nexport function biasAdd(x, bias, dataFormat) {\n  return tidy(() => {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n\n    checkDataFormat(dataFormat);\n    return tfc.add(x, reshapeBias(x.rank, bias, dataFormat));\n  });\n}\n/**\n * Exponential linear unit (ELU).\n * @param x A tensor or variable to compute the activation function for.\n * @param alpha: A scalar, a scaling factor for the negative section.\n * @return Output of the ELU operation.\n */\n\nexport function elu(x) {\n  let alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n\n  // TODO(cais): Add support for alpha values other than 1.\n  if (alpha !== 1) {\n    throw new NotImplementedError(`Support for alpha values other than 1 (${alpha}) is not implemented ` + `yet.`);\n  }\n\n  return tfc.elu(x);\n}\n/**\n * Softsign of a tensor.\n *\n * Defined as x / (abs(x) + 1), element-wise.\n *\n * @param x: Input.\n * @returns Output.\n */\n\nexport function softsign(x) {\n  return tidy(() => tfc.div(x, tfc.add(tfc.abs(x), 1)));\n}\n/**\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\n *\n * @param x input tensor.\n * @param level fraction of the entries in the tensor that will be set to 0.\n * @param noiseShape shape of randomly generated keep/drop flags, must be\n *   broadcastable to the shape of `x`. Optional.\n * @param seed random seed to ensure determinism. Optional.\n * @returns Result of the dropout operation.\n */\n\nexport function dropout(x, level, noiseShape, seed) {\n  return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n/**\n * Element-wise, segment-wise linear approximation of sigmoid.\n *\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n *\n * @param x Input tensor.\n * @returns Output tensor.\n */\n\nexport function hardSigmoid(x) {\n  return tidy(() => {\n    const y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\n/**\n * Invoke `x` in the training phase, and `alt` otherwise.\n *\n * Porting Note: We do not create placeholder tensors for the `training`\n * boolean flag here, because there is no such thing in the TF.js imperative\n * backend.\n *\n * @param x The function to invoke iff `training` is `true`.\n * @param alt The function to invoke iff `training` is `false`.\n * @param training Boolean flag for whether training phase is active.\n * @returns The return value of `x()` if `training` is `true`, or the return\n *   value of `alt()` if `training` is `false`.\n */\n\nexport function inTrainPhase(x, alt) {\n  let training = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return training ? x() : alt();\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAQC,QAAQ,IAAIC,YAApB,EAAkCC,MAAlC,EAA4DC,QAA5D,EAA8GC,IAA9G,EAAoHC,KAApH,EAA2HC,SAAS,IAAIC,aAAxI,QAA4J,uBAA5J;AACA,SAAQC,eAAR,QAA8B,WAA9B;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AAGA,OAAO,KAAKC,UAAZ,MAA4B,qBAA5B;AAEA,SAAQC,eAAR,QAA8B,UAA9B,C,CAEA;;AAEA;AAEA;;AACA,IAAIC,OAAO,GAAkB,OAA7B;AAEA,OAAM,SAAUC,UAAV,CAAqBC,gBAArB,EAAoD;AACxDhB,KAAG,CAACe,UAAJ,CAAeC,gBAAf;AACAF,SAAO,GAAGE,gBAAV;AACD;AAED,OAAM,SAAUC,UAAV,GAAoB;AACxB,SAAOH,OAAP;AACD;AAED;;;;;;;;AAOA,OAAM,SAAUI,iBAAV,GAA2B;AAC/B,SAAO,KAAP;AACD;AAED;;;;;;AAKA,OAAM,SAAUC,WAAV,CAAsBC,CAAtB,EAAiC;AACrC,QAAMC,KAAK,GAAGD,CAAC,CAACC,KAAhB;;AACA,MAAIA,KAAK,CAACC,MAAN,GAAe,CAAnB,EAAsB;AACpB,WAAOD,KAAK,CAACE,MAAN,CAAa,CAACC,CAAD,EAAYC,CAAZ,KAA0BD,CAAC,GAAGC,CAA3C,CAAP;AACD,GAFD,MAEO;AACL;AACA,WAAO,CAAP;AACD;AACF;AAED;;;;;;;AAMA,OAAM,SAAUC,IAAV,CAAeN,CAAf,EAA0BO,KAA1B,EAA6C;AACjD,SAAO3B,GAAG,CAAC0B,IAAJ,CAASN,CAAT,EAAYO,KAAZ,CAAP;AACD;AAED;;;;;;;AAMA,OAAM,SAAUC,UAAV,CAAqBR,CAArB,EAAyC;AAAA,MAATS,IAAS,uEAAF,CAAC,CAAC;AAC7C,QAAMC,QAAQ,GAAGV,CAAC,CAACC,KAAF,CAAQU,KAAR,EAAjB;;AACA,MAAIF,IAAI,GAAG,CAAX,EAAc;AACZA,QAAI,GAAGC,QAAQ,CAACR,MAAT,GAAkBO,IAAlB,GAAyB,CAAhC;AACD;;AACDC,UAAQ,CAACE,MAAT,CAAgBH,IAAhB,EAAsB,CAAtB,EAAyB,CAAzB;AACA,SAAO7B,GAAG,CAACiC,OAAJ,CAAYb,CAAZ,EAAeU,QAAf,CAAP;AACD;AAED;;;;;;;;;;;;AAWA,OAAM,SAAUI,MAAV,CAAiBd,CAAjB,EAA4Be,CAA5B,EAAqC;AACzC,SAAO9B,IAAI,CAAC,MAAK;AACf,QAAIe,CAAC,CAACC,KAAF,CAAQC,MAAR,KAAmB,CAAvB,EAA0B;AACxB,YAAM,IAAIX,UAAJ,CACF,sDACA,QAAQS,CAAC,CAACC,KAAF,CAAQC,MAAM,UAFpB,CAAN;AAGD;;AACD,UAAMc,CAAC,GAAGR,UAAU,CAACR,CAAD,EAAI,CAAJ,CAApB;AACA,WAAOiB,IAAI,CAACD,CAAD,EAAI,CAAC,CAAD,EAAID,CAAJ,EAAO,CAAP,CAAJ,CAAX;AACD,GARU,CAAX;AASD;AAED;;;;;;AAKA,OAAM,SAAUG,OAAV,CAAkBlB,CAAlB,EAA2B;AAC/B,QAAMmB,QAAQ,GAAG,CAAC3B,UAAU,CAAC4B,SAAX,CAAqBpB,CAAC,CAACC,KAAvB,CAAD,CAAjB;AACA,SAAOrB,GAAG,CAACiC,OAAJ,CAAYb,CAAZ,EAAemB,QAAf,CAAP;AACD;AAED;;;;;;;;;AAQA,OAAM,SAAUE,YAAV,CAAuBrB,CAAvB,EAAgC;AACpC,MAAIA,CAAC,CAACsB,IAAF,IAAU,CAAd,EAAiB;AACf,UAAM,IAAI/B,UAAJ,CACF,wDAAwDS,CAAC,CAACsB,IAAI,GAD5D,CAAN;AAED;;AACD,QAAMH,QAAQ,GAAG,CAACnB,CAAC,CAACC,KAAF,CAAQ,CAAR,CAAD,EAAaT,UAAU,CAAC4B,SAAX,CAAqBpB,CAAC,CAACC,KAAvB,EAA8B,CAA9B,CAAb,CAAjB;AACA,SAAOrB,GAAG,CAACiC,OAAJ,CAAYb,CAAZ,EAAemB,QAAf,CAAP;AACD;AAED;;;;;;;;;AAQA,OAAM,SAAUI,mBAAV,CACFC,KADE,EACaC,KADb,EAC4BC,IAD5B,EACwC;AAC5C,SAAOzC,IAAI,CAAC,MAAK;AACf,YAAQuC,KAAK,CAACF,IAAd;AACE,WAAK,CAAL;AACE,eAAO1C,GAAG,CAAC+C,OAAJ,CAAYH,KAAZ,EAA+BC,KAA/B,EAAsCC,IAAtC,CAAP;;AACF,WAAK,CAAL;AACE,eAAO9C,GAAG,CAACgD,OAAJ,CACHJ,KADG,EACgB,CAACC,KAAD,EAAQ,CAAR,CADhB,EAC4B,CAACC,IAAD,EAAOF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAP,CAD5B,CAAP;;AAEF,WAAK,CAAL;AACE,eAAOrB,GAAG,CAACiD,OAAJ,CACHL,KADG,EACgB,CAACC,KAAD,EAAQ,CAAR,EAAW,CAAX,CADhB,EAEH,CAACC,IAAD,EAAOF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAP,EAAuBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvB,CAFG,CAAP;;AAGF,WAAK,CAAL;AACE,eAAOrB,GAAG,CAACkD,OAAJ,CACHN,KADG,EACgB,CAACC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,CADhB,EAEH,CAACC,IAAD,EAAOF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAP,EAAuBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvB,EAAuCuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;AAGF,WAAK,CAAL;AACE,eAAOrB,GAAG,CAAC+B,KAAJ,CAAUa,KAAV,EAA6B,CAACC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,EAAiB,CAAjB,CAA7B,EAAkD,CACvDC,IADuD,EACjDF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADiD,EACjCuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADiC,EACjBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADiB,EACDuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADC,CAAlD,CAAP;;AAGF,WAAK,CAAL;AACE,eAAOrB,GAAG,CAAC+B,KAAJ,CAAUa,KAAV,EAAiB,CAACC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,EAAiB,CAAjB,EAAoB,CAApB,CAAjB,EAAyC,CAC9CC,IAD8C,EACxCF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADwC,EACxBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADwB,EACRuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADQ,EACQuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CADR,EAE9CuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAF8C,CAAzC,CAAP;;AAIF;AACE,cAAM,IAAIV,UAAJ,CACF,gEACA,GAAGiC,KAAK,CAACF,IAAI,EAFX,CAAN;AAxBJ;AA4BD,GA7BU,CAAX;AA8BD;AAED;;;;;;;;;AAQA,OAAM,SAAUS,kBAAV,CACFP,KADE,EACaC,KADb,EAC4BC,IAD5B,EACwC;AAC5C,SAAOzC,IAAI,CAAC,MAAK;AACf,YAAQuC,KAAK,CAACF,IAAd;AACE,WAAK,CAAL;AACE,eAAO1C,GAAG,CAAC+C,OAAJ,CAAYH,KAAZ,EAA+BC,KAA/B,EAAsCC,IAAtC,CAAP;;AACF,WAAK,CAAL;AACE,eAAO9C,GAAG,CAACgD,OAAJ,CACHJ,KADG,EACgB,CAAC,CAAD,EAAIC,KAAJ,CADhB,EAC4B,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiByB,IAAjB,CAD5B,CAAP;;AAEF,WAAK,CAAL;AACE,eAAO9C,GAAG,CAACiD,OAAJ,CACHL,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAOC,KAAP,CADhB,EAEH,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAjB,EAAiCyB,IAAjC,CAFG,CAAP;;AAGF,WAAK,CAAL;AACE,eAAO9C,GAAG,CAACkD,OAAJ,CACHN,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAUC,KAAV,CADhB,EAEH,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAjB,EAAiCuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAjC,EAAiDyB,IAAjD,CAFG,CAAP;;AAGF;AACE,cAAM,IAAInC,UAAJ,CACF,+DACA,GAAGiC,KAAK,CAACF,IAAI,EAFX,CAAN;AAfJ;AAmBD,GApBU,CAAX;AAqBD;AAED;;;;;;;;;;AASA,OAAM,SAAUU,cAAV,CACFR,KADE,EACaC,KADb,EAC4BC,IAD5B,EAC0CjB,IAD1C,EACsD;AAC1D,SAAOxB,IAAI,CAAC,MAAK;AACf,YAAQuC,KAAK,CAACF,IAAd;AACE,WAAK,CAAL;AACE,eAAO1C,GAAG,CAAC+C,OAAJ,CAAYH,KAAZ,EAA+BC,KAA/B,EAAsCC,IAAtC,CAAP;;AACF,WAAK,CAAL;AACE,gBAAQjB,IAAR;AACE,eAAK,CAAL;AACE,mBAAOc,mBAAmB,CAACC,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAA1B;;AACF,eAAK,CAAL;AACE,mBAAOK,kBAAkB,CAACP,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAAzB;;AACF;AACE,kBAAM,IAAInC,UAAJ,CACF,mDACA,GAAGkB,IAAI,EAFL,CAAN;AANJ;;AAUF,WAAK,CAAL;AACE,gBAAQA,IAAR;AACE,eAAK,CAAL;AACE,mBAAOc,mBAAmB,CAACC,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAA1B;;AACF,eAAK,CAAL;AACE,mBAAO9C,GAAG,CAACiD,OAAJ,CACHL,KADG,EACgB,CAAC,CAAD,EAAIC,KAAJ,EAAW,CAAX,CADhB,EAEH,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiByB,IAAjB,EAAuBF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvB,CAFG,CAAP;;AAGF,eAAK,CAAL;AACE,mBAAO8B,kBAAkB,CAACP,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAAzB;;AACF;AACE,kBAAM,IAAInC,UAAJ,CACF,mDACA,GAAGkB,IAAI,EAFL,CAAN;AAVJ;;AAcF,WAAK,CAAL;AACE,gBAAQA,IAAR;AACE,eAAK,CAAL;AACE,mBAAOc,mBAAmB,CAACC,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAA1B;;AACF,eAAK,CAAL;AACE,mBAAO9C,GAAG,CAACkD,OAAJ,CACHN,KADG,EACgB,CAAC,CAAD,EAAIC,KAAJ,EAAW,CAAX,EAAc,CAAd,CADhB,EAEH,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiByB,IAAjB,EAAuBF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvB,EAAuCuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;AAGF,eAAK,CAAL;AACE,mBAAOrB,GAAG,CAACkD,OAAJ,CACHN,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAOC,KAAP,EAAc,CAAd,CADhB,EAEH,CAACD,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAD,EAAiBuB,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAjB,EAAiCyB,IAAjC,EAAuCF,KAAK,CAACvB,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;AAGF,eAAK,CAAL;AACE,mBAAO8B,kBAAkB,CAACP,KAAD,EAAQC,KAAR,EAAeC,IAAf,CAAzB;;AACF;AACE,kBAAM,IAAInC,UAAJ,CACF,mDACA,GAAGkB,IAAI,EAFL,CAAN;AAdJ;;AAkBF;AACE,cAAM,IAAIlB,UAAJ,CACF,+DACA,GAAGiC,KAAK,CAACF,IAAI,EAFX,CAAN;AAjDJ;AAqDD,GAtDU,CAAX;AAuDD;AAED;;;;;;;AAMA,OAAM,SAAUW,WAAV,CAAsBC,OAAtB,EAAkD;AAAA,MAATzB,IAAS,uEAAF,CAAC,CAAC;AACtD,MAAIa,IAAJ;;AACA,MAAIb,IAAI,GAAG,CAAX,EAAc;AACZa,QAAI,GAAGY,OAAO,CAAC,CAAD,CAAP,CAAWZ,IAAlB;;AACA,QAAIA,IAAI,KAAK,CAAb,EAAgB;AACdb,UAAI,GAAGa,IAAP;AACD,KAFD,MAEO;AACLb,UAAI,GAAG,CAAP;AACD;AACF;;AACD,MAAIA,IAAI,KAAKyB,OAAO,CAAC,CAAD,CAAP,CAAWZ,IAAxB,EAA8B;AAC5B;AACA;AACAb,QAAI,GAAG,CAAC,CAAR;AACD,GAdqD,CAetD;;;AACA,SAAO7B,GAAG,CAACuD,MAAJ,CAAWD,OAAX,EAAoBzB,IAApB,CAAP;AACD;AAED;;;;;;;;AAOA,OAAM,SAAU2B,oBAAV,CAA+BhC,CAA/B,EAA0CC,CAA1C,EAAmD;AACvD,UAAQD,CAAC,CAACkB,IAAV;AACE,SAAK,CAAL;AACE,aAAO1C,GAAG,CAACyD,QAAJ,CAAa,CAACjC,CAAD,EAAgBC,CAAhB,CAAb,CAAP;;AACF,SAAK,CAAL;AACE,aAAOzB,GAAG,CAAC0D,QAAJ,CAAa,CAAClC,CAAD,EAAgBC,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;AACF,SAAK,CAAL;AACE,aAAOzB,GAAG,CAAC2D,QAAJ,CAAa,CAACnC,CAAD,EAAgBC,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;AACF,SAAK,CAAL;AACE,aAAOzB,GAAG,CAAC4D,QAAJ,CAAa,CAACpC,CAAD,EAAgBC,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;AACF;AACE,YAAM,IAAId,UAAJ,CACF,oDACA,gBAAgBa,CAAC,CAACkB,IAAI,EAFpB,CAAN;AAVJ;AAcD;AAED;;;;;;;;AAOA,OAAM,SAAUL,IAAV,CAAejB,CAAf,EAA0Be,CAA1B,EAA4C;AAChD,MAAI,CAAC0B,KAAK,CAACC,OAAN,CAAc3B,CAAd,CAAL,EAAuB;AACrBA,KAAC,GAAG,CAACA,CAAD,CAAJ;AACD;;AACD,MAAIf,CAAC,CAACsB,IAAF,KAAWP,CAAC,CAACb,MAAjB,EAAyB;AACvB,UAAM,IAAIX,UAAJ,CACF,0BAA0BwB,CAAC,CAACb,MAAM,mBAAlC,GACA,wCAAwCF,CAAC,CAACsB,IAAI,GAF5C,CAAN;AAGD;;AACD,SAAO1C,GAAG,CAACqC,IAAJ,CAASjB,CAAT,EAAYe,CAAZ,CAAP;AACD;AAED;;AAEA;;;;;;;;;;;AAUA,OAAM,SAAU4B,YAAV,CACF1C,KADE,EAEW;AAAA,MADC2C,IACD,uEADQ,GACR;AAAA,MADaC,MACb,uEADsB,GACtB;AAAA,MAD2BtC,KAC3B;AAAA,MAAbuC,IAAa;AACf,SAAOlE,GAAG,CAAC+D,YAAJ,CAAiB1C,KAAjB,EAAwB2C,IAAxB,EAA8BC,MAA9B,EAAsCtC,KAAtC,EAA6CuC,IAA7C,CAAP;AACD;AAED;;AAEA;;;;;;;;;;;;;;;;;AAgBA,OAAM,SAAUC,GAAV,CACF3C,CADE,EACSC,CADT,EACoB2C,UADpB,EAEFC,IAFE,EAEW;AACf,MAAK7C,CAAC,CAACkB,IAAF,GAAS,CAAV,IAAiBjB,CAAC,CAACiB,IAAF,GAAS,CAA9B,EAAkC;AAChC,UAAM,IAAIhC,mBAAJ,CACF,6CACA,sBAAsBc,CAAC,CAACH,KAAK,kBAAkBI,CAAC,CAACJ,KAAK,EAFpD,CAAN;AAGD;;AACD,MAAII,CAAC,CAACiB,IAAF,IAAU,CAAd,EAAiB;AACf,UAAM4B,QAAQ,GAAG9C,CAAC,CAACH,KAAF,CAAQU,KAAR,CAAc,CAAC,CAAf,EAAkB,CAAlB,CAAjB;AACA,UAAMwC,cAAc,GAAG9C,CAAC,CAACJ,KAAF,CAAQU,KAAR,CAAc,CAAC,CAAf,EAAkB,CAAlB,CAAvB;;AACA,QAAIuC,QAAQ,KAAKC,cAAjB,EAAiC;AAC/B,YAAM,IAAI7D,mBAAJ,CACF,6CACA,wDACIc,CAAC,CAACH,KAAK,OAFX,GAGA,cAAcI,CAAC,CAACJ,KAAK,EAJnB,CAAN;AAKD;AACF,GAhBc,CAiBf;;;AACA,MAAKG,CAAC,CAACkB,IAAF,KAAW,CAAZ,IAAmBjB,CAAC,CAACiB,IAAF,KAAW,CAAlC,EAAsC;AACpC,UAAM8B,UAAU,GAAG,KAAnB;AACA,UAAMC,UAAU,GAAG,KAAnB,CAFoC,CAGpC;AACA;AACA;;AACA,WAAOzE,GAAG,CAAC0E,KAAJ,CAAUC,MAAV,CAAiB;AACtBnD,OADsB;AAEtBC,OAAC,EAAEA,CAFmB;AAGtB+C,gBAHsB;AAItBC,gBAJsB;AAKtBJ,UAAI,EAAEA,IAAI,GAAGO,WAAW,CAACpD,CAAC,CAACkB,IAAH,EAAS2B,IAAT,EAAexD,eAAe,EAA9B,CAAd,GAAkD,IALtC;AAMtBuD;AANsB,KAAjB,CAAP;AAQD,GAdD,MAcO;AACL;AACA,UAAMS,UAAU,GAAGrD,CAAC,CAACH,KAAF,CAAQU,KAAR,EAAnB,CAFK,CAEgC;;AACrC,UAAM+C,QAAQ,GAAGD,UAAU,CAACE,GAAX,EAAjB;AACAvD,KAAC,GAAGxB,GAAG,CAACiC,OAAJ,CAAYT,CAAZ,EAAe,CAAC,CAAC,CAAF,EAAKsD,QAAL,CAAf,CAAJ,CAJK,CAML;AACA;;AACA,UAAME,MAAM,GAAGvD,CAAC,CAACJ,KAAF,CAAQU,KAAR,EAAf;AACA,UAAMkD,QAAQ,GAAGD,MAAM,CAACD,GAAP,EAAjB;AACA,UAAMR,cAAc,GAAGS,MAAM,CAACD,GAAP,EAAvB;AACA,UAAMG,UAAU,GAAG,CAAC,GAAGF,MAAJ,EAAYC,QAAZ,CAAnB,CAXK,CAYL;AACA;;AACA,UAAME,IAAI,GAAGtB,KAAK,CAACuB,IAAN,CAAW;AAAC9D,YAAM,EAAEG,CAAC,CAACiB;AAAX,KAAX,EAA6B,CAAC2C,CAAD,EAAIC,CAAJ,KAAS;AACjD,UAAIA,CAAC,KAAK,CAAV,EAAa;AACX,eAAO7D,CAAC,CAACiB,IAAF,GAAS,CAAhB;AACD,OAFD,MAEO,IAAI4C,CAAC,IAAI7D,CAAC,CAACiB,IAAF,GAAS,CAAlB,EAAqB;AAC1B,eAAO4C,CAAC,GAAG,CAAX;AACD;;AACD,aAAOA,CAAP;AACD,KAPY,CAAb;AAQA7D,KAAC,GAAGzB,GAAG,CAACiC,OAAJ,CAAYjC,GAAG,CAACuF,SAAJ,CAAc9D,CAAd,EAAiB0D,IAAjB,CAAZ,EAAoC,CAACZ,cAAD,EAAiB,CAAC,CAAlB,CAApC,CAAJ,CAtBK,CAwBL;;AACA,UAAMiB,WAAW,GAAG,CAAC,GAAGX,UAAJ,EAAgB,GAAGK,UAAnB,CAApB;AACA,UAAMV,UAAU,GAAG,KAAnB;AACA,UAAMC,UAAU,GAAG,KAAnB;AACA,WAAOzE,GAAG,CAACiC,OAAJ,CACHjC,GAAG,CAAC0E,KAAJ,CAAUC,MAAV,CAAiB;AACfnD,OADe;AAEfC,OAFe;AAGf+C,gBAHe;AAIfC,gBAJe;AAKfJ,UAAI,EAAEA,IAAI,GAAGO,WAAW,CAACpD,CAAC,CAACkB,IAAH,EAAS2B,IAAT,EAAexD,eAAe,EAA9B,CAAd,GAAkD,IAL7C;AAMfuD;AANe,KAAjB,CADG,EASHoB,WATG,CAAP;AAUD;AACF;AAED;;;;;;;;;;;AAUA,OAAM,SAAUC,IAAV,CAAerE,CAAf,EAAwB;AAC5B;AACA,SAAOf,IAAI,CAAC,MAAK;AACf,UAAMqF,UAAU,GAAGlF,aAAa,CAACY,CAAD,CAAhC;AACA,UAAMuE,SAAS,GAAGzF,YAAY,CAACkB,CAAD,CAA9B;AACA,WAAOd,KAAK,CACRN,GAAG,CAAC4F,KAAJ,CAAUxE,CAAV,EAAasE,UAAb,CADQ,EACkBA,UADlB,EAERpF,KAAK,CACDN,GAAG,CAAC6F,OAAJ,CAAYzE,CAAZ,EAAeZ,aAAa,CAACY,CAAD,CAA5B,CADC,EACiCuE,SADjC,EAED3F,GAAG,CAAC8F,GAAJ,CAAQ,CAAC,CAAT,EAAYH,SAAZ,CAFC,CAFG,CAAZ;AAKD,GARU,CAAX;AASD;AAED;;;;;;;;;AAQA,OAAM,SAAUI,MAAV,CAAiBC,OAAjB,EAAkCC,UAAlC,EAAoD;AACxD,SAAO5F,IAAI,CAAC,MAAK;AACf,QAAI2F,OAAO,CAACtD,IAAR,KAAiB,CAArB,EAAwB;AACtB,YAAM,IAAIwD,KAAJ,CACF,kDACA,gCAFE,CAAN;AAGD;;AACDF,WAAO,GAAGhG,GAAG,CAAC0B,IAAJ,CAASsE,OAAT,EAAkB,OAAlB,CAAV;AACA,WAAOhG,GAAG,CAAC0B,IAAJ,CAAS1B,GAAG,CAAC+F,MAAJ,CAAWC,OAAX,EAAgCC,UAAhC,CAAT,EAAsD,SAAtD,CAAP;AACD,GARU,CAAX;AASD;AAED;;AAEA;;;;;;;;AAOA,OAAM,SAAUE,MAAV,CACFC,SADE,EACiBJ,OADjB,EAC6CnE,IAD7C,EAC0D;AAC9D,SAAOxB,IAAI,CAAC,MAAK;AACf,QAAIwD,KAAK,CAACC,OAAN,CAAckC,OAAd,CAAJ,EAA4B;AAC1BA,aAAO,GAAG5F,QAAQ,CAAC4F,OAAD,EAAU,OAAV,CAAlB;AACD,KAFD,MAEO;AACLA,aAAO,GAAGhG,GAAG,CAAC0B,IAAJ,CAASsE,OAAT,EAAkB,OAAlB,CAAV;AACD;;AACD,WAAOhG,GAAG,CAACmG,MAAJ,CAAWC,SAAX,EAAsBJ,OAAtB,EAA+BnE,IAA/B,CAAP;AACD,GAPU,CAAX;AAQD;AAED;;;;;;AAKA,OAAM,SAAUwE,MAAV,CAAiBjF,CAAjB,EAA0B;AAC9B,SAAOpB,GAAG,CAAC8F,GAAJ,CAAQ1E,CAAR,EAAWA,CAAX,CAAP;AACD;AAED;;;;;;;;;;;;;AAYA,OAAM,SAAUkF,GAAV,CAAclF,CAAd,EAAyBI,CAAzB,EAAyC;AAC7C,SAAOnB,IAAI,CAAC,MAAK;AACf,QAAI,OAAQmB,CAAR,KAAe,QAAnB,EAA6B;AAC3BA,OAAC,GAAGrB,MAAM,CAACoG,IAAI,CAACC,KAAL,CAAWhF,CAAX,CAAD,EAAgB,OAAhB,CAAV;AACD;;AACD,QAAIA,CAAC,CAACG,KAAF,KAAY,OAAhB,EAAyB;AACvB,YAAM,IAAIjB,mBAAJ,CACF,oBAAoBc,CAAC,CAACG,KAAK,iCADzB,CAAN;AAED;;AACD,WAAO3B,GAAG,CAACsG,GAAJ,CAAQlF,CAAR,EAAWI,CAAX,CAAP;AACD,GATU,CAAX;AAUD;AAED;;;;AAGA,SAASoD,WAAT,CAAqB6B,KAArB,EAAoCpC,IAApC,EAAkDqC,UAAlD,EAAoE;AAClE,QAAMC,SAAS,GAAGtC,IAAI,CAAChD,KAAvB;;AAEA,MAAIgD,IAAI,CAAC3B,IAAL,KAAc,CAAd,IAAmB2B,IAAI,CAAC3B,IAAL,KAAc+D,KAArC,EAA4C;AAC1C,UAAM,IAAI9F,UAAJ,CACF,+BAA+B0D,IAAI,CAAC3B,IAAI,EAAxC,GACA,4BAA4B+D,KAAK,EAF/B,CAAN;AAGD;;AAED,MAAIA,KAAK,KAAK,CAAd,EAAiB;AACf,QAAIC,UAAU,KAAK,eAAnB,EAAoC;AAClC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,EAAqB,CAArB,EAAwB,CAAxB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CACHoC,IADG,EACG,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkBA,SAAS,CAAC,CAAD,CAA3B,EAAgCA,SAAS,CAAC,CAAD,CAAzC,EAA8CA,SAAS,CAAC,CAAD,CAAvD,CADH,CAAP;AAED;AACF,KAPD,MAOO,IAAID,UAAU,KAAK,cAAnB,EAAmC;AACxC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,CAAV,EAAasC,SAAS,CAAC,CAAD,CAAtB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAId,MAAJ,CAAWoD,SAAX,CAAlB,CAAP;AACD;AACF;AACF,GAfD,MAeO,IAAIF,KAAK,KAAK,CAAd,EAAiB;AACtB,QAAIC,UAAU,KAAK,eAAnB,EAAoC;AAClC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,EAAqB,CAArB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkBA,SAAS,CAAC,CAAD,CAA3B,EAAgCA,SAAS,CAAC,CAAD,CAAzC,CAAlB,CAAP;AACD;AACF,KAND,MAMO,IAAID,UAAU,KAAK,cAAnB,EAAmC;AACxC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAUsC,SAAS,CAAC,CAAD,CAAnB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAId,MAAJ,CAAWoD,SAAX,CAAlB,CAAP;AACD;AACF;AACF,GAdM,MAcA,IAAIF,KAAK,KAAK,CAAd,EAAiB;AACtB,QAAIC,UAAU,KAAK,eAAnB,EAAoC;AAClC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAIsC,SAAS,CAAC,CAAD,CAAb,EAAkBA,SAAS,CAAC,CAAD,CAA3B,CAAlB,CAAP;AACD;AACF,KAND,MAMO,IAAID,UAAU,KAAK,cAAnB,EAAmC;AACxC,UAAIC,SAAS,CAACrF,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,eAAOtB,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAI,CAAJ,EAAOsC,SAAS,CAAC,CAAD,CAAhB,CAAlB,CAAP;AACD,OAFD,MAEO;AACL,eAAO3G,GAAG,CAACiC,OAAJ,CAAYoC,IAAZ,EAAkB,CAAC,CAAD,EAAId,MAAJ,CAAWoD,SAAX,CAAlB,CAAP;AACD;AACF;AACF,GAdM,MAcA,IAAIF,KAAK,GAAG,CAAZ,EAAe;AACpB,WAAOpC,IAAP;AACD;;AACD,QAAM,IAAI1D,UAAJ,CAAe,sCAAsC0D,IAAI,CAAC3B,IAAI,EAA9D,CAAN;AACD;AAED;;AAEA;;;;;;;;;;AAQA,OAAM,SAAUkE,OAAV,CACFxF,CADE,EACSiD,IADT,EACuBqC,UADvB,EAC8C;AAClD,SAAOrG,IAAI,CAAC,MAAK;AACf,QAAIqG,UAAU,IAAI,IAAlB,EAAwB;AACtBA,gBAAU,GAAG7F,eAAe,EAA5B;AACD;;AACDJ,mBAAe,CAACiG,UAAD,CAAf;AAEA,WAAO1G,GAAG,CAAC6G,GAAJ,CAAQzF,CAAR,EAAWwD,WAAW,CAACxD,CAAC,CAACsB,IAAH,EAAS2B,IAAT,EAAeqC,UAAf,CAAtB,CAAP;AACD,GAPU,CAAX;AAQD;AAED;;;;;;;AAMA,OAAM,SAAUI,GAAV,CAAc1F,CAAd,EAAkC;AAAA,MAAT2F,KAAS,uEAAD,CAAC;;AACtC;AACA,MAAIA,KAAK,KAAK,CAAd,EAAiB;AACf,UAAM,IAAIrG,mBAAJ,CACF,0CAA0CqG,KAAK,uBAA/C,GACA,MAFE,CAAN;AAGD;;AACD,SAAO/G,GAAG,CAAC8G,GAAJ,CAAQ1F,CAAR,CAAP;AACD;AAED;;;;;;;;;AAQA,OAAM,SAAU4F,QAAV,CAAmB5F,CAAnB,EAA4B;AAChC,SAAOf,IAAI,CAAC,MAAML,GAAG,CAACiH,GAAJ,CAAQ7F,CAAR,EAAWpB,GAAG,CAAC6G,GAAJ,CAAQ7G,GAAG,CAACkH,GAAJ,CAAQ9F,CAAR,CAAR,EAAoB,CAApB,CAAX,CAAP,CAAX;AACD;AAED;;;;;;;;;;;AAUA,OAAM,SAAU+F,OAAV,CACF/F,CADE,EACSgG,KADT,EACwBC,UADxB,EAC+CnD,IAD/C,EAC4D;AAChE,SAAO7D,IAAI,CAAC,MAAML,GAAG,CAACmH,OAAJ,CAAY/F,CAAZ,EAAegG,KAAf,EAAsBC,UAAtB,EAAkCnD,IAAlC,CAAP,CAAX;AACD;AAED;;;;;;;;;;AASA,OAAM,SAAUoD,WAAV,CAAsBlG,CAAtB,EAA+B;AACnC,SAAOf,IAAI,CAAC,MAAK;AACf,UAAM+B,CAAC,GAAGpC,GAAG,CAAC6G,GAAJ,CAAQ,EAAR,EAAY7G,GAAG,CAAC8F,GAAJ,CAAQ,EAAR,EAAY1E,CAAZ,CAAZ,CAAV;AACA,WAAOpB,GAAG,CAACuH,WAAJ,CAAgBnF,CAAhB,EAAmB,CAAnB,EAAsB,CAAtB,CAAP;AACD,GAHU,CAAX;AAID;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUoF,YAAV,CAA0BpG,CAA1B,EAAsCqG,GAAtC,EAAoE;AAAA,MAAhBC,QAAgB,uEAAL,KAAK;AACxE,SAAOA,QAAQ,GAAGtG,CAAC,EAAJ,GAASqG,GAAG,EAA3B;AACD","names":["tfc","onesLike","coreOnesLike","scalar","tensor1d","tidy","where","zerosLike","coreZerosLike","checkDataFormat","NotImplementedError","ValueError","math_utils","imageDataFormat","backend","setBackend","requestedBackend","getBackend","isBackendSymbolic","countParams","x","shape","length","reduce","a","b","cast","dtype","expandDims","axis","outShape","slice","splice","reshape","repeat","n","y","tile","flatten","newShape","arrayProd","batchFlatten","rank","sliceAlongFirstAxis","array","start","size","slice1d","slice2d","slice3d","slice4d","sliceAlongLastAxis","sliceAlongAxis","concatenate","tensors","concat","concatAlongFirstAxis","concat1d","concat2d","concat3d","concat4d","Array","isArray","randomNormal","mean","stddev","seed","dot","activation","bias","xLastDim","ySecondLastDim","transposeA","transposeB","fused","matMul","reshapeBias","aFirstDims","aLastDim","pop","bShape","bLastDim","yOtherDims","perm","from","_","i","transpose","outputShape","sign","zerosLikeX","onesLikeX","equal","greater","mul","oneHot","indices","numClasses","Error","gather","reference","square","pow","Math","round","xRank","dataFormat","biasShape","biasAdd","add","elu","alpha","softsign","div","abs","dropout","level","noiseShape","hardSigmoid","clipByValue","inTrainPhase","alt","training"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/backend/tfjs_backend.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * deeplearn.js backend.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {onesLike as coreOnesLike, scalar, Tensor, Tensor1D, tensor1d, Tensor2D, Tensor3D, Tensor4D, Tensor5D, tidy, where, zerosLike as coreZerosLike} from '@tensorflow/tfjs-core';\nimport {checkDataFormat} from '../common';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {DataFormat, Shape} from '../keras_format/common';\nimport {HasShape} from '../types';\nimport * as math_utils from '../utils/math_utils';\n\nimport {imageDataFormat} from './common';\n\n// tslint:enable\n\n/* Setting and getting backend from deeplearn.js. */\n\n// Default deeplearn.js backend is WebGL (GPU).\nlet backend: 'cpu'|'webgl' = 'webgl';\n\nexport function setBackend(requestedBackend: 'cpu'|'webgl') {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\n\nexport function getBackend(): 'cpu'|'webgl' {\n  return backend;\n}\n\n/**\n * Indicates whether the backend is operating symbolically.\n *\n * This function will be used to determine how to interpret user code. If\n * it returns true, calls to the backend construct a symbolic graph; if\n * it returns false, calls to the backend execute immediately.\n */\nexport function isBackendSymbolic(): boolean {\n  return false;\n}\n\n/**\n * Get the number of elements in a Tensor.\n * @param x The Tensor.\n * @return Number of elements in `x`.\n */\nexport function countParams(x: HasShape): number {\n  const shape = x.shape;\n  if (shape.length > 0) {\n    return shape.reduce((a: number, b: number) => a * b);\n  } else {\n    // Scalar.\n    return 1;\n  }\n}\n\n/**\n * Casts a tensor to a different dtype and returns it.\n * @param x Input tensor.\n * @param dtype String: 'float32'|'int32'|'bool'.\n * @returns Tensor of the specified `dtype`.\n */\nexport function cast(x: Tensor, dtype: tfc.DataType): Tensor {\n  return tfc.cast(x, dtype);\n}\n\n/**\n * Adds a 1-sized dimension at index \"axis\".\n * @param x Input tensor.\n * @param axis Position where to add the new axis.\n * @returns Result of the dimension expansion.\n */\nexport function expandDims(x: Tensor, axis = -1): Tensor {\n  const outShape = x.shape.slice();\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n  outShape.splice(axis, 0, 1);\n  return tfc.reshape(x, outShape);\n}\n\n/**\n * Repeats a 2D tensor.\n *\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\n * will have shape `[samples, 2, dim]`.\n *\n * @param x Input tensor.\n * @param n Integer, number of times to repeat.\n * @returns The result of the repeat operation.\n * @throws ValueError: If input tensor is not 2D.\n */\nexport function repeat(x: Tensor, n: number): Tensor {\n  return tidy(() => {\n    if (x.shape.length !== 2) {\n      throw new ValueError(\n          `repeat() expects a rank-2 tensor, but received a ` +\n          `rank-${x.shape.length} tensor.`);\n    }\n    const y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\n\n/**\n * Flatten a Tensor into 1D.\n * @param x Input tensor.\n * @return The result of the flattening `x`.\n */\nexport function flatten(x: Tensor): Tensor {\n  const newShape = [math_utils.arrayProd(x.shape)];\n  return tfc.reshape(x, newShape);\n}\n\n/**\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\n * In other words, it flattens each data samples of a batch.\n *\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\n *   or higher.\n * @return The result of the flattening.\n */\nexport function batchFlatten(x: Tensor): Tensor {\n  if (x.rank <= 1) {\n    throw new ValueError(\n        `batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n  }\n  const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return tfc.reshape(x, newShape);\n}\n\n/**\n * Do slicing along the first axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the first axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongFirstAxis(\n    array: Tensor, start: number, size: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        return tfc.slice2d(\n            array as Tensor2D, [start, 0], [size, array.shape[1]]);\n      case 3:\n        return tfc.slice3d(\n            array as Tensor3D, [start, 0, 0],\n            [size, array.shape[1], array.shape[2]]);\n      case 4:\n        return tfc.slice4d(\n            array as Tensor4D, [start, 0, 0, 0],\n            [size, array.shape[1], array.shape[2], array.shape[3]]);\n      case 5:\n        return tfc.slice(array as Tensor5D, [start, 0, 0, 0, 0], [\n          size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]\n        ]);\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [\n          size, array.shape[1], array.shape[2], array.shape[3], array.shape[4],\n          array.shape[5]\n        ]);\n      default:\n        throw new ValueError(\n            `sliceAlongFirstAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Do slicing along the last axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the last axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongLastAxis(\n    array: Tensor, start: number, size: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        return tfc.slice2d(\n            array as Tensor2D, [0, start], [array.shape[0], size]);\n      case 3:\n        return tfc.slice3d(\n            array as Tensor3D, [0, 0, start],\n            [array.shape[0], array.shape[1], size]);\n      case 4:\n        return tfc.slice4d(\n            array as Tensor4D, [0, 0, 0, start],\n            [array.shape[0], array.shape[1], array.shape[2], size]);\n      default:\n        throw new ValueError(\n            `sliceAlongLastAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Do slicing along the sepcified axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size of the slice along the chosen axis.\n * @param choose an axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongAxis(\n    array: Tensor, start: number, size: number, axis: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice3d(\n                array as Tensor3D, [0, start, 0],\n                [array.shape[0], size, array.shape[2]]);\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice4d(\n                array as Tensor4D, [0, start, 0, 0],\n                [array.shape[0], size, array.shape[2], array.shape[3]]);\n          case 3:\n            return tfc.slice4d(\n                array as Tensor4D, [0, 0, start, 0],\n                [array.shape[0], array.shape[1], size, array.shape[3]]);\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      default:\n        throw new ValueError(\n            `sliceAlongLastAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Concatenates a list of tensors alongside the specified axis.\n * @param tensors `Array` of tensors to concatenate.\n * @param axis Concatenation axis.\n * @returns The result of the concatenation.\n */\nexport function concatenate(tensors: Tensor[], axis = -1): Tensor {\n  let rank: number;\n  if (axis < 0) {\n    rank = tensors[0].rank;\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n  if (axis === tensors[0].rank) {\n    // Porting Note: This is necessary because tfc.concat() requires axis to be\n    //   in the interval [-rank, rank).\n    axis = -1;\n  }\n  // Porting Note: Sparse concat is not supported yet.\n  return tfc.concat(tensors, axis);\n}\n\n/**\n * Concatenate two arrays along the first dimension.\n * @param a The 1st `tf.Tensor` to concatenate.\n * @param b The 2nd `tf.Tensor` to concatenate.\n * @returns Result of the concatenation.\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function concatAlongFirstAxis(a: Tensor, b: Tensor): Tensor {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a as Tensor1D, b as Tensor1D]);\n    case 2:\n      return tfc.concat2d([a as Tensor2D, b as Tensor2D], 0);\n    case 3:\n      return tfc.concat3d([a as Tensor3D, b as Tensor3D], 0);\n    case 4:\n      return tfc.concat4d([a as Tensor4D, b as Tensor4D], 0);\n    default:\n      throw new ValueError(\n          `concatAlongFirstAxis() received an unsupported ` +\n          `tensor rank: ${a.rank}`);\n  }\n}\n\n/**\n * Creates a tensor by tiling `x` by `n`.\n * @param x A tensor.\n * @param n An Array of integers or a single integer. If an Array, the length\n *   must be the same as the number of dimensions in `x`. If a single integer,\n *   it will be treated as an Array of length 1.\n */\nexport function tile(x: Tensor, n: number|number[]): Tensor {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n  if (x.rank !== n.length) {\n    throw new ValueError(\n        `The length of input n (${n.length}) does not match ` +\n        `the number of dimensions in input x (${x.rank})`);\n  }\n  return tfc.tile(x, n);\n}\n\n/* Creation of random tensors. */\n\n/**\n * Get a tensor with normal distribution of values.\n *\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @return The normal tensor.\n */\nexport function randomNormal(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: 'float32'|'int32',\n    seed?: number): Tensor {\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n\n/* Linear Algebra */\n\n/**\n * Multiply two tensors and returns the result as a tensor.\n *\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\n * For tensors of higher ranks, it follows the Theano behavior,\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\n *\n * For N dimensions it is a sum product over the last axis of x and the\n * second-to-last of y:\n *\n * @param a A tensor of at least rank 2.\n * @param b A tensor of at least rank 2.\n * @param activation (optional) A string identifying the activation\n *   function.\n * @return Result of the dot operation.\n */\nexport function dot(\n    a: Tensor, b: Tensor, activation?: tfc.fused.Activation,\n    bias?: Tensor): Tensor {\n  if ((a.rank < 2) || (b.rank < 2)) {\n    throw new NotImplementedError(\n        `dot requires both inputs to be rank >= 2` +\n        ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n  }\n  if (b.rank >= 3) {\n    const xLastDim = a.shape.slice(-1)[0];\n    const ySecondLastDim = b.shape.slice(-2)[0];\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(\n          `If rank y >= 3, then the second last dim` +\n          ` of y must equal the last dim of x but got x shape = ${\n              a.shape} and ` +\n          ` y shape = ${b.shape}`);\n    }\n  }\n  // Handle basic 2D x 2D case.\n  if ((a.rank === 2) && (b.rank === 2)) {\n    const transposeA = false;\n    const transposeB = false;\n    // tfc.fused.matMul only fuses certain activation functions. Unsupported\n    // activation functions are treated as 'linear' activations, which is\n    // equivalent to a no-op.\n    return tfc.fused.matMul({\n      a,\n      b: b as Tensor2D,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    });\n  } else {\n    // Reshape x into the analogous 2D Tensor.\n    const aFirstDims = a.shape.slice();  // Holds all but the last dim of x.\n    const aLastDim = aFirstDims.pop();\n    a = tfc.reshape(a, [-1, aLastDim]);\n\n    // Reshape y into the analogous 2D Tensor, and keep track of the\n    // required dimensions to reproduce the output shape.\n    const bShape = b.shape.slice();\n    const bLastDim = bShape.pop();\n    const ySecondLastDim = bShape.pop();\n    const yOtherDims = [...bShape, bLastDim];\n    // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n    // where r is the rank of y.\n    const perm = Array.from({length: b.rank}, (_, i) => {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n      return i;\n    });\n    b = tfc.reshape(tfc.transpose(b, perm), [ySecondLastDim, -1]);\n\n    // Multiply x and y as 2D Tensors, and then reshape back to original.\n    const outputShape = [...aFirstDims, ...yOtherDims];\n    const transposeA = false;\n    const transposeB = false;\n    return tfc.reshape(\n        tfc.fused.matMul({\n          a,\n          b,\n          transposeA,\n          transposeB,\n          bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n          activation\n        }),\n        outputShape);\n  }\n}\n\n/**\n * Compute the sign Tensor of an input Tensor.\n *\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\n *\n * @param x Input `tf.Tensor`.\n * @return The sign `tf.Tensor`.\n */\nexport function sign(x: Tensor): Tensor {\n  // TODO(cais): Move to the core.\n  return tidy(() => {\n    const zerosLikeX = coreZerosLike(x);\n    const onesLikeX = coreOnesLike(x);\n    return where(\n        tfc.equal(x, zerosLikeX), zerosLikeX,\n        where(\n            tfc.greater(x, coreZerosLike(x)), onesLikeX,\n            tfc.mul(-1, onesLikeX)));\n  });\n}\n\n/**\n * Computes the one-hot representation of an integer tensor.\n * @param indices nD integer tensor of shape\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\n * @param numClasses Integer, number of classes to consider.\n * @returns (n + 1)D one hot representation of the input\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n */\nexport function oneHot(indices: Tensor, numClasses: number): Tensor {\n  return tidy(() => {\n    if (indices.rank !== 1) {\n      throw new Error(\n          'Only 1D one-hot tensors are supported in the ' +\n          'deeplearn backend, at present.');\n    }\n    indices = tfc.cast(indices, 'int32');\n    return tfc.cast(tfc.oneHot(indices as Tensor1D, numClasses), 'float32');\n  });\n}\n\n/* Elementary math functions. */\n\n/**\n * Retrieves the elements of indices `indices` in the tensor `reference`.\n * @param reference A tensor.\n * @param indices An integer tensor of indices or an `Array` of integers.\n * @param axis Axis along which to perform the gather operation.\n * @returns The result of the gathering as a tensor.\n */\nexport function gather(\n    reference: Tensor, indices: number[]|Tensor1D, axis?: number): Tensor {\n  return tidy(() => {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = tfc.cast(indices, 'int32');\n    }\n    return tfc.gather(reference, indices, axis);\n  });\n}\n\n/**\n * Element-wise square.\n * @param x Input tensor.\n * @return element-wise x^2\n */\nexport function square(x: Tensor): Tensor {\n  return tfc.mul(x, x);\n}\n\n/**\n * Element-wise exponentiation.\n *\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\n *\n * @param x The base tensor.\n * @param a The exponent, tensor or number. If a number, it is rounded to the\n *   nearest integer and converted to a tensor.\n * @returns A tensor of the same shape as `x`.\n */\nexport function pow(x: Tensor, a: Tensor|number): Tensor {\n  return tidy(() => {\n    if (typeof (a) === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(\n          `Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n    }\n    return tfc.pow(x, a);\n  });\n}\n\n/**\n * Reshapes bias tensor according to rank of x.\n */\nfunction reshapeBias(xRank: number, bias: Tensor, dataFormat: string) {\n  const biasShape = bias.shape;\n\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(\n        `Unexpected bias dimensions: ${bias.rank}` +\n        `; expected it to be 1 or ${xRank}`);\n  }\n\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1, 1]);\n      } else {\n        return tfc.reshape(\n            bias, [1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n  throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n\n/* Neural-network operations. */\n\n/**\n * Add a bias to a tensor.\n *\n * @param x The tensor to add the bias to.\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\n * @return Result of the bias adding.\n * @throws ValueError: If the rank of `bias` is incorrect.\n */\nexport function biasAdd(\n    x: Tensor, bias: Tensor, dataFormat?: DataFormat): Tensor {\n  return tidy(() => {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n    checkDataFormat(dataFormat);\n\n    return tfc.add(x, reshapeBias(x.rank, bias, dataFormat));\n  });\n}\n\n/**\n * Exponential linear unit (ELU).\n * @param x A tensor or variable to compute the activation function for.\n * @param alpha: A scalar, a scaling factor for the negative section.\n * @return Output of the ELU operation.\n */\nexport function elu(x: Tensor, alpha = 1): Tensor {\n  // TODO(cais): Add support for alpha values other than 1.\n  if (alpha !== 1) {\n    throw new NotImplementedError(\n        `Support for alpha values other than 1 (${alpha}) is not implemented ` +\n        `yet.`);\n  }\n  return tfc.elu(x);\n}\n\n/**\n * Softsign of a tensor.\n *\n * Defined as x / (abs(x) + 1), element-wise.\n *\n * @param x: Input.\n * @returns Output.\n */\nexport function softsign(x: Tensor): Tensor {\n  return tidy(() => tfc.div(x, tfc.add(tfc.abs(x), 1)));\n}\n\n/**\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\n *\n * @param x input tensor.\n * @param level fraction of the entries in the tensor that will be set to 0.\n * @param noiseShape shape of randomly generated keep/drop flags, must be\n *   broadcastable to the shape of `x`. Optional.\n * @param seed random seed to ensure determinism. Optional.\n * @returns Result of the dropout operation.\n */\nexport function dropout(\n    x: Tensor, level: number, noiseShape?: number[], seed?: number): Tensor {\n  return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n\n/**\n * Element-wise, segment-wise linear approximation of sigmoid.\n *\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n *\n * @param x Input tensor.\n * @returns Output tensor.\n */\nexport function hardSigmoid(x: Tensor): Tensor {\n  return tidy(() => {\n    const y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\n\n/**\n * Invoke `x` in the training phase, and `alt` otherwise.\n *\n * Porting Note: We do not create placeholder tensors for the `training`\n * boolean flag here, because there is no such thing in the TF.js imperative\n * backend.\n *\n * @param x The function to invoke iff `training` is `true`.\n * @param alt The function to invoke iff `training` is `false`.\n * @param training Boolean flag for whether training phase is active.\n * @returns The return value of `x()` if `training` is `true`, or the return\n *   value of `alt()` if `training` is `false`.\n */\nexport function inTrainPhase<T>(x: () => T, alt: () => T, training = false): T {\n  return training ? x() : alt();\n}\n"]},"metadata":{},"sourceType":"module"}