{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\n\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs,\n    initialState,\n    constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\n\nexport function rnn(stepFunction, inputs, initialStates) {\n  let goBackwards = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n  let mask = arguments.length > 4 ? arguments[4] : undefined;\n  let constants = arguments.length > 5 ? arguments[5] : undefined;\n  let unroll = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : false;\n  let needPerStepOutputs = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : false;\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n\n    if (ndim < 3) {\n      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n    } // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n\n\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    } // Porting Note: the unroll option is ignored by the imperative backend.\n\n\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    } // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n\n    const perStepOutputs = [];\n    let lastOutput;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks;\n\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask); // TODO(cais): Would tfc.where() be better for performance?\n\n          const output = tfc.add(tfc.mul(stepOutputs[0], stepMask), tfc.mul(states[0], negStepMask));\n          const newStates = states.map((state, i) => {\n            return tfc.add(tfc.mul(stepOutputs[1][i], stepMask), tfc.mul(state, negStepMask));\n          });\n          return {\n            output,\n            newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n\n    let outputs;\n\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n\n    return [lastOutput, outputs, states];\n  });\n}\nexport class RNN extends Layer {\n  constructor(args) {\n    super(args);\n    let cell;\n\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n\n    this.cell = cell;\n    this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    this.stateSpec = null;\n    this.states_ = null; // TODO(cais): Add constantsSpec and numConstants.\n\n    this.numConstants = null; // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    this.keptStates = [];\n  } // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n\n\n  getStates() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  } // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n\n\n  setStates(states) {\n    this.states_ = states;\n  }\n\n  computeOutputShape(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape; // TODO(cais): Remove the casting once stacked RNN cells become supported.\n\n    let stateSize = this.cell.stateSize;\n\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n\n    const outputDim = stateSize[0];\n    let outputShape;\n\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      const stateShape = [];\n\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n\n  computeMask(inputs, mask) {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      const outputMask = this.returnSequences ? mask : null;\n\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n\n\n  get states() {\n    if (this.states_ == null) {\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output = [];\n\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n\n  set states(s) {\n    this.states_ = s;\n  }\n\n  build(inputShape) {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape = null;\n\n    if (this.numConstants != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    }\n\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    const batchSize = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape.slice(2);\n    this.inputSpec[0] = new InputSpec({\n      shape: [batchSize, null, ...inputDim]\n    }); // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (constantShape != null) {\n      throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    } // Set or validate stateSpec.\n\n\n    let stateSize;\n\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]), stateSize)) {\n        throw new ValueError(`An initialState was passed that is not compatible with ` + `cell.stateSize. Received stateSpec=${this.stateSpec}; ` + `However cell.stateSize is ${this.cell.stateSize}`);\n      }\n    } else {\n      this.stateSpec = stateSize.map(dim => new InputSpec({\n        shape: [null, dim]\n      }));\n    }\n\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n\n\n  resetStates(states) {\n    let training = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n\n      const batchSize = this.inputSpec[0].shape[0];\n\n      if (batchSize == null) {\n        throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n      } // Initialize state if null.\n\n\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_); // For stateful RNNs, fully dispose kept old states.\n\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ = this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n\n        if (states.length !== this.states_.length) {\n          throw new ValueError(`Layer ${this.name} expects ${this.states_.length} state(s), ` + `but it received ${states.length} state value(s). Input ` + `received: ${states}`);\n        }\n\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(`State ${index} is incompatible with layer ${this.name}: ` + `expected shape=${expectedShape}, received shape=${value.shape}`);\n          }\n\n          this.states_[index] = value;\n        }\n      }\n\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n\n  apply(inputs, kwargs) {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState = kwargs == null ? null : kwargs['initialState'];\n    let constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants; // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n\n    let additionalInputs = [];\n    let additionalSpecs = [];\n\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({\n          shape: state.shape\n        }));\n      } // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n\n\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants); // TODO(cais): Add this.constantsSpec.\n\n      this.numConstants = constants.length;\n    }\n\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call with temporarily replaced inputSpec.\n\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  } // tslint:disable-next-line:no-any\n\n\n  call(inputs, kwargs) {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState = kwargs == null ? null : kwargs['initialState'];\n      inputs = getExactlyOneTensor(inputs);\n\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n\n      const numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n\n      if (initialState.length !== numStates) {\n        throw new ValueError(`RNN Layer has ${numStates} state(s) but was passed ` + `${initialState.length} initial state(s).`);\n      }\n\n      if (this.unroll) {\n        console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n\n      const cellCallKwargs = {\n        training\n      }; // TODO(cais): Add support for constants.\n\n      const step = (inputs, states) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs = this.cell.call([inputs].concat(states), cellCallKwargs); // Marshall the return value into output and new states.\n\n        return [outputs[0], outputs.slice(1)];\n      }; // TODO(cais): Add support for constants.\n\n\n      const rnnOutputs = rnn(step, inputs, initialState, this.goBackwards, mask, null, this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n\n      const output = this.returnSequences ? outputs : lastOutput; // TODO(cais): Porperty set learning phase flag.\n\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n\n  getInitialState(inputs) {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape); // [Samples].\n\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState); // [Samples, 1].\n\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ? [K.tile(initialState, [1, this.cell.stateSize])] : [initialState];\n      }\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    } // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n\n\n    return this.cell.trainableWeights;\n  }\n\n  get nonTrainableWeights() {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n\n    return this.cell.nonTrainableWeights;\n  }\n\n  setFastWeightInitDuringBuild(value) {\n    super.setFastWeightInitDuringBuild(value);\n\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n\n    const cellConfig = this.cell.getConfig();\n\n    if (this.getClassName() === RNN.className) {\n      config['cell'] = {\n        'className': this.cell.getClassName(),\n        'config': cellConfig\n      };\n    } // this order is necessary, to prevent cell name from replacing layer name\n\n\n    return Object.assign({}, cellConfig, baseConfig, config);\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const cellConfig = config['cell'];\n    const cell = deserialize(cellConfig, customObjects);\n    return new cls(Object.assign(config, {\n      cell\n    }));\n  }\n\n}\n/** @nocollapse */\n\nRNN.className = 'RNN';\nserialization.registerClass(RNN); // Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\n\nexport class RNNCell extends Layer {}\nexport class SimpleRNNCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, `units`);\n    this.activation = getActivation(args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape); // TODO(cais): Use regularizer.\n\n    this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  } // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(`SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n      }\n\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(prevOutput),\n          rate: this.recurrentDropout,\n          training,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      let h;\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      } // TODO(cais): Properly set learning phase on output tensor?\n\n\n      return [output, output];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport class SimpleRNN extends RNN {\n  constructor(args) {\n    args.cell = new SimpleRNNCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport class GRUCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n    if (args.resetAfter) {\n      throw new ValueError(`GRUCell does not support reset_after parameter set to true.`);\n    }\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n\n      if (inputs.length !== 2) {\n        throw new ValueError(`GRUCell expects 2 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      inputs = inputs[0]; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 3,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 3,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask;\n      let z;\n      let r;\n      let hh;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let matrixX = K.dot(inputs, this.kernel.read());\n\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      const recurrentKernelValue = this.recurrentKernel.read();\n      const [rk1, rk2] = tfc.split(recurrentKernelValue, [2 * this.units, this.units], recurrentKernelValue.rank - 1);\n      const matrixInner = K.dot(hTMinus1, rk1);\n      const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n      const [recurrentZ, recurrentR] = tfc.split(matrixInner, 2, matrixInner.rank - 1);\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n      const h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport class GRU extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new GRUCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport class LSTMCell extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.DEFAULT_ACTIVATION = 'tanh';\n    this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation === undefined ? this.DEFAULT_ACTIVATION : args.activation);\n    this.recurrentActivation = getActivation(args.recurrentActivation === undefined ? this.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape) {\n    var _a;\n\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    let biasInitializer;\n\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (_a = class CustomInit extends Initializer {\n          apply(shape, dtype) {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = new Ones().apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n\n        },\n        /** @nocollapse */\n        _a.className = 'CustomInit', _a)();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n\n      this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs;\n\n      if (inputs.length !== 3) {\n        throw new ValueError(`LSTMCell expects 3 input Tensors (inputs, h, c), got ` + `${inputs.length}.`);\n      }\n\n      let hTMinus1 = inputs[1]; // Previous memory state.\n\n      const cTMinus1 = inputs[2]; // Previous carry state.\n\n      inputs = inputs[0];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(inputs),\n          rate: this.dropout,\n          training,\n          count: 4,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 && this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n          ones: () => tfc.onesLike(hTMinus1),\n          rate: this.recurrentDropout,\n          training,\n          count: 4,\n          dropoutFunc: this.dropoutFunc\n        });\n      }\n\n      const dpMask = this.dropoutMask;\n      const recDpMask = this.recurrentDropoutMask; // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n\n      let i;\n      let f;\n      let c;\n      let o;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n\n      let z = K.dot(inputs, this.kernel.read());\n\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n      const h = tfc.mul(o, this.activation.apply(c)); // TODO(cais): Add use_learning_phase flag properly.\n\n      return [h, h, c];\n    });\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n\n}\n/** @nocollapse */\n\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport class LSTM extends RNN {\n  constructor(args) {\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new LSTMCell(args);\n    super(args); // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState = kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {\n        mask,\n        training,\n        initialState\n      });\n    });\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  }\n\n}\n/** @nocollapse */\n\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport class StackedRNNCells extends RNNCell {\n  constructor(args) {\n    super(args);\n    this.cells = args.cells;\n  }\n\n  get stateSize() {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize = [];\n\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n\n    return stateSize;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = inputs;\n      let states = inputs.slice(1); // Recover per-cell states.\n\n      const nestedStates = [];\n\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n\n      nestedStates.reverse(); // Call the cells in order and store the returned states.\n\n      const newNestedStates = [];\n      let callInputs;\n\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i]; // TODO(cais): Take care of constants.\n\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n\n        callInputs = cell.call(callInputs, kwargs);\n        newNestedStates.push(callInputs.slice(1));\n      } // Format the new states as a flat list in reverse cell order.\n\n\n      states = [];\n\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n\n      return [callInputs[0]].concat(states);\n    });\n  }\n\n  build(inputShape) {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    let outputDim;\n    this.cells.forEach((cell, i) => {\n      nameScope(`RNNCell_${i}`, () => {\n        // TODO(cais): Take care of input constants.\n        cell.build(inputShape);\n\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n\n        inputShape = [inputShape[0], outputDim];\n      });\n    });\n    this.built = true;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n\n    const getCellConfig = cell => {\n      return {\n        'className': cell.getClassName(),\n        'config': cell.getConfig()\n      };\n    };\n\n    const cellConfigs = this.cells.map(getCellConfig);\n    const config = {\n      'cells': cellConfigs\n    };\n    return Object.assign({}, baseConfig, config);\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    let customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    const cells = [];\n\n    for (const cellConfig of config['cells']) {\n      cells.push(deserialize(cellConfig, customObjects));\n    }\n\n    return new cls({\n      cells\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) {\n      return [];\n    }\n\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n\n    return weights;\n  }\n\n  get nonTrainableWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      const trainableWeights = [];\n\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n\n      return trainableWeights.concat(weights);\n    }\n\n    return weights;\n  }\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n\n\n  getWeights() {\n    const weights = [];\n\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n\n    return batchGetValue(weights);\n  }\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n\n\n  setWeights(weights) {\n    const tuples = [];\n\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n\n    batchSetValue(tuples);\n  }\n\n}\n/** @nocollapse */\n\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n  const {\n    ones,\n    rate,\n    training = false,\n    count = 1,\n    dropoutFunc\n  } = args;\n\n  const droppedInputs = () => dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n\n  const createMask = () => K.inTrainPhase(droppedInputs, ones, training); // just in case count is provided with null or undefined\n\n\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n\n  const masks = Array(count).fill(undefined).map(createMask);\n  return masks.map(m => tfc.keep(m.clone()));\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAkBC,aAAlB,EAAyCC,IAAzC,EAA+CC,IAA/C,QAA0D,uBAA1D;AAEA,SAAoBC,aAApB,EAAmCC,mBAAnC,QAA6D,gBAA7D;AACA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAAQC,SAAR,QAAwB,WAAxB;AACA,SAA0CC,aAA1C,EAAyDC,mBAAzD,QAAmF,gBAAnF;AACA,SAAQC,SAAR,EAAmBC,cAAnB,QAAwC,oBAAxC;AACA,SAAQC,KAAR,QAA+B,oBAA/B;AACA,SAAQC,cAAR,EAAwBC,mBAAxB,EAA6CC,UAA7C,QAA8D,WAA9D;AACA,SAAQC,cAAR,EAAwBC,WAAxB,EAA4DC,IAA5D,EAAkEC,oBAAlE,QAA6F,iBAA7F;AAGA,SAAQC,cAAR,EAA4DC,oBAA5D,QAAuF,iBAAvF;AAEA,SAAQC,qBAAR,QAAoC,wBAApC;AACA,OAAO,KAAKC,UAAZ,MAA4B,qBAA5B;AACA,SAAQC,kBAAR,EAA4BC,mBAA5B,EAAiDC,eAAjD,QAAuE,sBAAvE;AACA,SAAQC,aAAR,EAAuBC,aAAvB,QAA0D,cAA1D;AAEA,SAAQC,WAAR,QAA0B,iBAA1B;AAEA;;;;;;;;;;;;;;;;;;;;;;;AAsBA,OAAM,SAAUC,eAAV,CACFC,MADE,EAEFC,YAFE,EAGFC,SAHE,EAIFC,YAJE,EAImB;AAKvB,MAAIC,KAAK,CAACC,OAAN,CAAcL,MAAd,CAAJ,EAA2B;AACzB,QAAIC,YAAY,IAAI,IAAhB,IAAwBC,SAAS,IAAI,IAAzC,EAA+C;AAC7C,YAAM,IAAIlB,UAAJ,CACF,gEACA,oBAFE,CAAN;AAGD;;AACD,QAAImB,YAAY,IAAI,IAApB,EAA0B;AACxBD,eAAS,GAAGF,MAAM,CAACM,KAAP,CAAaN,MAAM,CAACO,MAAP,GAAgBJ,YAA7B,EAA2CH,MAAM,CAACO,MAAlD,CAAZ;AACAP,YAAM,GAAGA,MAAM,CAACM,KAAP,CAAa,CAAb,EAAgBN,MAAM,CAACO,MAAP,GAAgBJ,YAAhC,CAAT;AACD;;AACD,QAAIH,MAAM,CAACO,MAAP,GAAgB,CAApB,EAAuB;AACrBN,kBAAY,GAAGD,MAAM,CAACM,KAAP,CAAa,CAAb,EAAgBN,MAAM,CAACO,MAAvB,CAAf;AACD;;AACDP,UAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;AACD;;AAED,WAASQ,YAAT,CAAsBC,CAAtB,EACsC;AACpC,QAAIA,CAAC,IAAI,IAAL,IAAaL,KAAK,CAACC,OAAN,CAAcI,CAAd,CAAjB,EAAmC;AACjC,aAAOA,CAAP;AACD,KAFD,MAEO;AACL,aAAO,CAACA,CAAD,CAAP;AACD;AACF;;AAEDR,cAAY,GAAGO,YAAY,CAACP,YAAD,CAA3B;AACAC,WAAS,GAAGM,YAAY,CAACN,SAAD,CAAxB;AAEA,SAAO;AAACF,UAAD;AAASC,gBAAT;AAAuBC;AAAvB,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2CA,OAAM,SAAUQ,GAAV,CACFC,YADE,EAC6BX,MAD7B,EAC6CY,aAD7C,EAGwB;AAAA,MAD1BC,WAC0B,uEADZ,KACY;AAAA,MADLC,IACK;AAAA,MADUZ,SACV;AAAA,MADgCa,MAChC,uEADyC,KACzC;AAAA,MAA1BC,kBAA0B,uEAAL,KAAK;AAC5B,SAAO/C,GAAG,CAACE,IAAJ,CAAS,MAAK;AACnB,UAAM8C,IAAI,GAAGjB,MAAM,CAACkB,KAAP,CAAaX,MAA1B;;AACA,QAAIU,IAAI,GAAG,CAAX,EAAc;AACZ,YAAM,IAAIjC,UAAJ,CAAe,uCAAuCiC,IAAI,IAA1D,CAAN;AACD,KAJkB,CAMnB;AACA;;;AACA,UAAME,IAAI,GAAG,CAAC,CAAD,EAAI,CAAJ,EAAOC,MAAP,CAAc5B,UAAU,CAAC6B,KAAX,CAAiB,CAAjB,EAAoBJ,IAApB,CAAd,CAAb;AACAjB,UAAM,GAAG/B,GAAG,CAACqD,SAAJ,CAActB,MAAd,EAAsBmB,IAAtB,CAAT;;AAEA,QAAIjB,SAAS,IAAI,IAAjB,EAAuB;AACrB,YAAM,IAAInB,mBAAJ,CACF,qEACA,gBAFE,CAAN;AAGD,KAfkB,CAiBnB;;;AACA,QAAIgC,MAAJ,EAAY;AACVQ,aAAO,CAACC,IAAR,CACI,sEACA,kCAFJ;AAGD;;AAED,QAAIV,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG7C,GAAG,CAACwD,IAAJ,CAASxD,GAAG,CAACwD,IAAJ,CAASX,IAAT,EAAe,MAAf,CAAT,EAAiC,SAAjC,CAAP;;AACA,UAAIA,IAAI,CAACY,IAAL,KAAcT,IAAI,GAAG,CAAzB,EAA4B;AAC1BH,YAAI,GAAG7C,GAAG,CAAC0D,UAAJ,CAAeb,IAAf,EAAqB,CAAC,CAAtB,CAAP;AACD;;AACDA,UAAI,GAAG7C,GAAG,CAACqD,SAAJ,CAAcR,IAAd,EAAoBK,IAApB,CAAP;AACD;;AAED,QAAIN,WAAJ,EAAiB;AACfb,YAAM,GAAG/B,GAAG,CAAC2D,OAAJ,CAAY5B,MAAZ,EAAoB,CAApB,CAAT;;AACA,UAAIc,IAAI,IAAI,IAAZ,EAAkB;AAChBA,YAAI,GAAG7C,GAAG,CAAC2D,OAAJ,CAAYd,IAAZ,EAAkB,CAAlB,CAAP;AACD;AACF,KArCkB,CAuCnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AAEA,UAAMe,cAAc,GAAa,EAAjC;AACA,QAAIC,UAAJ;AACA,QAAIC,MAAM,GAAGnB,aAAb;AACA,UAAMoB,SAAS,GAAGhC,MAAM,CAACkB,KAAP,CAAa,CAAb,CAAlB;AACA,UAAMe,aAAa,GAAGhE,GAAG,CAACiE,OAAJ,CAAYlC,MAAZ,CAAtB;AACA,QAAImC,YAAJ;;AACA,QAAIrB,IAAI,IAAI,IAAZ,EAAkB;AAChBqB,kBAAY,GAAGlE,GAAG,CAACiE,OAAJ,CAAYpB,IAAZ,CAAf;AACD;;AAED,SAAK,IAAIsB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGJ,SAApB,EAA+B,EAAEI,CAAjC,EAAoC;AAClC,YAAMC,YAAY,GAAGJ,aAAa,CAACG,CAAD,CAAlC;AACA,YAAME,WAAW,GAAGrE,GAAG,CAACE,IAAJ,CAAS,MAAMwC,YAAY,CAAC0B,YAAD,EAAeN,MAAf,CAA3B,CAApB;;AAEA,UAAIjB,IAAI,IAAI,IAAZ,EAAkB;AAChBgB,kBAAU,GAAGQ,WAAW,CAAC,CAAD,CAAxB;AACAP,cAAM,GAAGO,WAAW,CAAC,CAAD,CAApB;AACD,OAHD,MAGO;AACL,cAAMC,aAAa,GAAGtE,GAAG,CAACE,IAAJ,CAAS,MAAK;AAClC,gBAAMqE,QAAQ,GAAGL,YAAY,CAACC,CAAD,CAA7B;AACA,gBAAMK,WAAW,GAAGxE,GAAG,CAACyE,GAAJ,CAAQzE,GAAG,CAAC0E,QAAJ,CAAaH,QAAb,CAAR,EAAgCA,QAAhC,CAApB,CAFkC,CAGlC;;AACA,gBAAMI,MAAM,GAAG3E,GAAG,CAAC4E,GAAJ,CACX5E,GAAG,CAAC6E,GAAJ,CAAQR,WAAW,CAAC,CAAD,CAAnB,EAAwBE,QAAxB,CADW,EAEXvE,GAAG,CAAC6E,GAAJ,CAAQf,MAAM,CAAC,CAAD,CAAd,EAAmBU,WAAnB,CAFW,CAAf;AAGA,gBAAMM,SAAS,GAAGhB,MAAM,CAACiB,GAAP,CAAW,CAACC,KAAD,EAAQC,CAAR,KAAa;AACxC,mBAAOjF,GAAG,CAAC4E,GAAJ,CACH5E,GAAG,CAAC6E,GAAJ,CAAQR,WAAW,CAAC,CAAD,CAAX,CAAeY,CAAf,CAAR,EAA2BV,QAA3B,CADG,EAEHvE,GAAG,CAAC6E,GAAJ,CAAQG,KAAR,EAAeR,WAAf,CAFG,CAAP;AAGD,WAJiB,CAAlB;AAKA,iBAAO;AAACG,kBAAD;AAASG;AAAT,WAAP;AACD,SAbqB,CAAtB;AAcAjB,kBAAU,GAAGS,aAAa,CAACK,MAA3B;AACAb,cAAM,GAAGQ,aAAa,CAACQ,SAAvB;AACD;;AAED,UAAI/B,kBAAJ,EAAwB;AACtBa,sBAAc,CAACsB,IAAf,CAAoBrB,UAApB;AACD;AACF;;AACD,QAAIsB,OAAJ;;AACA,QAAIpC,kBAAJ,EAAwB;AACtB,YAAMqC,IAAI,GAAG,CAAb;AACAD,aAAO,GAAGnF,GAAG,CAACqF,KAAJ,CAAUzB,cAAV,EAA0BwB,IAA1B,CAAV;AACD;;AACD,WAAO,CAACvB,UAAD,EAAasB,OAAb,EAAsBrB,MAAtB,CAAP;AACD,GA/FM,CAAP;AAgGD;AAuGD,OAAM,MAAOwB,GAAP,SAAmB1E,KAAnB,CAAwB;AAqB5B2E,cAAYC,IAAZ,EAA8B;AAC5B,UAAMA,IAAN;AACA,QAAIC,IAAJ;;AACA,QAAID,IAAI,CAACC,IAAL,IAAa,IAAjB,EAAuB;AACrB,YAAM,IAAI1E,UAAJ,CACF,sDADE,CAAN;AAED,KAHD,MAGO,IAAIoB,KAAK,CAACC,OAAN,CAAcoD,IAAI,CAACC,IAAnB,CAAJ,EAA8B;AACnCA,UAAI,GAAG,IAAIC,eAAJ,CAAoB;AAACC,aAAK,EAAEH,IAAI,CAACC;AAAb,OAApB,CAAP;AACD,KAFM,MAEA;AACLA,UAAI,GAAGD,IAAI,CAACC,IAAZ;AACD;;AACD,QAAIA,IAAI,CAACG,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,YAAM,IAAI7E,UAAJ,CACF,iEACA,uCAFE,CAAN;AAGD;;AACD,SAAK0E,IAAL,GAAYA,IAAZ;AACA,SAAKI,eAAL,GACIL,IAAI,CAACK,eAAL,IAAwB,IAAxB,GAA+B,KAA/B,GAAuCL,IAAI,CAACK,eADhD;AAEA,SAAKC,WAAL,GAAmBN,IAAI,CAACM,WAAL,IAAoB,IAApB,GAA2B,KAA3B,GAAmCN,IAAI,CAACM,WAA3D;AACA,SAAKlD,WAAL,GAAmB4C,IAAI,CAAC5C,WAAL,IAAoB,IAApB,GAA2B,KAA3B,GAAmC4C,IAAI,CAAC5C,WAA3D;AACA,SAAKmD,SAAL,GAAiBP,IAAI,CAACQ,QAAL,IAAiB,IAAjB,GAAwB,KAAxB,GAAgCR,IAAI,CAACQ,QAAtD;AACA,SAAKlD,MAAL,GAAc0C,IAAI,CAAC1C,MAAL,IAAe,IAAf,GAAsB,KAAtB,GAA8B0C,IAAI,CAAC1C,MAAjD;AAEA,SAAKmD,eAAL,GAAuB,IAAvB;AACA,SAAKC,SAAL,GAAiB,CAAC,IAAIxF,SAAJ,CAAc;AAACsC,UAAI,EAAE;AAAP,KAAd,CAAD,CAAjB;AACA,SAAKmD,SAAL,GAAiB,IAAjB;AACA,SAAKC,OAAL,GAAe,IAAf,CA3B4B,CA4B5B;;AACA,SAAKlE,YAAL,GAAoB,IAApB,CA7B4B,CA8B5B;AACA;;AAEA,SAAKmE,UAAL,GAAkB,EAAlB;AACD,GAvD2B,CAyD5B;AACA;;;AACAC,WAAS;AACP,QAAI,KAAKF,OAAL,IAAgB,IAApB,EAA0B;AACxB,YAAMG,SAAS,GACXpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CADtE;AAEA,aAAOf,UAAU,CAAC6B,KAAX,CAAiB,CAAjB,EAAoBmD,SAApB,EAA+BxB,GAA/B,CAAmCvC,CAAC,IAAI,IAAxC,CAAP;AACD,KAJD,MAIO;AACL,aAAO,KAAK4D,OAAZ;AACD;AACF,GAnE2B,CAqE5B;AACA;;;AACAI,WAAS,CAAC1C,MAAD,EAAiB;AACxB,SAAKsC,OAAL,GAAetC,MAAf;AACD;;AAED2C,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,QAAIhF,eAAe,CAACgF,UAAD,CAAnB,EAAiC;AAC/BA,gBAAU,GAAIA,UAAsB,CAAC,CAAD,CAApC;AACD;;AACDA,cAAU,GAAGA,UAAb,CAJ0C,CAM1C;;AACA,QAAId,SAAS,GAAG,KAAKH,IAAL,CAAUG,SAA1B;;AACA,QAAI,CAACzD,KAAK,CAACC,OAAN,CAAcwD,SAAd,CAAL,EAA+B;AAC7BA,eAAS,GAAG,CAACA,SAAD,CAAZ;AACD;;AACD,UAAMe,SAAS,GAAGf,SAAS,CAAC,CAAD,CAA3B;AACA,QAAIgB,WAAJ;;AACA,QAAI,KAAKf,eAAT,EAA0B;AACxBe,iBAAW,GAAG,CAACF,UAAU,CAAC,CAAD,CAAX,EAAgBA,UAAU,CAAC,CAAD,CAA1B,EAA+BC,SAA/B,CAAd;AACD,KAFD,MAEO;AACLC,iBAAW,GAAG,CAACF,UAAU,CAAC,CAAD,CAAX,EAAgBC,SAAhB,CAAd;AACD;;AAED,QAAI,KAAKb,WAAT,EAAsB;AACpB,YAAMe,UAAU,GAAY,EAA5B;;AACA,WAAK,MAAMC,GAAX,IAAkBlB,SAAlB,EAA6B;AAC3BiB,kBAAU,CAAC3B,IAAX,CAAgB,CAACwB,UAAU,CAAC,CAAD,CAAX,EAAgBI,GAAhB,CAAhB;AACD;;AACD,aAAO,CAACF,WAAD,EAAczD,MAAd,CAAqB0D,UAArB,CAAP;AACD,KAND,MAMO;AACL,aAAOD,WAAP;AACD;AACF;;AAEDG,aAAW,CAAChF,MAAD,EAA0Bc,IAA1B,EAAgD;AAEzD,WAAO7C,GAAG,CAACE,IAAJ,CAAS,MAAK;AACnB,UAAIiC,KAAK,CAACC,OAAN,CAAcS,IAAd,CAAJ,EAAyB;AACvBA,YAAI,GAAGA,IAAI,CAAC,CAAD,CAAX;AACD;;AACD,YAAMmE,UAAU,GAAG,KAAKnB,eAAL,GAAuBhD,IAAvB,GAA8B,IAAjD;;AAEA,UAAI,KAAKiD,WAAT,EAAsB;AACpB,cAAMmB,SAAS,GAAG,KAAKnD,MAAL,CAAYiB,GAAZ,CAAgBmC,CAAC,IAAI,IAArB,CAAlB;AACA,eAAO,CAACF,UAAD,EAAa7D,MAAb,CAAoB8D,SAApB,CAAP;AACD,OAHD,MAGO;AACL,eAAOD,UAAP;AACD;AACF,KAZM,CAAP;AAaD;AAED;;;;;;;;AAMU,MAANlD,MAAM;AACR,QAAI,KAAKsC,OAAL,IAAgB,IAApB,EAA0B;AACxB,YAAMG,SAAS,GACXpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CADtE;AAEA,YAAMqC,MAAM,GAAa,EAAzB;;AACA,WAAK,IAAIM,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGsB,SAApB,EAA+B,EAAEtB,CAAjC,EAAoC;AAClCN,cAAM,CAACO,IAAP,CAAY,IAAZ;AACD;;AACD,aAAOP,MAAP;AACD,KARD,MAQO;AACL,aAAO,KAAKyB,OAAZ;AACD;AACF;;AAES,MAANtC,MAAM,CAACoD,CAAD,EAAY;AACpB,SAAKd,OAAL,GAAec,CAAf;AACD;;AAEMC,OAAK,CAACT,UAAD,EAA0B;AACpC;AACA;AACA,UAAMU,aAAa,GAAY,IAA/B;;AACA,QAAI,KAAKlF,YAAL,IAAqB,IAAzB,EAA+B;AAC7B,YAAM,IAAIpB,mBAAJ,CACF,kDADE,CAAN;AAED;;AAED,QAAIY,eAAe,CAACgF,UAAD,CAAnB,EAAiC;AAC/BA,gBAAU,GAAIA,UAAsB,CAAC,CAAD,CAApC;AACD;;AACDA,cAAU,GAAGA,UAAb;AAEA,UAAMW,SAAS,GAAW,KAAKrB,QAAL,GAAgBU,UAAU,CAAC,CAAD,CAA1B,GAAgC,IAA1D;AACA,UAAMY,QAAQ,GAAGZ,UAAU,CAACrE,KAAX,CAAiB,CAAjB,CAAjB;AACA,SAAK6D,SAAL,CAAe,CAAf,IAAoB,IAAIxF,SAAJ,CAAc;AAACuC,WAAK,EAAE,CAACoE,SAAD,EAAY,IAAZ,EAAkB,GAAGC,QAArB;AAAR,KAAd,CAApB,CAhBoC,CAkBpC;AACA;;AACA,UAAMC,cAAc,GAAG,CAACb,UAAU,CAAC,CAAD,CAAX,EAAgBvD,MAAhB,CAAuBuD,UAAU,CAACrE,KAAX,CAAiB,CAAjB,CAAvB,CAAvB;;AACA,QAAI+E,aAAa,IAAI,IAArB,EAA2B;AACzB,YAAM,IAAItG,mBAAJ,CACF,kDADE,CAAN;AAED,KAHD,MAGO;AACL,WAAK2E,IAAL,CAAU0B,KAAV,CAAgBI,cAAhB;AACD,KA1BmC,CA4BpC;;;AACA,QAAI3B,SAAJ;;AACA,QAAIzD,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;AACtCA,eAAS,GAAG,KAAKH,IAAL,CAAUG,SAAtB;AACD,KAFD,MAEO;AACLA,eAAS,GAAG,CAAC,KAAKH,IAAL,CAAUG,SAAX,CAAZ;AACD;;AAED,QAAI,KAAKO,SAAL,IAAkB,IAAtB,EAA4B;AAC1B,UAAI,CAAChG,IAAI,CAACqH,WAAL,CACG,KAAKrB,SAAL,CAAepB,GAAf,CAAmB0C,IAAI,IAAIA,IAAI,CAACxE,KAAL,CAAWwE,IAAI,CAACxE,KAAL,CAAWX,MAAX,GAAoB,CAA/B,CAA3B,CADH,EAEGsD,SAFH,CAAL,EAEoB;AAClB,cAAM,IAAI7E,UAAJ,CACF,4DACA,sCAAsC,KAAKoF,SAAS,IADpD,GAEA,6BAA6B,KAAKV,IAAL,CAAUG,SAAS,EAH9C,CAAN;AAID;AACF,KATD,MASO;AACL,WAAKO,SAAL,GACIP,SAAS,CAACb,GAAV,CAAc+B,GAAG,IAAI,IAAIpG,SAAJ,CAAc;AAACuC,aAAK,EAAE,CAAC,IAAD,EAAO6D,GAAP;AAAR,OAAd,CAArB,CADJ;AAED;;AACD,QAAI,KAAKd,QAAT,EAAmB;AACjB,WAAK0B,WAAL;AACD;AACF;AAED;;;;;;;;;;;;;;;;;;;AAiBAA,aAAW,CAAC5D,MAAD,EAA2C;AAAA,QAAhB6D,QAAgB,uEAAL,KAAK;AACpDzH,QAAI,CAAC,MAAK;AACR,UAAI,CAAC,KAAK8F,QAAV,EAAoB;AAClB,cAAM,IAAInF,cAAJ,CACF,iEADE,CAAN;AAED;;AACD,YAAMwG,SAAS,GAAG,KAAKnB,SAAL,CAAe,CAAf,EAAkBjD,KAAlB,CAAwB,CAAxB,CAAlB;;AACA,UAAIoE,SAAS,IAAI,IAAjB,EAAuB;AACrB,cAAM,IAAItG,UAAJ,CACF,qEACA,0CADA,GAEA,2DAFA,GAGA,2DAHA,GAIA,2DAJA,GAKA,oDANE,CAAN;AAOD,OAdO,CAeR;;;AACA,UAAI,KAAKqF,OAAL,IAAgB,IAApB,EAA0B;AACxB,YAAIjE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;AACtC,eAAKQ,OAAL,GACI,KAAKX,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CAAwB+B,GAAG,IAAI9G,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAYP,GAAZ,CAAV,CAA/B,CADJ;AAED,SAHD,MAGO;AACL,eAAKV,OAAL,GAAe,CAACpG,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAY,KAAK5B,IAAL,CAAUG,SAAtB,CAAV,CAAD,CAAf;AACD;AACF,OAPD,MAOO,IAAI9B,MAAM,IAAI,IAAd,EAAoB;AACzB;AACA9D,WAAG,CAAC6H,OAAJ,CAAY,KAAKzB,OAAjB,EAFyB,CAGzB;;AACA,YAAI,KAAKC,UAAL,IAAmB,IAAvB,EAA6B;AAC3BrG,aAAG,CAAC6H,OAAJ,CAAY,KAAKxB,UAAjB;AACA,eAAKA,UAAL,GAAkB,EAAlB;AACD;;AAED,YAAIlE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;AACtC,eAAKQ,OAAL,GACI,KAAKX,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CAAwB+B,GAAG,IAAI9G,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAYP,GAAZ,CAAV,CAA/B,CADJ;AAED,SAHD,MAGO;AACL,eAAKV,OAAL,CAAa,CAAb,IAAkBpG,GAAG,CAAC4H,KAAJ,CAAU,CAACP,SAAD,EAAY,KAAK5B,IAAL,CAAUG,SAAtB,CAAV,CAAlB;AACD;AACF,OAfM,MAeA;AACL,YAAI,CAACzD,KAAK,CAACC,OAAN,CAAc0B,MAAd,CAAL,EAA4B;AAC1BA,gBAAM,GAAG,CAACA,MAAD,CAAT;AACD;;AACD,YAAIA,MAAM,CAACxB,MAAP,KAAkB,KAAK8D,OAAL,CAAa9D,MAAnC,EAA2C;AACzC,gBAAM,IAAIvB,UAAJ,CACF,SAAS,KAAK+G,IAAI,YAAY,KAAK1B,OAAL,CAAa9D,MAAM,aAAjD,GACA,mBAAmBwB,MAAM,CAACxB,MAAM,yBADhC,GAEA,aAAawB,MAAM,EAHjB,CAAN;AAID;;AAED,YAAI6D,QAAQ,KAAK,IAAjB,EAAuB;AACrB;AACA;AACA;AACA;AACA,eAAKtB,UAAL,CAAgBnB,IAAhB,CAAqB,KAAKkB,OAAL,CAAa/D,KAAb,EAArB;AACD,SAND,MAMO;AACLrC,aAAG,CAAC6H,OAAJ,CAAY,KAAKzB,OAAjB;AACD;;AAED,aAAK,IAAI2B,KAAK,GAAG,CAAjB,EAAoBA,KAAK,GAAG,KAAK3B,OAAL,CAAa9D,MAAzC,EAAiD,EAAEyF,KAAnD,EAA0D;AACxD,gBAAMC,KAAK,GAAGlE,MAAM,CAACiE,KAAD,CAApB;AACA,gBAAMjB,GAAG,GAAG3E,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IACR,KAAKH,IAAL,CAAUG,SAAV,CAAoBmC,KAApB,CADQ,GAER,KAAKtC,IAAL,CAAUG,SAFd;AAGA,gBAAMqC,aAAa,GAAG,CAACZ,SAAD,EAAYP,GAAZ,CAAtB;;AACA,cAAI,CAAC3G,IAAI,CAACqH,WAAL,CAAiBQ,KAAK,CAAC/E,KAAvB,EAA8BgF,aAA9B,CAAL,EAAmD;AACjD,kBAAM,IAAIlH,UAAJ,CACF,SAASgH,KAAK,+BAA+B,KAAKD,IAAI,IAAtD,GACA,kBAAkBG,aAAa,oBAC3BD,KAAK,CAAC/E,KAAK,EAHb,CAAN;AAID;;AACD,eAAKmD,OAAL,CAAa2B,KAAb,IAAsBC,KAAtB;AACD;AACF;;AACD,WAAK5B,OAAL,GAAe,KAAKA,OAAL,CAAarB,GAAb,CAAiBC,KAAK,IAAIhF,GAAG,CAACkI,IAAJ,CAASlD,KAAK,CAACmD,KAAN,EAAT,CAA1B,CAAf;AACD,KA3EG,CAAJ;AA4ED;;AAEDC,OAAK,CACDrG,MADC,EAEDsG,MAFC,EAEc;AACjB;AACA,QAAIrG,YAAY,GACZqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAEA,QAAIpG,SAAS,GACToG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,WAAD,CADlC;;AAEA,QAAIA,MAAM,IAAI,IAAd,EAAoB;AAClBA,YAAM,GAAG,EAAT;AACD;;AAED,UAAMC,YAAY,GACdxG,eAAe,CAACC,MAAD,EAASC,YAAT,EAAuBC,SAAvB,EAAkC,KAAKC,YAAvC,CADnB;AAEAH,UAAM,GAAGuG,YAAY,CAACvG,MAAtB;AACAC,gBAAY,GAAGsG,YAAY,CAACtG,YAA5B;AACAC,aAAS,GAAGqG,YAAY,CAACrG,SAAzB,CAdiB,CAgBjB;AACA;AACA;;AAEA,QAAIsG,gBAAgB,GAAiC,EAArD;AACA,QAAIC,eAAe,GAAgB,EAAnC;;AACA,QAAIxG,YAAY,IAAI,IAApB,EAA0B;AACxBqG,YAAM,CAAC,cAAD,CAAN,GAAyBrG,YAAzB;AACAuG,sBAAgB,GAAGA,gBAAgB,CAACpF,MAAjB,CAAwBnB,YAAxB,CAAnB;AACA,WAAKmE,SAAL,GAAiB,EAAjB;;AACA,WAAK,MAAMnB,KAAX,IAAoBhD,YAApB,EAAkC;AAChC,aAAKmE,SAAL,CAAejB,IAAf,CAAoB,IAAIxE,SAAJ,CAAc;AAACuC,eAAK,EAAE+B,KAAK,CAAC/B;AAAd,SAAd,CAApB;AACD,OANuB,CAOxB;AACA;AACA;;;AACAuF,qBAAe,GAAGA,eAAe,CAACrF,MAAhB,CAAuB,KAAKgD,SAA5B,CAAlB;AACD;;AACD,QAAIlE,SAAS,IAAI,IAAjB,EAAuB;AACrBoG,YAAM,CAAC,WAAD,CAAN,GAAsBpG,SAAtB;AACAsG,sBAAgB,GAAGA,gBAAgB,CAACpF,MAAjB,CAAwBlB,SAAxB,CAAnB,CAFqB,CAGrB;;AACA,WAAKC,YAAL,GAAoBD,SAAS,CAACK,MAA9B;AACD;;AAED,UAAMmG,QAAQ,GAAGF,gBAAgB,CAAC,CAAD,CAAhB,YAA+B5H,cAAhD;;AACA,QAAI8H,QAAJ,EAAc;AACZ;AACA,YAAMC,SAAS,GACX,CAAC3G,MAAD,EAASoB,MAAT,CAAgBoF,gBAAhB,CADJ;AAEA,YAAMI,aAAa,GAAG,KAAKzC,SAAL,CAAe/C,MAAf,CAAsBqF,eAAtB,CAAtB,CAJY,CAKZ;;AACA,YAAMI,iBAAiB,GAAG,KAAK1C,SAA/B;AACA,WAAKA,SAAL,GAAiByC,aAAjB;AACA,YAAMhE,MAAM,GAAG,MAAMyD,KAAN,CAAYM,SAAZ,EAAuBL,MAAvB,CAAf;AACA,WAAKnC,SAAL,GAAiB0C,iBAAjB;AACA,aAAOjE,MAAP;AACD,KAXD,MAWO;AACL,aAAO,MAAMyD,KAAN,CAAYrG,MAAZ,EAAoBsG,MAApB,CAAP;AACD;AACF,GAlW2B,CAoW5B;;;AACAQ,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C;AACA;AACA;AACA,WAAOnI,IAAI,CAAC,MAAK;AACf,YAAM2C,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;AACA,YAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;AACA,UAAIrG,YAAY,GACZqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAGAtG,YAAM,GAAGN,mBAAmB,CAACM,MAAD,CAA5B;;AACA,UAAIC,YAAY,IAAI,IAApB,EAA0B;AACxB,YAAI,KAAKgE,QAAT,EAAmB;AACjBhE,sBAAY,GAAG,KAAKoE,OAApB;AACD,SAFD,MAEO;AACLpE,sBAAY,GAAG,KAAK8G,eAAL,CAAqB/G,MAArB,CAAf;AACD;AACF;;AAED,YAAMwE,SAAS,GACXpE,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,IAAqC,KAAKH,IAAL,CAAUG,SAAV,CAAoBtD,MAAzD,GAAkE,CADtE;;AAEA,UAAIN,YAAY,CAACM,MAAb,KAAwBiE,SAA5B,EAAuC;AACrC,cAAM,IAAIxF,UAAJ,CACF,iBAAiBwF,SAAS,2BAA1B,GACA,GAAGvE,YAAY,CAACM,MAAM,oBAFpB,CAAN;AAGD;;AACD,UAAI,KAAKQ,MAAT,EAAiB;AACfQ,eAAO,CAACC,IAAR,CACI,kEADJ;AAED;;AAED,YAAMwF,cAAc,GAAW;AAACpB;AAAD,OAA/B,CA3Be,CA6Bf;;AACA,YAAMqB,IAAI,GAAG,CAACjH,MAAD,EAAiB+B,MAAjB,KAAqC;AAChD;AACA;AACA,cAAMqB,OAAO,GACT,KAAKM,IAAL,CAAUoD,IAAV,CAAe,CAAC9G,MAAD,EAASoB,MAAT,CAAgBW,MAAhB,CAAf,EAAwCiF,cAAxC,CADJ,CAHgD,CAKhD;;AACA,eAAO,CAAC5D,OAAO,CAAC,CAAD,CAAR,EAAaA,OAAO,CAAC9C,KAAR,CAAc,CAAd,CAAb,CAAP;AACD,OAPD,CA9Be,CAuCf;;;AAEA,YAAM4G,UAAU,GACZxG,GAAG,CAACuG,IAAD,EAAOjH,MAAP,EAAeC,YAAf,EAA6B,KAAKY,WAAlC,EAA+CC,IAA/C,EAAqD,IAArD,EACC,KAAKC,MADN,EACc,KAAK+C,eADnB,CADP;AAGA,YAAMhC,UAAU,GAAGoF,UAAU,CAAC,CAAD,CAA7B;AACA,YAAM9D,OAAO,GAAG8D,UAAU,CAAC,CAAD,CAA1B;AACA,YAAMnF,MAAM,GAAGmF,UAAU,CAAC,CAAD,CAAzB;;AAEA,UAAI,KAAKjD,QAAT,EAAmB;AACjB,aAAK0B,WAAL,CAAiB5D,MAAjB,EAAyB6D,QAAzB;AACD;;AAED,YAAMhD,MAAM,GAAG,KAAKkB,eAAL,GAAuBV,OAAvB,GAAiCtB,UAAhD,CApDe,CAsDf;;AAEA,UAAI,KAAKiC,WAAT,EAAsB;AACpB,eAAO,CAACnB,MAAD,EAASxB,MAAT,CAAgBW,MAAhB,CAAP;AACD,OAFD,MAEO;AACL,eAAOa,MAAP;AACD;AACF,KA7DU,CAAX;AA8DD;;AAEDmE,iBAAe,CAAC/G,MAAD,EAAe;AAC5B,WAAO7B,IAAI,CAAC,MAAK;AACf;AACA;AACA,UAAI8B,YAAY,GAAGhC,GAAG,CAAC4H,KAAJ,CAAU7F,MAAM,CAACkB,KAAjB,CAAnB,CAHe,CAIf;;AACAjB,kBAAY,GAAGhC,GAAG,CAACkJ,GAAJ,CAAQlH,YAAR,EAAsB,CAAC,CAAD,EAAI,CAAJ,CAAtB,CAAf;AACAA,kBAAY,GAAG1B,CAAC,CAACoD,UAAF,CAAa1B,YAAb,CAAf,CANe,CAM6B;;AAE5C,UAAIG,KAAK,CAACC,OAAN,CAAc,KAAKqD,IAAL,CAAUG,SAAxB,CAAJ,EAAwC;AACtC,eAAO,KAAKH,IAAL,CAAUG,SAAV,CAAoBb,GAApB,CACH+B,GAAG,IAAIA,GAAG,GAAG,CAAN,GAAUxG,CAAC,CAAC6I,IAAF,CAAOnH,YAAP,EAAqB,CAAC,CAAD,EAAI8E,GAAJ,CAArB,CAAV,GAA2C9E,YAD/C,CAAP;AAED,OAHD,MAGO;AACL,eAAO,KAAKyD,IAAL,CAAUG,SAAV,GAAsB,CAAtB,GACH,CAACtF,CAAC,CAAC6I,IAAF,CAAOnH,YAAP,EAAqB,CAAC,CAAD,EAAI,KAAKyD,IAAL,CAAUG,SAAd,CAArB,CAAD,CADG,GAEH,CAAC5D,YAAD,CAFJ;AAGD;AACF,KAhBU,CAAX;AAiBD;;AAEmB,MAAhBoH,gBAAgB;AAClB,QAAI,CAAC,KAAKC,SAAV,EAAqB;AACnB,aAAO,EAAP;AACD,KAHiB,CAIlB;;;AACA,WAAO,KAAK5D,IAAL,CAAU2D,gBAAjB;AACD;;AAEsB,MAAnBE,mBAAmB;AACrB;AACA,QAAI,CAAC,KAAKD,SAAV,EAAqB;AACnB,aAAO,KAAK5D,IAAL,CAAU8D,OAAjB;AACD;;AACD,WAAO,KAAK9D,IAAL,CAAU6D,mBAAjB;AACD;;AAEDE,8BAA4B,CAACxB,KAAD,EAAe;AACzC,UAAMwB,4BAAN,CAAmCxB,KAAnC;;AACA,QAAI,KAAKvC,IAAL,IAAa,IAAjB,EAAuB;AACrB,WAAKA,IAAL,CAAU+D,4BAAV,CAAuCxB,KAAvC;AACD;AACF;;AAEDyB,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AAEA,UAAME,MAAM,GAA6B;AACvC9D,qBAAe,EAAE,KAAKA,eADiB;AAEvCC,iBAAW,EAAE,KAAKA,WAFqB;AAGvClD,iBAAW,EAAE,KAAKA,WAHqB;AAIvCoD,cAAQ,EAAE,KAAKA,QAJwB;AAKvClD,YAAM,EAAE,KAAKA;AAL0B,KAAzC;;AAQA,QAAI,KAAKZ,YAAL,IAAqB,IAAzB,EAA+B;AAC7ByH,YAAM,CAAC,cAAD,CAAN,GAAyB,KAAKzH,YAA9B;AACD;;AAED,UAAM0H,UAAU,GAAG,KAAKnE,IAAL,CAAUgE,SAAV,EAAnB;;AAEA,QAAI,KAAKI,YAAL,OAAwBvE,GAAG,CAACwE,SAAhC,EAA2C;AACzCH,YAAM,CAAC,MAAD,CAAN,GAAiB;AACf,qBAAa,KAAKlE,IAAL,CAAUoE,YAAV,EADE;AAEf,kBAAUD;AAFK,OAAjB;AAID,KAtBM,CAwBP;;;AACA,6BAAWA,UAAX,EAA0BF,UAA1B,EAAyCC,MAAzC;AACD;AAED;;;AACiB,SAAVI,UAAU,CACbC,GADa,EAEbL,MAFa,EAGiC;AAAA,QAA9CM,aAA8C,uEAA9B,EAA8B;AAChD,UAAML,UAAU,GAAGD,MAAM,CAAC,MAAD,CAAzB;AACA,UAAMlE,IAAI,GAAG5D,WAAW,CAAC+H,UAAD,EAAaK,aAAb,CAAxB;AACA,WAAO,IAAID,GAAJ,CAAQE,MAAM,CAACC,MAAP,CAAcR,MAAd,EAAsB;AAAClE;AAAD,KAAtB,CAAR,CAAP;AACD;;AAxf2B;AAC5B;;AACOH,gBAAY,KAAZ;AAwfTrF,aAAa,CAACmK,aAAd,CAA4B9E,GAA5B,E,CAEA;AACA;AACA;;AACA;;;;;;AAKA,OAAM,MAAgB+E,OAAhB,SAAgCzJ,KAAhC,CAAqC;AA+F3C,OAAM,MAAO0J,aAAP,SAA6BD,OAA7B,CAAoC;AAkCxC9E,cAAYC,IAAZ,EAAwC;AACtC,UAAMA,IAAN;AANO,8BAAqB,MAArB;AACA,sCAA6B,cAA7B;AACA,yCAAgC,YAAhC;AACA,oCAAkD,OAAlD;AAIP,SAAK+E,KAAL,GAAa/E,IAAI,CAAC+E,KAAlB;AACAjJ,yBAAqB,CAAC,KAAKiJ,KAAN,EAAa,OAAb,CAArB;AACA,SAAKC,UAAL,GAAkBpK,aAAa,CAC3BoF,IAAI,CAACgF,UAAL,IAAmB,IAAnB,GAA0B,KAAKC,kBAA/B,GAAoDjF,IAAI,CAACgF,UAD9B,CAA/B;AAEA,SAAKE,OAAL,GAAelF,IAAI,CAACkF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BlF,IAAI,CAACkF,OAAlD;AAEA,SAAKC,iBAAL,GAAyB3J,cAAc,CACnCwE,IAAI,CAACmF,iBAAL,IAA0B,KAAKC,0BADI,CAAvC;AAEA,SAAKC,oBAAL,GAA4B7J,cAAc,CACtCwE,IAAI,CAACqF,oBAAL,IAA6B,KAAKC,6BADI,CAA1C;AAGA,SAAKC,eAAL,GACI/J,cAAc,CAACwE,IAAI,CAACuF,eAAL,IAAwB,KAAKC,wBAA9B,CADlB;AAGA,SAAKC,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;AACA,SAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;AACA,SAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;AAEA,SAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;AACA,SAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;AACA,SAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;AAEA,SAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CACX,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CADW,CAAf;AAEA,SAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACrC,CADqC,EAErCjK,UAAU,CAACkK,GAAX,CACI,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CADJ,CAFqC,CAAf,CAAxB;AAKA,SAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;AACA,SAAK/F,SAAL,GAAiB,KAAK2E,KAAtB;AACA,SAAKqB,WAAL,GAAmB,IAAnB;AACA,SAAKC,oBAAL,GAA4B,IAA5B;AACD;;AAED1E,OAAK,CAACT,UAAD,EAA0B;AAC7BA,cAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B,CAD6B,CAE7B;;AACA,SAAKoF,MAAL,GAAc,KAAKC,SAAL,CACV,QADU,EACA,CAACrF,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAAX,EAAoC,KAAKiI,KAAzC,CADA,EACiD,IADjD,EAEV,KAAKI,iBAFK,EAEc,KAAKM,iBAFnB,EAEsC,IAFtC,EAGV,KAAKG,gBAHK,CAAd;AAIA,SAAKY,eAAL,GAAuB,KAAKD,SAAL,CACnB,kBADmB,EACC,CAAC,KAAKxB,KAAN,EAAa,KAAKA,KAAlB,CADD,EAC2B,IAD3B,EAEnB,KAAKM,oBAFc,EAEQ,KAAKK,oBAFb,EAEmC,IAFnC,EAGnB,KAAKG,mBAHc,CAAvB;;AAIA,QAAI,KAAKX,OAAT,EAAkB;AAChB,WAAKuB,IAAL,GAAY,KAAKF,SAAL,CACR,MADQ,EACA,CAAC,KAAKxB,KAAN,CADA,EACc,IADd,EACoB,KAAKQ,eADzB,EAER,KAAKI,eAFG,EAEc,IAFd,EAEoB,KAAKG,cAFzB,CAAZ;AAGD,KAJD,MAIO;AACL,WAAKW,IAAL,GAAY,IAAZ;AACD;;AACD,SAAKC,KAAL,GAAa,IAAb;AACD,GA1FuC,CA4FxC;AACA;AACA;AACA;AACA;AACA;;;AACArD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf6B,YAAM,GAAGA,MAAT;;AACA,UAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;AACvB,cAAM,IAAIvB,UAAJ,CACF,8CAA8CgB,MAAM,CAACO,MAAM,GADzD,CAAN;AAED;;AACD,UAAI6J,UAAU,GAAGpK,MAAM,CAAC,CAAD,CAAvB;AACAA,YAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;AACA,YAAM4F,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;;AAEA,UAAI,IAAI,KAAKkD,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;AACpE,aAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADM;AAElBuK,cAAI,EAAE,KAAKf,OAFO;AAGlB5D,kBAHkB;AAIlBgE,qBAAW,EAAE,KAAKA;AAJA,SAAD,CAAtC;AAMD;;AACD,UAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;AACrC,aAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAayH,UAAb,CADM;AAElBG,cAAI,EAAE,KAAKZ,gBAFO;AAGlB/D,kBAHkB;AAIlBgE,qBAAW,EAAE,KAAKA;AAJA,SAAD,CAA/C;AAMD;;AACD,UAAIY,CAAJ;AACA,YAAMC,MAAM,GAAW,KAAKZ,WAA5B;AACA,YAAMa,SAAS,GAAW,KAAKZ,oBAA/B;;AACA,UAAIW,MAAM,IAAI,IAAd,EAAoB;AAClBD,SAAC,GAAGjM,CAAC,CAACoM,GAAF,CAAM1M,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAhB,CAAN,EAA+B,KAAKV,MAAL,CAAYa,IAAZ,EAA/B,CAAJ;AACD,OAFD,MAEO;AACLJ,SAAC,GAAGjM,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAJ;AACD;;AACD,UAAI,KAAKV,IAAL,IAAa,IAAjB,EAAuB;AACrBM,SAAC,GAAGjM,CAAC,CAACsM,OAAF,CAAUL,CAAV,EAAa,KAAKN,IAAL,CAAUU,IAAV,EAAb,CAAJ;AACD;;AACD,UAAIF,SAAS,IAAI,IAAjB,EAAuB;AACrBN,kBAAU,GAAGnM,GAAG,CAAC6E,GAAJ,CAAQsH,UAAR,EAAoBM,SAApB,CAAb;AACD;;AACD,UAAI9H,MAAM,GAAG3E,GAAG,CAAC4E,GAAJ,CAAQ2H,CAAR,EAAWjM,CAAC,CAACoM,GAAF,CAAMP,UAAN,EAAkB,KAAKH,eAAL,CAAqBW,IAArB,EAAlB,CAAX,CAAb;;AACA,UAAI,KAAKnC,UAAL,IAAmB,IAAvB,EAA6B;AAC3B7F,cAAM,GAAG,KAAK6F,UAAL,CAAgBpC,KAAhB,CAAsBzD,MAAtB,CAAT;AACD,OA5Cc,CA8Cf;;;AACA,aAAO,CAACA,MAAD,EAASA,MAAT,CAAP;AACD,KAhDU,CAAX;AAiDD;;AAED8E,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AAEA,UAAME,MAAM,GAA6B;AACvCY,WAAK,EAAE,KAAKA,KAD2B;AAEvCC,gBAAU,EAAEnK,mBAAmB,CAAC,KAAKmK,UAAN,CAFQ;AAGvCE,aAAO,EAAE,KAAKA,OAHyB;AAIvCC,uBAAiB,EAAExJ,oBAAoB,CAAC,KAAKwJ,iBAAN,CAJA;AAKvCE,0BAAoB,EAAE1J,oBAAoB,CAAC,KAAK0J,oBAAN,CALH;AAMvCE,qBAAe,EAAE5J,oBAAoB,CAAC,KAAK4J,eAAN,CANE;AAOvCE,uBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CAPA;AAQvCC,0BAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CARH;AASvCC,qBAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CATE;AAUvC0B,yBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAVF;AAWvCzB,sBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAXE;AAYvCC,yBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAZD;AAavCC,oBAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAbI;AAcvCC,aAAO,EAAE,KAAKA,OAdyB;AAevCG,sBAAgB,EAAE,KAAKA;AAfgB,KAAzC;AAkBA,6BAAWhC,UAAX,EAA0BC,MAA1B;AACD;;AA5KuC;AACxC;;AACOW,0BAAY,eAAZ;AA4KTrK,aAAa,CAACmK,aAAd,CAA4BE,aAA5B;AAgGA,OAAM,MAAOwC,SAAP,SAAyBxH,GAAzB,CAA4B;AAGhCC,cAAYC,IAAZ,EAAoC;AAClCA,QAAI,CAACC,IAAL,GAAY,IAAI6E,aAAJ,CAAkB9E,IAAlB,CAAZ;AACA,UAAMA,IAAN,EAFkC,CAGlC;AACD;;AAEDqD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf,UAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;AACjC5L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;AACA,aAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;AACD;;AACD,UAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;AAC1C7L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;AACA,aAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;AACD;;AACD,YAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;AACA,YAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;AACA,YAAMrG,YAAY,GACdqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAEA,aAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;AAACc,YAAD;AAAO8E,gBAAP;AAAiB3F;AAAjB,OAAnB,CAAP;AACD,KAdU,CAAX;AAeD;AAED;;;AACiB,SAAV+H,UAAU,CACbC,GADa,EAEbL,MAFa,EAEmB;AAClC,WAAO,IAAIK,GAAJ,CAAQL,MAAR,CAAP;AACD;;AAhC+B;AAChC;;AACOmD,sBAAY,WAAZ;AAgCT7M,aAAa,CAACmK,aAAd,CAA4B0C,SAA5B;AAqCA,OAAM,MAAOC,OAAP,SAAuB1C,OAAvB,CAA8B;AAsClC9E,cAAYC,IAAZ,EAAkC;AAChC,UAAMA,IAAN;AAZO,8BAAqB,MAArB;AACA,wCAAqD,aAArD;AAEA,sCAA6B,cAA7B;AACA,yCAAgC,YAAhC;AACA,oCAAkD,OAAlD;;AAQP,QAAIA,IAAI,CAACwH,UAAT,EAAqB;AACnB,YAAM,IAAIjM,UAAJ,CACF,6DADE,CAAN;AAED;;AACD,SAAKwJ,KAAL,GAAa/E,IAAI,CAAC+E,KAAlB;AACAjJ,yBAAqB,CAAC,KAAKiJ,KAAN,EAAa,OAAb,CAArB;AACA,SAAKC,UAAL,GAAkBpK,aAAa,CAC3BoF,IAAI,CAACgF,UAAL,KAAoByC,SAApB,GAAgC,KAAKxC,kBAArC,GACgCjF,IAAI,CAACgF,UAFV,CAA/B;AAGA,SAAK0C,mBAAL,GAA2B9M,aAAa,CACpCoF,IAAI,CAAC0H,mBAAL,KAA6BD,SAA7B,GACI,KAAKE,4BADT,GAEI3H,IAAI,CAAC0H,mBAH2B,CAAxC;AAIA,SAAKxC,OAAL,GAAelF,IAAI,CAACkF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BlF,IAAI,CAACkF,OAAlD;AAEA,SAAKC,iBAAL,GAAyB3J,cAAc,CACnCwE,IAAI,CAACmF,iBAAL,IAA0B,KAAKC,0BADI,CAAvC;AAEA,SAAKC,oBAAL,GAA4B7J,cAAc,CACtCwE,IAAI,CAACqF,oBAAL,IAA6B,KAAKC,6BADI,CAA1C;AAGA,SAAKC,eAAL,GACI/J,cAAc,CAACwE,IAAI,CAACuF,eAAL,IAAwB,KAAKC,wBAA9B,CADlB;AAGA,SAAKC,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;AACA,SAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;AACA,SAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;AAEA,SAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;AACA,SAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;AACA,SAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;AAEA,SAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CACX,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CADW,CAAf;AAEA,SAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACrC,CADqC,EAErCjK,UAAU,CAACkK,GAAX,CACI,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CADJ,CAFqC,CAAf,CAAxB;AAKA,SAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;AACA,SAAKyB,cAAL,GAAsB5H,IAAI,CAAC4H,cAA3B;AACA,SAAKxH,SAAL,GAAiB,KAAK2E,KAAtB;AACA,SAAKqB,WAAL,GAAmB,IAAnB;AACA,SAAKC,oBAAL,GAA4B,IAA5B;AACD;;AAEM1E,OAAK,CAACT,UAAD,EAA0B;AACpCA,cAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B;AACA,UAAMY,QAAQ,GAAGZ,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAA3B;AACA,SAAKwJ,MAAL,GAAc,KAAKC,SAAL,CACV,QADU,EACA,CAACzE,QAAD,EAAW,KAAKiD,KAAL,GAAa,CAAxB,CADA,EAC4B,IAD5B,EACkC,KAAKI,iBADvC,EAEV,KAAKM,iBAFK,EAEc,IAFd,EAEoB,KAAKG,gBAFzB,CAAd;AAGA,SAAKY,eAAL,GAAuB,KAAKD,SAAL,CACnB,kBADmB,EACC,CAAC,KAAKxB,KAAN,EAAa,KAAKA,KAAL,GAAa,CAA1B,CADD,EAC+B,IAD/B,EAEnB,KAAKM,oBAFc,EAEQ,KAAKK,oBAFb,EAEmC,IAFnC,EAGnB,KAAKG,mBAHc,CAAvB;;AAIA,QAAI,KAAKX,OAAT,EAAkB;AAChB,WAAKuB,IAAL,GAAY,KAAKF,SAAL,CACR,MADQ,EACA,CAAC,KAAKxB,KAAL,GAAa,CAAd,CADA,EACkB,IADlB,EACwB,KAAKQ,eAD7B,EAER,KAAKI,eAFG,EAEc,IAFd,EAEoB,KAAKG,cAFzB,CAAZ;AAGD,KAJD,MAIO;AACL,WAAKW,IAAL,GAAY,IAAZ;AACD,KAhBmC,CAiBpC;AACA;;;AACA,SAAKC,KAAL,GAAa,IAAb;AACD;;AAEDrD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf6B,YAAM,GAAGA,MAAT;;AACA,UAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;AACvB,cAAM,IAAIvB,UAAJ,CACF,yDACA,GAAGgB,MAAM,CAACO,MAAM,GAFd,CAAN;AAGD;;AAED,YAAMqF,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;AACA,UAAIgF,QAAQ,GAAGtL,MAAM,CAAC,CAAD,CAArB,CATe,CASY;;AAC3BA,YAAM,GAAGA,MAAM,CAAC,CAAD,CAAf,CAVe,CAYf;AACA;AACA;;AACA,UAAI,IAAI,KAAKwJ,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;AACpE,aAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADM;AAElBuK,cAAI,EAAE,KAAKf,OAFO;AAGlB5D,kBAHkB;AAIlB2F,eAAK,EAAE,CAJW;AAKlB3B,qBAAW,EAAE,KAAKA;AALA,SAAD,CAAtC;AAOD;;AACD,UAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;AACrC,aAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa2I,QAAb,CADM;AAElBf,cAAI,EAAE,KAAKZ,gBAFO;AAGlB/D,kBAHkB;AAIlB2F,eAAK,EAAE,CAJW;AAKlB3B,qBAAW,EAAE,KAAKA;AALA,SAAD,CAA/C;AAOD;;AACD,YAAMa,MAAM,GAAG,KAAKZ,WAApB;AACA,YAAMa,SAAS,GAAG,KAAKZ,oBAAvB;AACA,UAAI0B,CAAJ;AACA,UAAIC,CAAJ;AACA,UAAIC,EAAJ;;AAEA,UAAI,IAAI,KAAKlC,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAvC,EAA0C;AACxCxJ,cAAM,GAAG/B,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAM,CAAC,CAAD,CAAtB,CAAT;AACD;;AACD,UAAIkB,OAAO,GAAGpN,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAd;;AACA,UAAI,KAAKjC,OAAT,EAAkB;AAChBgD,eAAO,GAAGpN,CAAC,CAACsM,OAAF,CAAUc,OAAV,EAAmB,KAAKzB,IAAL,CAAUU,IAAV,EAAnB,CAAV;AACD;;AACD,UAAI,IAAI,KAAKjB,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAAzD,EAA4D;AAC1D2B,gBAAQ,GAAGrN,GAAG,CAAC6E,GAAJ,CAAQwI,QAAR,EAAkBZ,SAAS,CAAC,CAAD,CAA3B,CAAX;AACD;;AAED,YAAMkB,oBAAoB,GAAG,KAAK3B,eAAL,CAAqBW,IAArB,EAA7B;AACA,YAAM,CAACiB,GAAD,EAAMC,GAAN,IAAa7N,GAAG,CAAC8N,KAAJ,CACfH,oBADe,EACO,CAAC,IAAI,KAAKpD,KAAV,EAAiB,KAAKA,KAAtB,CADP,EAEfoD,oBAAoB,CAAClK,IAArB,GAA4B,CAFb,CAAnB;AAGA,YAAMsK,WAAW,GAAGzN,CAAC,CAACoM,GAAF,CAAMW,QAAN,EAAgBO,GAAhB,CAApB;AAEA,YAAM,CAACI,EAAD,EAAKC,EAAL,EAASC,EAAT,IAAelO,GAAG,CAAC8N,KAAJ,CAAUJ,OAAV,EAAmB,CAAnB,EAAsBA,OAAO,CAACjK,IAAR,GAAe,CAArC,CAArB;AACA,YAAM,CAAC0K,UAAD,EAAaC,UAAb,IACFpO,GAAG,CAAC8N,KAAJ,CAAUC,WAAV,EAAuB,CAAvB,EAA0BA,WAAW,CAACtK,IAAZ,GAAmB,CAA7C,CADJ;AAEA8J,OAAC,GAAG,KAAKL,mBAAL,CAAyB9E,KAAzB,CAA+BpI,GAAG,CAAC4E,GAAJ,CAAQoJ,EAAR,EAAYG,UAAZ,CAA/B,CAAJ;AACAX,OAAC,GAAG,KAAKN,mBAAL,CAAyB9E,KAAzB,CAA+BpI,GAAG,CAAC4E,GAAJ,CAAQqJ,EAAR,EAAYG,UAAZ,CAA/B,CAAJ;AAEA,YAAMC,UAAU,GAAG/N,CAAC,CAACoM,GAAF,CAAM1M,GAAG,CAAC6E,GAAJ,CAAQ2I,CAAR,EAAWH,QAAX,CAAN,EAA4BQ,GAA5B,CAAnB;AACAJ,QAAE,GAAG,KAAKjD,UAAL,CAAgBpC,KAAhB,CAAsBpI,GAAG,CAAC4E,GAAJ,CAAQsJ,EAAR,EAAYG,UAAZ,CAAtB,CAAL;AAEA,YAAM9B,CAAC,GACHvM,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQ0I,CAAR,EAAWF,QAAX,CAAR,EAA8BrN,GAAG,CAAC6E,GAAJ,CAAQ7E,GAAG,CAAC4E,GAAJ,CAAQ,CAAR,EAAW5E,GAAG,CAACsO,GAAJ,CAAQf,CAAR,CAAX,CAAR,EAAgCE,EAAhC,CAA9B,CADJ,CAlEe,CAoEf;;AACA,aAAO,CAAClB,CAAD,EAAIA,CAAJ,CAAP;AACD,KAtEU,CAAX;AAuED;;AAED9C,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AAEA,UAAME,MAAM,GAA6B;AACvCY,WAAK,EAAE,KAAKA,KAD2B;AAEvCC,gBAAU,EAAEnK,mBAAmB,CAAC,KAAKmK,UAAN,CAFQ;AAGvC0C,yBAAmB,EAAE7M,mBAAmB,CAAC,KAAK6M,mBAAN,CAHD;AAIvCxC,aAAO,EAAE,KAAKA,OAJyB;AAKvCC,uBAAiB,EAAExJ,oBAAoB,CAAC,KAAKwJ,iBAAN,CALA;AAMvCE,0BAAoB,EAAE1J,oBAAoB,CAAC,KAAK0J,oBAAN,CANH;AAOvCE,qBAAe,EAAE5J,oBAAoB,CAAC,KAAK4J,eAAN,CAPE;AAQvCE,uBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CARA;AASvCC,0BAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CATH;AAUvCC,qBAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CAVE;AAWvC0B,yBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAXF;AAYvCzB,sBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAZE;AAavCC,yBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAbD;AAcvCC,oBAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAdI;AAevCC,aAAO,EAAE,KAAKA,OAfyB;AAgBvCG,sBAAgB,EAAE,KAAKA,gBAhBgB;AAiBvC0B,oBAAc,EAAE,KAAKA,cAjBkB;AAkBvCJ,gBAAU,EAAE;AAlB2B,KAAzC;AAqBA,6BAAWtD,UAAX,EAA0BC,MAA1B;AACD;;AA9MiC;AAClC;;AACOoD,oBAAY,SAAZ;AA8MT9M,aAAa,CAACmK,aAAd,CAA4B2C,OAA5B;AA8BA,OAAM,MAAOwB,GAAP,SAAmBjJ,GAAnB,CAAsB;AAG1BC,cAAYC,IAAZ,EAA8B;AAC5B,QAAIA,IAAI,CAAC4H,cAAL,KAAwB,CAA5B,EAA+B;AAC7B9J,aAAO,CAACC,IAAR,CACI,iEACA,oDAFJ;AAGD;;AACDiC,QAAI,CAACC,IAAL,GAAY,IAAIsH,OAAJ,CAAYvH,IAAZ,CAAZ;AACA,UAAMA,IAAN,EAP4B,CAQ5B;AACD;;AAEDqD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf,UAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;AACjC5L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;AACA,aAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;AACD;;AACD,UAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;AAC1C7L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;AACA,aAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;AACD;;AACD,YAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;AACA,YAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;AACA,YAAMrG,YAAY,GACdqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAEA,aAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;AAACc,YAAD;AAAO8E,gBAAP;AAAiB3F;AAAjB,OAAnB,CAAP;AACD,KAdU,CAAX;AAeD;AAED;;;AACiB,SAAV+H,UAAU,CACbC,GADa,EAEbL,MAFa,EAEmB;AAClC,QAAIA,MAAM,CAAC,eAAD,CAAN,KAA4B,CAAhC,EAAmC;AACjCA,YAAM,CAAC,gBAAD,CAAN,GAA2B,CAA3B;AACD;;AACD,WAAO,IAAIK,GAAJ,CAAQL,MAAR,CAAP;AACD;;AAxCyB;AAC1B;;AACO4E,gBAAY,KAAZ;AAwCTtO,aAAa,CAACmK,aAAd,CAA4BmE,GAA5B;AAuCA,OAAM,MAAOC,QAAP,SAAwBnE,OAAxB,CAA+B;AAuCnC9E,cAAYC,IAAZ,EAAmC;AACjC,UAAMA,IAAN;AAZO,8BAAqB,MAArB;AACA,wCAA+B,aAA/B;AACA,sCAA6B,cAA7B;AACA,yCAAgC,YAAhC;AAEA,oCAA2B,OAA3B;AASP,SAAK+E,KAAL,GAAa/E,IAAI,CAAC+E,KAAlB;AACAjJ,yBAAqB,CAAC,KAAKiJ,KAAN,EAAa,OAAb,CAArB;AACA,SAAKC,UAAL,GAAkBpK,aAAa,CAC3BoF,IAAI,CAACgF,UAAL,KAAoByC,SAApB,GAAgC,KAAKxC,kBAArC,GACgCjF,IAAI,CAACgF,UAFV,CAA/B;AAGA,SAAK0C,mBAAL,GAA2B9M,aAAa,CACpCoF,IAAI,CAAC0H,mBAAL,KAA6BD,SAA7B,GACI,KAAKE,4BADT,GAEI3H,IAAI,CAAC0H,mBAH2B,CAAxC;AAIA,SAAKxC,OAAL,GAAelF,IAAI,CAACkF,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8BlF,IAAI,CAACkF,OAAlD;AAEA,SAAKC,iBAAL,GAAyB3J,cAAc,CACnCwE,IAAI,CAACmF,iBAAL,IAA0B,KAAKC,0BADI,CAAvC;AAEA,SAAKC,oBAAL,GAA4B7J,cAAc,CACtCwE,IAAI,CAACqF,oBAAL,IAA6B,KAAKC,6BADI,CAA1C;AAGA,SAAKC,eAAL,GACI/J,cAAc,CAACwE,IAAI,CAACuF,eAAL,IAAwB,KAAKC,wBAA9B,CADlB;AAEA,SAAKyD,cAAL,GAAsBjJ,IAAI,CAACiJ,cAA3B;AAEA,SAAKxD,iBAAL,GAAyB7J,cAAc,CAACoE,IAAI,CAACyF,iBAAN,CAAvC;AACA,SAAKC,oBAAL,GAA4B9J,cAAc,CAACoE,IAAI,CAAC0F,oBAAN,CAA1C;AACA,SAAKC,eAAL,GAAuB/J,cAAc,CAACoE,IAAI,CAAC2F,eAAN,CAArC;AAEA,SAAKC,gBAAL,GAAwB5K,aAAa,CAACgF,IAAI,CAAC4F,gBAAN,CAArC;AACA,SAAKC,mBAAL,GAA2B7K,aAAa,CAACgF,IAAI,CAAC6F,mBAAN,CAAxC;AACA,SAAKC,cAAL,GAAsB9K,aAAa,CAACgF,IAAI,CAAC8F,cAAN,CAAnC;AAEA,SAAKC,OAAL,GAAehK,UAAU,CAACiK,GAAX,CACX,CAAC,CAAD,EAAIjK,UAAU,CAACkK,GAAX,CAAe,CAAC,CAAD,EAAIjG,IAAI,CAAC+F,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B/F,IAAI,CAAC+F,OAApC,CAAf,CAAJ,CADW,CAAf;AAEA,SAAKG,gBAAL,GAAwBnK,UAAU,CAACiK,GAAX,CAAe,CACrC,CADqC,EAErCjK,UAAU,CAACkK,GAAX,CACI,CAAC,CAAD,EAAIjG,IAAI,CAACkG,gBAAL,IAAyB,IAAzB,GAAgC,CAAhC,GAAoClG,IAAI,CAACkG,gBAA7C,CADJ,CAFqC,CAAf,CAAxB;AAKA,SAAKC,WAAL,GAAmBnG,IAAI,CAACmG,WAAxB;AACA,SAAKyB,cAAL,GAAsB5H,IAAI,CAAC4H,cAA3B;AACA,SAAKxH,SAAL,GAAiB,CAAC,KAAK2E,KAAN,EAAa,KAAKA,KAAlB,CAAjB;AACA,SAAKqB,WAAL,GAAmB,IAAnB;AACA,SAAKC,oBAAL,GAA4B,IAA5B;AACD;;AAEM1E,OAAK,CAACT,UAAD,EAA0B;;;AACpCA,cAAU,GAAGlF,kBAAkB,CAACkF,UAAD,CAA/B;AACA,UAAMY,QAAQ,GAAGZ,UAAU,CAACA,UAAU,CAACpE,MAAX,GAAoB,CAArB,CAA3B;AACA,SAAKwJ,MAAL,GAAc,KAAKC,SAAL,CACV,QADU,EACA,CAACzE,QAAD,EAAW,KAAKiD,KAAL,GAAa,CAAxB,CADA,EAC4B,IAD5B,EACkC,KAAKI,iBADvC,EAEV,KAAKM,iBAFK,EAEc,IAFd,EAEoB,KAAKG,gBAFzB,CAAd;AAGA,SAAKY,eAAL,GAAuB,KAAKD,SAAL,CACnB,kBADmB,EACC,CAAC,KAAKxB,KAAN,EAAa,KAAKA,KAAL,GAAa,CAA1B,CADD,EAC+B,IAD/B,EAEnB,KAAKM,oBAFc,EAEQ,KAAKK,oBAFb,EAEmC,IAFnC,EAGnB,KAAKG,mBAHc,CAAvB;AAIA,QAAIN,eAAJ;;AACA,QAAI,KAAKL,OAAT,EAAkB;AAChB,UAAI,KAAK+D,cAAT,EAAyB;AACvB,cAAMC,gBAAgB,GAAG,KAAK3D,eAA9B;AACA,cAAM4D,aAAa,GAAG,KAAKpE,KAA3B;AACAQ,uBAAe,GAAG,KAAI6D,KAAC,MAAMC,UAAN,SAAyB5N,WAAzB,CAAoC;AAIzDmH,eAAK,CAACnF,KAAD,EAAe6L,KAAf,EAA+B;AAClC;AACA,kBAAMC,EAAE,GAAGL,gBAAgB,CAACtG,KAAjB,CAAuB,CAACuG,aAAD,CAAvB,CAAX;AACA,kBAAMK,EAAE,GAAI,IAAI9N,IAAJ,EAAD,CAAakH,KAAb,CAAmB,CAACuG,aAAD,CAAnB,CAAX;AACA,kBAAMM,MAAM,GAAGP,gBAAgB,CAACtG,KAAjB,CAAuB,CAACuG,aAAa,GAAG,CAAjB,CAAvB,CAAf;AACA,mBAAOrO,CAAC,CAAC4O,oBAAF,CACH5O,CAAC,CAAC4O,oBAAF,CAAuBH,EAAvB,EAA2BC,EAA3B,CADG,EAC6BC,MAD7B,CAAP;AAED;;AAXwD,SAArC;AACpB;AACOL,uBAAY,YAFC,IAAJ,GAAlB;AAaD,OAhBD,MAgBO;AACL7D,uBAAe,GAAG,KAAKA,eAAvB;AACD;;AACD,WAAKkB,IAAL,GAAY,KAAKF,SAAL,CACR,MADQ,EACA,CAAC,KAAKxB,KAAL,GAAa,CAAd,CADA,EACkB,IADlB,EACwBQ,eADxB,EACyC,KAAKI,eAD9C,EAER,IAFQ,EAEF,KAAKG,cAFH,CAAZ;AAGD,KAvBD,MAuBO;AACL,WAAKW,IAAL,GAAY,IAAZ;AACD,KApCmC,CAqCpC;AACA;;;AACA,SAAKC,KAAL,GAAa,IAAb;AACD;;AAEDrD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf,YAAMyH,QAAQ,GAAGU,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;AACAtG,YAAM,GAAGA,MAAT;;AACA,UAAIA,MAAM,CAACO,MAAP,KAAkB,CAAtB,EAAyB;AACvB,cAAM,IAAIvB,UAAJ,CACF,0DACA,GAAGgB,MAAM,CAACO,MAAM,GAFd,CAAN;AAGD;;AACD,UAAI+K,QAAQ,GAAGtL,MAAM,CAAC,CAAD,CAArB,CARe,CAQc;;AAC7B,YAAMoN,QAAQ,GAAGpN,MAAM,CAAC,CAAD,CAAvB,CATe,CASc;;AAC7BA,YAAM,GAAGA,MAAM,CAAC,CAAD,CAAf;;AACA,UAAI,IAAI,KAAKwJ,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAnC,IAAwC,KAAKK,WAAL,IAAoB,IAAhE,EAAsE;AACpE,aAAKA,WAAL,GAAmBQ,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa3C,MAAb,CADM;AAElBuK,cAAI,EAAE,KAAKf,OAFO;AAGlB5D,kBAHkB;AAIlB2F,eAAK,EAAE,CAJW;AAKlB3B,qBAAW,EAAE,KAAKA;AALA,SAAD,CAAtC;AAOD;;AACD,UAAI,IAAI,KAAKD,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAArD,IACA,KAAKG,oBAAL,IAA6B,IADjC,EACuC;AACrC,aAAKA,oBAAL,GAA4BO,mBAAmB,CAAC;AAClBC,cAAI,EAAE,MAAMrM,GAAG,CAAC0E,QAAJ,CAAa2I,QAAb,CADM;AAElBf,cAAI,EAAE,KAAKZ,gBAFO;AAGlB/D,kBAHkB;AAIlB2F,eAAK,EAAE,CAJW;AAKlB3B,qBAAW,EAAE,KAAKA;AALA,SAAD,CAA/C;AAOD;;AACD,YAAMa,MAAM,GAAG,KAAKZ,WAApB;AACA,YAAMa,SAAS,GACX,KAAKZ,oBADT,CA/Be,CAkCf;AACA;AACA;;AACA,UAAI5G,CAAJ;AACA,UAAImK,CAAJ;AACA,UAAIC,CAAJ;AACA,UAAIC,CAAJ;;AACA,UAAI,IAAI,KAAK/D,OAAT,IAAoB,KAAKA,OAAL,GAAe,CAAvC,EAA0C;AACxCxJ,cAAM,GAAG/B,GAAG,CAAC6E,GAAJ,CAAQ9C,MAAR,EAAgByK,MAAM,CAAC,CAAD,CAAtB,CAAT;AACD;;AACD,UAAIe,CAAC,GAAGjN,CAAC,CAACoM,GAAF,CAAM3K,MAAN,EAAc,KAAK+J,MAAL,CAAYa,IAAZ,EAAd,CAAR;;AACA,UAAI,IAAI,KAAKjB,gBAAT,IAA6B,KAAKA,gBAAL,GAAwB,CAAzD,EAA4D;AAC1D2B,gBAAQ,GAAGrN,GAAG,CAAC6E,GAAJ,CAAQwI,QAAR,EAAkBZ,SAAS,CAAC,CAAD,CAA3B,CAAX;AACD;;AACDc,OAAC,GAAGvN,GAAG,CAAC4E,GAAJ,CAAQ2I,CAAR,EAAWjN,CAAC,CAACoM,GAAF,CAAMW,QAAN,EAAgB,KAAKrB,eAAL,CAAqBW,IAArB,EAAhB,CAAX,CAAJ;;AACA,UAAI,KAAKjC,OAAT,EAAkB;AAChB6C,SAAC,GAAGjN,CAAC,CAACsM,OAAF,CAAUW,CAAV,EAAa,KAAKtB,IAAL,CAAUU,IAAV,EAAb,CAAJ;AACD;;AAED,YAAM,CAAC4C,EAAD,EAAKC,EAAL,EAASC,EAAT,EAAaC,EAAb,IAAmB1P,GAAG,CAAC8N,KAAJ,CAAUP,CAAV,EAAa,CAAb,EAAgBA,CAAC,CAAC9J,IAAF,GAAS,CAAzB,CAAzB;AAEAwB,OAAC,GAAG,KAAKiI,mBAAL,CAAyB9E,KAAzB,CAA+BmH,EAA/B,CAAJ;AACAH,OAAC,GAAG,KAAKlC,mBAAL,CAAyB9E,KAAzB,CAA+BoH,EAA/B,CAAJ;AACAH,OAAC,GAAGrP,GAAG,CAAC4E,GAAJ,CAAQ5E,GAAG,CAAC6E,GAAJ,CAAQuK,CAAR,EAAWD,QAAX,CAAR,EAA8BnP,GAAG,CAAC6E,GAAJ,CAAQI,CAAR,EAAW,KAAKuF,UAAL,CAAgBpC,KAAhB,CAAsBqH,EAAtB,CAAX,CAA9B,CAAJ;AACAH,OAAC,GAAG,KAAKpC,mBAAL,CAAyB9E,KAAzB,CAA+BsH,EAA/B,CAAJ;AAEA,YAAMnD,CAAC,GAAGvM,GAAG,CAAC6E,GAAJ,CAAQyK,CAAR,EAAW,KAAK9E,UAAL,CAAgBpC,KAAhB,CAAsBiH,CAAtB,CAAX,CAAV,CA5De,CA6Df;;AACA,aAAO,CAAC9C,CAAD,EAAIA,CAAJ,EAAO8C,CAAP,CAAP;AACD,KA/DU,CAAX;AAgED;;AAED5F,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AAEA,UAAME,MAAM,GAA6B;AACvCY,WAAK,EAAE,KAAKA,KAD2B;AAEvCC,gBAAU,EAAEnK,mBAAmB,CAAC,KAAKmK,UAAN,CAFQ;AAGvC0C,yBAAmB,EAAE7M,mBAAmB,CAAC,KAAK6M,mBAAN,CAHD;AAIvCxC,aAAO,EAAE,KAAKA,OAJyB;AAKvCC,uBAAiB,EAAExJ,oBAAoB,CAAC,KAAKwJ,iBAAN,CALA;AAMvCE,0BAAoB,EAAE1J,oBAAoB,CAAC,KAAK0J,oBAAN,CANH;AAOvCE,qBAAe,EAAE5J,oBAAoB,CAAC,KAAK4J,eAAN,CAPE;AAQvC0D,oBAAc,EAAE,KAAKA,cARkB;AASvCxD,uBAAiB,EAAE5J,oBAAoB,CAAC,KAAK4J,iBAAN,CATA;AAUvCC,0BAAoB,EAAE7J,oBAAoB,CAAC,KAAK6J,oBAAN,CAVH;AAWvCC,qBAAe,EAAE9J,oBAAoB,CAAC,KAAK8J,eAAN,CAXE;AAYvC0B,yBAAmB,EAAExL,oBAAoB,CAAC,KAAKwL,mBAAN,CAZF;AAavCzB,sBAAgB,EAAE3K,mBAAmB,CAAC,KAAK2K,gBAAN,CAbE;AAcvCC,yBAAmB,EAAE5K,mBAAmB,CAAC,KAAK4K,mBAAN,CAdD;AAevCC,oBAAc,EAAE7K,mBAAmB,CAAC,KAAK6K,cAAN,CAfI;AAgBvCC,aAAO,EAAE,KAAKA,OAhByB;AAiBvCG,sBAAgB,EAAE,KAAKA,gBAjBgB;AAkBvC0B,oBAAc,EAAE,KAAKA;AAlBkB,KAAzC;AAqBA,6BAAW1D,UAAX,EAA0BC,MAA1B;AACD;;AA1NkC;AACnC;;AACO6E,qBAAY,UAAZ;AA0NTvO,aAAa,CAACmK,aAAd,CAA4BoE,QAA5B;AAqCA,OAAM,MAAOmB,IAAP,SAAoBrK,GAApB,CAAuB;AAG3BC,cAAYC,IAAZ,EAA+B;AAC7B,QAAIA,IAAI,CAAC4H,cAAL,KAAwB,CAA5B,EAA+B;AAC7B9J,aAAO,CAACC,IAAR,CACI,iEACA,oDAFJ;AAGD;;AACDiC,QAAI,CAACC,IAAL,GAAY,IAAI+I,QAAJ,CAAahJ,IAAb,CAAZ;AACA,UAAMA,IAAN,EAP6B,CAQ7B;AACD;;AAEDqD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf,UAAI,KAAKuF,IAAL,CAAUmG,WAAV,IAAyB,IAA7B,EAAmC;AACjC5L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUmG,WAAtB;AACA,aAAKnG,IAAL,CAAUmG,WAAV,GAAwB,IAAxB;AACD;;AACD,UAAI,KAAKnG,IAAL,CAAUoG,oBAAV,IAAkC,IAAtC,EAA4C;AAC1C7L,WAAG,CAAC6H,OAAJ,CAAY,KAAKpC,IAAL,CAAUoG,oBAAtB;AACA,aAAKpG,IAAL,CAAUoG,oBAAV,GAAiC,IAAjC;AACD;;AACD,YAAMhJ,IAAI,GAAGwF,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,MAAD,CAA3C;AACA,YAAMV,QAAQ,GAAGU,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,UAAD,CAA/C;AACA,YAAMrG,YAAY,GACdqG,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwBA,MAAM,CAAC,cAAD,CADlC;AAEA,aAAO,MAAMQ,IAAN,CAAW9G,MAAX,EAAmB;AAACc,YAAD;AAAO8E,gBAAP;AAAiB3F;AAAjB,OAAnB,CAAP;AACD,KAdU,CAAX;AAeD;AAED;;;AACiB,SAAV+H,UAAU,CACbC,GADa,EAEbL,MAFa,EAEmB;AAClC,QAAIA,MAAM,CAAC,eAAD,CAAN,KAA4B,CAAhC,EAAmC;AACjCA,YAAM,CAAC,gBAAD,CAAN,GAA2B,CAA3B;AACD;;AACD,WAAO,IAAIK,GAAJ,CAAQL,MAAR,CAAP;AACD;;AAxC0B;AAC3B;;AACOgG,iBAAY,MAAZ;AAwCT1P,aAAa,CAACmK,aAAd,CAA4BuF,IAA5B;AASA,OAAM,MAAOjK,eAAP,SAA+B2E,OAA/B,CAAsC;AAK1C9E,cAAYC,IAAZ,EAAqC;AACnC,UAAMA,IAAN;AACA,SAAKG,KAAL,GAAaH,IAAI,CAACG,KAAlB;AACD;;AAEY,MAATC,SAAS;AACX;AACA;AACA;AACA;AACA,UAAMA,SAAS,GAAa,EAA5B;;AACA,SAAK,MAAMH,IAAX,IAAmB,KAAKE,KAAL,CAAWtD,KAAX,GAAmBsB,OAAnB,EAAnB,EAAiD;AAC/C,UAAIxB,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;AACjCA,iBAAS,CAACV,IAAV,CAAe,GAAGO,IAAI,CAACG,SAAvB;AACD,OAFD,MAEO;AACLA,iBAAS,CAACV,IAAV,CAAeO,IAAI,CAACG,SAApB;AACD;AACF;;AACD,WAAOA,SAAP;AACD;;AAEDiD,MAAI,CAAC9G,MAAD,EAA0BsG,MAA1B,EAAwC;AAC1C,WAAOnI,IAAI,CAAC,MAAK;AACf6B,YAAM,GAAGA,MAAT;AACA,UAAI+B,MAAM,GAAG/B,MAAM,CAACM,KAAP,CAAa,CAAb,CAAb,CAFe,CAIf;;AACA,YAAMuN,YAAY,GAAe,EAAjC;;AACA,WAAK,MAAMnK,IAAX,IAAmB,KAAKE,KAAL,CAAWtD,KAAX,GAAmBsB,OAAnB,EAAnB,EAAiD;AAC/C,YAAIxB,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;AACjCgK,sBAAY,CAAC1K,IAAb,CAAkBpB,MAAM,CAAC+L,MAAP,CAAc,CAAd,EAAiBpK,IAAI,CAACG,SAAL,CAAetD,MAAhC,CAAlB;AACD,SAFD,MAEO;AACLsN,sBAAY,CAAC1K,IAAb,CAAkBpB,MAAM,CAAC+L,MAAP,CAAc,CAAd,EAAiB,CAAjB,CAAlB;AACD;AACF;;AACDD,kBAAY,CAACjM,OAAb,GAbe,CAef;;AACA,YAAMmM,eAAe,GAAe,EAApC;AACA,UAAIC,UAAJ;;AACA,WAAK,IAAI9K,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKU,KAAL,CAAWrD,MAA/B,EAAuC,EAAE2C,CAAzC,EAA4C;AAC1C,cAAMQ,IAAI,GAAG,KAAKE,KAAL,CAAWV,CAAX,CAAb;AACAnB,cAAM,GAAG8L,YAAY,CAAC3K,CAAD,CAArB,CAF0C,CAG1C;;AACA,YAAIA,CAAC,KAAK,CAAV,EAAa;AACX8K,oBAAU,GAAG,CAAChO,MAAM,CAAC,CAAD,CAAP,EAAYoB,MAAZ,CAAmBW,MAAnB,CAAb;AACD,SAFD,MAEO;AACLiM,oBAAU,GAAG,CAACA,UAAU,CAAC,CAAD,CAAX,EAAgB5M,MAAhB,CAAuBW,MAAvB,CAAb;AACD;;AACDiM,kBAAU,GAAGtK,IAAI,CAACoD,IAAL,CAAUkH,UAAV,EAAsB1H,MAAtB,CAAb;AACAyH,uBAAe,CAAC5K,IAAhB,CAAqB6K,UAAU,CAAC1N,KAAX,CAAiB,CAAjB,CAArB;AACD,OA7Bc,CA+Bf;;;AACAyB,YAAM,GAAG,EAAT;;AACA,WAAK,MAAMkM,UAAX,IAAyBF,eAAe,CAACzN,KAAhB,GAAwBsB,OAAxB,EAAzB,EAA4D;AAC1DG,cAAM,CAACoB,IAAP,CAAY,GAAG8K,UAAf;AACD;;AACD,aAAO,CAACD,UAAU,CAAC,CAAD,CAAX,EAAgB5M,MAAhB,CAAuBW,MAAvB,CAAP;AACD,KArCU,CAAX;AAsCD;;AAEMqD,OAAK,CAACT,UAAD,EAA0B;AACpC,QAAIhF,eAAe,CAACgF,UAAD,CAAnB,EAAiC;AAC/B;AACA;AACAA,gBAAU,GAAIA,UAAsB,CAAC,CAAD,CAApC;AACD;;AACDA,cAAU,GAAGA,UAAb;AACA,QAAIC,SAAJ;AACA,SAAKhB,KAAL,CAAWsK,OAAX,CAAmB,CAACxK,IAAD,EAAOR,CAAP,KAAY;AAC7B1E,eAAS,CAAC,WAAW0E,CAAC,EAAb,EAAiB,MAAK;AAC7B;AAEAQ,YAAI,CAAC0B,KAAL,CAAWT,UAAX;;AACA,YAAIvE,KAAK,CAACC,OAAN,CAAcqD,IAAI,CAACG,SAAnB,CAAJ,EAAmC;AACjCe,mBAAS,GAAGlB,IAAI,CAACG,SAAL,CAAe,CAAf,CAAZ;AACD,SAFD,MAEO;AACLe,mBAAS,GAAGlB,IAAI,CAACG,SAAjB;AACD;;AACDc,kBAAU,GAAG,CAACA,UAAU,CAAC,CAAD,CAAX,EAAgBC,SAAhB,CAAb;AACD,OAVQ,CAAT;AAWD,KAZD;AAaA,SAAKuF,KAAL,GAAa,IAAb;AACD;;AAEDzC,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;;AAEA,UAAMyG,aAAa,GAAIzK,IAAD,IAAkB;AACtC,aAAO;AACL,qBAAaA,IAAI,CAACoE,YAAL,EADR;AAEL,kBAAUpE,IAAI,CAACgE,SAAL;AAFL,OAAP;AAID,KALD;;AAOA,UAAM0G,WAAW,GAAG,KAAKxK,KAAL,CAAWZ,GAAX,CAAemL,aAAf,CAApB;AAEA,UAAMvG,MAAM,GAAG;AAAC,eAASwG;AAAV,KAAf;AAEA,6BAAWzG,UAAX,EAA0BC,MAA1B;AACD;AAED;;;AACiB,SAAVI,UAAU,CACbC,GADa,EAEbL,MAFa,EAGiC;AAAA,QAA9CM,aAA8C,uEAA9B,EAA8B;AAChD,UAAMtE,KAAK,GAAc,EAAzB;;AACA,SAAK,MAAMiE,UAAX,IAA0BD,MAAM,CAAC,OAAD,CAAhC,EAA0E;AACxEhE,WAAK,CAACT,IAAN,CAAWrD,WAAW,CAAC+H,UAAD,EAAaK,aAAb,CAAtB;AACD;;AACD,WAAO,IAAID,GAAJ,CAAQ;AAACrE;AAAD,KAAR,CAAP;AACD;;AAEmB,MAAhByD,gBAAgB;AAClB,QAAI,CAAC,KAAKC,SAAV,EAAqB;AACnB,aAAO,EAAP;AACD;;AACD,UAAME,OAAO,GAAoB,EAAjC;;AACA,SAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;AAC7B4D,aAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC2D,gBAArB;AACD;;AACD,WAAOG,OAAP;AACD;;AAEsB,MAAnBD,mBAAmB;AACrB,UAAMC,OAAO,GAAoB,EAAjC;;AACA,SAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;AAC7B4D,aAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC6D,mBAArB;AACD;;AACD,QAAI,CAAC,KAAKD,SAAV,EAAqB;AACnB,YAAMD,gBAAgB,GAAoB,EAA1C;;AACA,WAAK,MAAM3D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;AAC7ByD,wBAAgB,CAAClE,IAAjB,CAAsB,GAAGO,IAAI,CAAC2D,gBAA9B;AACD;;AACD,aAAOA,gBAAgB,CAACjG,MAAjB,CAAwBoG,OAAxB,CAAP;AACD;;AACD,WAAOA,OAAP;AACD;AAED;;;;;;;AAKA6G,YAAU;AACR,UAAM7G,OAAO,GAAoB,EAAjC;;AACA,SAAK,MAAM9D,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;AAC7B4D,aAAO,CAACrE,IAAR,CAAa,GAAGO,IAAI,CAAC8D,OAArB;AACD;;AACD,WAAO5H,aAAa,CAAC4H,OAAD,CAApB;AACD;AAED;;;;;;;;AAMA8G,YAAU,CAAC9G,OAAD,EAAkB;AAC1B,UAAM+G,MAAM,GAAmC,EAA/C;;AACA,SAAK,MAAM7K,IAAX,IAAmB,KAAKE,KAAxB,EAA+B;AAC7B,YAAM4K,SAAS,GAAG9K,IAAI,CAAC8D,OAAL,CAAajH,MAA/B;AACA,YAAMkO,YAAY,GAAGjH,OAAO,CAACsG,MAAR,CAAeU,SAAf,CAArB;;AACA,WAAK,IAAItL,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGQ,IAAI,CAAC8D,OAAL,CAAajH,MAAjC,EAAyC,EAAE2C,CAA3C,EAA8C;AAC5CqL,cAAM,CAACpL,IAAP,CAAY,CAACO,IAAI,CAAC8D,OAAL,CAAatE,CAAb,CAAD,EAAkBuL,YAAY,CAACvL,CAAD,CAA9B,CAAZ;AACD;AACF;;AACDrD,iBAAa,CAAC0O,MAAD,CAAb;AACD;;AA/KyC;AAC1C;;AACO5K,4BAAY,iBAAZ;AAiLTzF,aAAa,CAACmK,aAAd,CAA4B1E,eAA5B;AAEA,OAAM,SAAU0G,mBAAV,CAA8B5G,IAA9B,EAML;AACC,QAAM;AAAC6G,QAAD;AAAOC,QAAP;AAAa3E,YAAQ,GAAG,KAAxB;AAA+B2F,SAAK,GAAG,CAAvC;AAA0C3B;AAA1C,MAAyDnG,IAA/D;;AAEA,QAAMiL,aAAa,GAAG,MAClB9E,WAAW,IAAI,IAAf,GAAsBA,WAAW,CAACU,IAAI,EAAL,EAASC,IAAT,CAAjC,GAAkDhM,CAAC,CAACiL,OAAF,CAAUc,IAAI,EAAd,EAAkBC,IAAlB,CADtD;;AAGA,QAAMoE,UAAU,GAAG,MAAMpQ,CAAC,CAACqQ,YAAF,CAAeF,aAAf,EAA8BpE,IAA9B,EAAoC1E,QAApC,CAAzB,CAND,CAQC;;;AACA,MAAI,CAAC2F,KAAD,IAAUA,KAAK,IAAI,CAAvB,EAA0B;AACxB,WAAOtN,GAAG,CAACkI,IAAJ,CAASwI,UAAU,GAAGvI,KAAb,EAAT,CAAP;AACD;;AAED,QAAMyI,KAAK,GAAGzO,KAAK,CAACmL,KAAD,CAAL,CAAauD,IAAb,CAAkB5D,SAAlB,EAA6BlI,GAA7B,CAAiC2L,UAAjC,CAAd;AAEA,SAAOE,KAAK,CAAC7L,GAAN,CAAU+L,CAAC,IAAI9Q,GAAG,CAACkI,IAAJ,CAAS4I,CAAC,CAAC3I,KAAF,EAAT,CAAf,CAAP;AACD","names":["tfc","serialization","tidy","util","getActivation","serializeActivation","K","nameScope","getConstraint","serializeConstraint","InputSpec","SymbolicTensor","Layer","AttributeError","NotImplementedError","ValueError","getInitializer","Initializer","Ones","serializeInitializer","getRegularizer","serializeRegularizer","assertPositiveInteger","math_utils","getExactlyOneShape","getExactlyOneTensor","isArrayOfShapes","batchGetValue","batchSetValue","deserialize","standardizeArgs","inputs","initialState","constants","numConstants","Array","isArray","slice","length","toListOrNull","x","rnn","stepFunction","initialStates","goBackwards","mask","unroll","needPerStepOutputs","ndim","shape","axes","concat","range","transpose","console","warn","cast","rank","expandDims","reverse","perStepOutputs","lastOutput","states","timeSteps","perStepInputs","unstack","perStepMasks","t","currentInput","stepOutputs","maskedOutputs","stepMask","negStepMask","sub","onesLike","output","add","mul","newStates","map","state","i","push","outputs","axis","stack","RNN","constructor","args","cell","StackedRNNCells","cells","stateSize","returnSequences","returnState","_stateful","stateful","supportsMasking","inputSpec","stateSpec","states_","keptStates","getStates","numStates","setStates","computeOutputShape","inputShape","outputDim","outputShape","stateShape","dim","computeMask","outputMask","stateMask","s","build","constantShape","batchSize","inputDim","stepInputShape","arraysEqual","spec","resetStates","training","zeros","dispose","name","index","value","expectedShape","keep","clone","apply","kwargs","standardized","additionalInputs","additionalSpecs","isTensor","fullInput","fullInputSpec","originalInputSpec","call","getInitialState","cellCallKwargs","step","rnnOutputs","sum","tile","trainableWeights","trainable","nonTrainableWeights","weights","setFastWeightInitDuringBuild","getConfig","baseConfig","config","cellConfig","getClassName","className","fromConfig","cls","customObjects","Object","assign","registerClass","RNNCell","SimpleRNNCell","units","activation","DEFAULT_ACTIVATION","useBias","kernelInitializer","DEFAULT_KERNEL_INITIALIZER","recurrentInitializer","DEFAULT_RECURRENT_INITIALIZER","biasInitializer","DEFAULT_BIAS_INITIALIZER","kernelRegularizer","recurrentRegularizer","biasRegularizer","kernelConstraint","recurrentConstraint","biasConstraint","dropout","min","max","recurrentDropout","dropoutFunc","dropoutMask","recurrentDropoutMask","kernel","addWeight","recurrentKernel","bias","built","prevOutput","generateDropoutMask","ones","rate","h","dpMask","recDpMask","dot","read","biasAdd","activityRegularizer","SimpleRNN","GRUCell","resetAfter","undefined","recurrentActivation","DEFAULT_RECURRENT_ACTIVATION","implementation","hTMinus1","count","z","r","hh","matrixX","recurrentKernelValue","rk1","rk2","split","matrixInner","xZ","xR","xH","recurrentZ","recurrentR","recurrentH","neg","GRU","LSTMCell","unitForgetBias","capturedBiasInit","capturedUnits","_a","CustomInit","dtype","bI","bF","bCAndH","concatAlongFirstAxis","cTMinus1","f","c","o","z0","z1","z2","z3","LSTM","nestedStates","splice","newNestedStates","callInputs","cellStates","forEach","getCellConfig","cellConfigs","getWeights","setWeights","tuples","numParams","inputWeights","droppedInputs","createMask","inTrainPhase","masks","fill","m"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/layers/recurrent.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {DataType, serialization, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Activation, getActivation, serializeActivation} from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, SymbolicTensor} from '../engine/topology';\nimport {Layer, LayerArgs} from '../engine/topology';\nimport {AttributeError, NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, Ones, serializeInitializer} from '../initializers';\nimport {ActivationIdentifier} from '../keras_format/activation_config';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs, RnnStepFunction} from '../types';\nimport {assertPositiveInteger} from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes} from '../utils/types_utils';\nimport {batchGetValue, batchSetValue, LayerVariable} from '../variables';\n\nimport {deserialize} from './serialization';\n\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(\n    inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    initialState: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    constants: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    numConstants?: number): {\n  inputs: Tensor|SymbolicTensor,\n  initialState: Tensor[]|SymbolicTensor[],\n  constants: Tensor[]|SymbolicTensor[]\n} {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError(\n          'When inputs is an array, neither initialState or constants ' +\n          'should be provided');\n    }\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x: Tensor|Tensor[]|SymbolicTensor|\n                        SymbolicTensor[]): Tensor[]|SymbolicTensor[] {\n    if (x == null || Array.isArray(x)) {\n      return x as Tensor[] | SymbolicTensor[];\n    } else {\n      return [x] as Tensor[] | SymbolicTensor[];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n\n  return {inputs, initialState, constants};\n}\n\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(\n    stepFunction: RnnStepFunction, inputs: Tensor, initialStates: Tensor[],\n    goBackwards = false, mask?: Tensor, constants?: Tensor[], unroll = false,\n    needPerStepOutputs = false): [Tensor, Tensor, Tensor[]] {\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n    if (ndim < 3) {\n      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n    }\n\n    // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError(\n          'The rnn() functoin of the deeplearn.js backend does not support ' +\n          'constants yet.');\n    }\n\n    // Porting Note: the unroll option is ignored by the imperative backend.\n    if (unroll) {\n      console.warn(\n          'Backend rnn(): the unroll = true option is not applicable to the ' +\n          'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    }\n\n    // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n    const perStepOutputs: Tensor[] = [];\n    let lastOutput: Tensor;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks: Tensor[];\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask);\n          // TODO(cais): Would tfc.where() be better for performance?\n          const output = tfc.add(\n              tfc.mul(stepOutputs[0], stepMask),\n              tfc.mul(states[0], negStepMask));\n          const newStates = states.map((state, i) => {\n            return tfc.add(\n                tfc.mul(stepOutputs[1][i], stepMask),\n                tfc.mul(state, negStepMask));\n          });\n          return {output, newStates};\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n    let outputs: Tensor;\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n    return [lastOutput, outputs, states] as [Tensor, Tensor, Tensor[]];\n  });\n}\n\nexport declare interface BaseRNNLayerArgs extends LayerArgs {\n  /**\n   * A RNN cell instance. A RNN cell is a class that has:\n   *   - a `call()` method, which takes `[Tensor, Tensor]` as the\n   *     first input argument. The first item is the input at time t, and\n   *     second item is the cell state at time t.\n   *     The `call()` method returns `[outputAtT, statesAtTPlus1]`.\n   *     The `call()` method of the cell can also take the argument `constants`,\n   *     see section \"Note on passing external constants\" below.\n   *     Porting Node: PyKeras overrides the `call()` signature of RNN cells,\n   *       which are Layer subtypes, to accept two arguments. tfjs-layers does\n   *       not do such overriding. Instead we preseve the `call()` signature,\n   *       which due to its `Tensor|Tensor[]` argument and return value, is\n   *       flexible enough to handle the inputs and states.\n   *   - a `stateSize` attribute. This can be a single integer (single state)\n   *     in which case it is the size of the recurrent state (which should be\n   *     the same as the size of the cell output). This can also be an Array of\n   *     integers (one size per state). In this case, the first entry\n   *     (`stateSize[0]`) should be the same as the size of the cell output.\n   * It is also possible for `cell` to be a list of RNN cell instances, in which\n   * case the cells get stacked on after the other in the RNN, implementing an\n   * efficient stacked RNN.\n   */\n  cell?: RNNCell|RNNCell[];\n\n  /**\n   * Whether to return the last output in the output sequence, or the full\n   * sequence.\n   */\n  returnSequences?: boolean;\n\n  /**\n   * Whether to return the last state in addition to the output.\n   */\n  returnState?: boolean;\n\n  /**\n   * If `true`, process the input sequence backwards and return the reversed\n   * sequence (default: `false`).\n   */\n  goBackwards?: boolean;\n\n  /**\n   * If `true`, the last state for each sample at index i in a batch will be\n   * used as initial state of the sample of index i in the following batch\n   * (default: `false`).\n   *\n   * You can set RNN layers to be \"stateful\", which means that the states\n   * computed for the samples in one batch will be reused as initial states\n   * for the samples in the next batch. This assumes a one-to-one mapping\n   * between samples in different successive batches.\n   *\n   * To enable \"statefulness\":\n   *   - specify `stateful: true` in the layer constructor.\n   *   - specify a fixed batch size for your model, by passing\n   *     - if sequential model:\n   *       `batchInputShape: [...]` to the first layer in your model.\n   *     - else for functional model with 1 or more Input layers:\n   *       `batchShape: [...]` to all the first layers in your model.\n   *     This is the expected shape of your inputs\n   *     *including the batch size*.\n   *     It should be a tuple of integers, e.g., `[32, 10, 100]`.\n   *   - specify `shuffle: false` when calling `LayersModel.fit()`.\n   *\n   * To reset the state of your model, call `resetStates()` on either the\n   * specific layer or on the entire model.\n   */\n  stateful?: boolean;\n  // TODO(cais): Explore whether we can warn users when they fail to set\n  //   `shuffle: false` when training a model consisting of stateful RNNs\n  //   and any stateful Layers in general.\n\n  /**\n   * If `true`, the network will be unrolled, else a symbolic loop will be\n   * used. Unrolling can speed-up a RNN, although it tends to be more memory-\n   * intensive. Unrolling is only suitable for short sequences (default:\n   * `false`).\n   * Porting Note: tfjs-layers has an imperative backend. RNNs are executed with\n   *   normal TypeScript control flow. Hence this property is inapplicable and\n   *   ignored in tfjs-layers.\n   */\n  unroll?: boolean;\n\n  /**\n   * Dimensionality of the input (integer).\n   *   This option (or alternatively, the option `inputShape`) is required when\n   *   this layer is used as the first layer in a model.\n   */\n  inputDim?: number;\n\n  /**\n   * Length of the input sequences, to be specified when it is constant.\n   * This argument is required if you are going to connect `Flatten` then\n   * `Dense` layers upstream (without it, the shape of the dense outputs cannot\n   * be computed). Note that if the recurrent layer is not the first layer in\n   * your model, you would need to specify the input length at the level of the\n   * first layer (e.g., via the `inputShape` option).\n   */\n  inputLength?: number;\n}\n\nexport class RNN extends Layer {\n  /** @nocollapse */\n  static className = 'RNN';\n  public readonly cell: RNNCell;\n  public readonly returnSequences: boolean;\n  public readonly returnState: boolean;\n  public readonly goBackwards: boolean;\n  public readonly unroll: boolean;\n\n  public stateSpec: InputSpec[];\n  protected states_: Tensor[];\n\n  // NOTE(cais): For stateful RNNs, the old states cannot be disposed right\n  // away when new states are set, because the old states may need to be used\n  // later for backpropagation through time (BPTT) and other purposes. So we\n  // keep them here for final disposal when the state is reset completely\n  // (i.e., through no-arg call to `resetStates()`).\n  protected keptStates: Tensor[][];\n\n  private numConstants: number;\n\n  constructor(args: RNNLayerArgs) {\n    super(args);\n    let cell: RNNCell;\n    if (args.cell == null) {\n      throw new ValueError(\n          'cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({cells: args.cell});\n    } else {\n      cell = args.cell;\n    }\n    if (cell.stateSize == null) {\n      throw new ValueError(\n          'The RNN cell should have an attribute `stateSize` (tuple of ' +\n          'integers, one integer per RNN state).');\n    }\n    this.cell = cell;\n    this.returnSequences =\n        args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({ndim: 3})];\n    this.stateSpec = null;\n    this.states_ = null;\n    // TODO(cais): Add constantsSpec and numConstants.\n    this.numConstants = null;\n    // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    this.keptStates = [];\n  }\n\n  // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n  getStates(): Tensor[] {\n    if (this.states_ == null) {\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  }\n\n  // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n  setStates(states: Tensor[]): void {\n    this.states_ = states;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n\n    // TODO(cais): Remove the casting once stacked RNN cells become supported.\n    let stateSize = this.cell.stateSize;\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n    const outputDim = stateSize[0];\n    let outputShape: Shape|Shape[];\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      const stateShape: Shape[] = [];\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n\n  computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n      const outputMask = this.returnSequences ? mask : null;\n\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n  get states(): Tensor[] {\n    if (this.states_ == null) {\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output: Tensor[] = [];\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n\n  set states(s: Tensor[]) {\n    this.states_ = s;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape: Shape[] = null;\n    if (this.numConstants != null) {\n      throw new NotImplementedError(\n          'Constants support is not implemented in RNN yet.');\n    }\n\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n\n    const batchSize: number = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape.slice(2);\n    this.inputSpec[0] = new InputSpec({shape: [batchSize, null, ...inputDim]});\n\n    // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (constantShape != null) {\n      throw new NotImplementedError(\n          'Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    }\n\n    // Set or validate stateSpec.\n    let stateSize: number[];\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(\n              this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]),\n              stateSize)) {\n        throw new ValueError(\n            `An initialState was passed that is not compatible with ` +\n            `cell.stateSize. Received stateSpec=${this.stateSpec}; ` +\n            `However cell.stateSize is ${this.cell.stateSize}`);\n      }\n    } else {\n      this.stateSpec =\n          stateSize.map(dim => new InputSpec({shape: [null, dim]}));\n    }\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n  resetStates(states?: Tensor|Tensor[], training = false): void {\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError(\n            'Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n      const batchSize = this.inputSpec[0].shape[0];\n      if (batchSize == null) {\n        throw new ValueError(\n            'If an RNN is stateful, it needs to know its batch size. Specify ' +\n            'the batch size of your input tensors: \\n' +\n            '- If using a Sequential model, specify the batch size by ' +\n            'passing a `batchInputShape` option to your first layer.\\n' +\n            '- If using the functional API, specify the batch size by ' +\n            'passing a `batchShape` option to your Input layer.');\n      }\n      // Initialize state if null.\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ =\n              this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_);\n        // For stateful RNNs, fully dispose kept old states.\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ =\n              this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n        if (states.length !== this.states_.length) {\n          throw new ValueError(\n              `Layer ${this.name} expects ${this.states_.length} state(s), ` +\n              `but it received ${states.length} state value(s). Input ` +\n              `received: ${states}`);\n        }\n\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ?\n              this.cell.stateSize[index] :\n              this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(\n                `State ${index} is incompatible with layer ${this.name}: ` +\n                `expected shape=${expectedShape}, received shape=${\n                    value.shape}`);\n          }\n          this.states_[index] = value;\n        }\n      }\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n\n  apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n\n    let additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    let additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({shape: state.shape}));\n      }\n      // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants);\n      // TODO(cais): Add this.constantsSpec.\n      this.numConstants = constants.length;\n    }\n\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput =\n          [inputs].concat(additionalInputs) as Tensor[] | SymbolicTensor[];\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call with temporarily replaced inputSpec.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  // tslint:disable-next-line:no-any\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'] as Tensor;\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n\n      inputs = getExactlyOneTensor(inputs);\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      if (initialState.length !== numStates) {\n        throw new ValueError(\n            `RNN Layer has ${numStates} state(s) but was passed ` +\n            `${initialState.length} initial state(s).`);\n      }\n      if (this.unroll) {\n        console.warn(\n            'Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n\n      const cellCallKwargs: Kwargs = {training};\n\n      // TODO(cais): Add support for constants.\n      const step = (inputs: Tensor, states: Tensor[]) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs =\n            this.cell.call([inputs].concat(states), cellCallKwargs) as Tensor[];\n        // Marshall the return value into output and new states.\n        return [outputs[0], outputs.slice(1)] as [Tensor, Tensor[]];\n      };\n\n      // TODO(cais): Add support for constants.\n\n      const rnnOutputs =\n          rnn(step, inputs, initialState, this.goBackwards, mask, null,\n              this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n\n      const output = this.returnSequences ? outputs : lastOutput;\n\n      // TODO(cais): Porperty set learning phase flag.\n\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n\n  getInitialState(inputs: Tensor): Tensor[] {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape);\n      // [Samples].\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState);  // [Samples, 1].\n\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(\n            dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ?\n            [K.tile(initialState, [1, this.cell.stateSize])] :\n            [initialState];\n      }\n    });\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    if (!this.trainable) {\n      return [];\n    }\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    return this.cell.trainableWeights;\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n    return this.cell.nonTrainableWeights;\n  }\n\n  setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll,\n    };\n\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n\n    const cellConfig = this.cell.getConfig();\n\n    if (this.getClassName() === RNN.className) {\n      config['cell'] = {\n        'className': this.cell.getClassName(),\n        'config': cellConfig,\n      } as serialization.ConfigDictValue;\n    }\n\n    // this order is necessary, to prevent cell name from replacing layer name\n    return {...cellConfig, ...baseConfig, ...config};\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const cellConfig = config['cell'] as serialization.ConfigDict;\n    const cell = deserialize(cellConfig, customObjects) as RNNCell;\n    return new cls(Object.assign(config, {cell}));\n  }\n}\nserialization.registerClass(RNN);\n\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport abstract class RNNCell extends Layer {\n  /**\n   * Size(s) of the states.\n   * For RNN cells with only a single state, this is a single integer.\n   */\n  // See\n  // https://www.typescriptlang.org/docs/handbook/release-notes/typescript-4-0.html#properties-overriding-accessors-and-vice-versa-is-an-error\n  public abstract stateSize: number|number[];\n  public dropoutMask: Tensor|Tensor[];\n  public recurrentDropoutMask: Tensor|Tensor[];\n}\n\nexport declare interface SimpleRNNCellLayerArgs extends LayerArgs {\n  /**\n   * units: Positive integer, dimensionality of the output space.\n   */\n  units: number;\n\n  /**\n   * Activation function to use.\n   * Default: hyperbolic tangent ('tanh').\n   * If you pass `null`,  'linear' activation will be applied.\n   */\n  activation?: ActivationIdentifier;\n\n  /**\n   * Whether the layer uses a bias vector.\n   */\n  useBias?: boolean;\n\n  /**\n   * Initializer for the `kernel` weights matrix, used for the linear\n   * transformation of the inputs.\n   */\n  kernelInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the `recurrentKernel` weights matrix, used for\n   * linear transformation of the recurrent state.\n   */\n  recurrentInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the bias vector.\n   */\n  biasInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Regularizer function applied to the `kernel` weights matrix.\n   */\n  kernelRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the `recurrent_kernel` weights matrix.\n   */\n  recurrentRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the bias vector.\n   */\n  biasRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Constraint function applied to the `kernel` weights matrix.\n   */\n  kernelConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the `recurrentKernel` weights matrix.\n   */\n  recurrentConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraintfunction applied to the bias vector.\n   */\n  biasConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Float number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the inputs.\n   */\n  dropout?: number;\n\n  /**\n   * Float number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the recurrent state.\n   */\n  recurrentDropout?: number;\n\n  /**\n   * This is added for test DI purpose.\n   */\n  dropoutFunc?: Function;\n}\n\nexport class SimpleRNNCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'SimpleRNNCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number;\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n  readonly DEFAULT_BIAS_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  constructor(args: SimpleRNNCellLayerArgs) {\n    super(args);\n    this.units = args.units;\n    assertPositiveInteger(this.units, `units`);\n    this.activation = getActivation(\n        args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    // TODO(cais): Use regularizer.\n    this.kernel = this.addWeight(\n        'kernel', [inputShape[inputShape.length - 1], this.units], null,\n        this.kernelInitializer, this.kernelRegularizer, true,\n        this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight(\n          'bias', [this.units], null, this.biasInitializer,\n          this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    this.built = true;\n  }\n\n  // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 2) {\n        throw new ValueError(\n            `SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n      }\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             dropoutFunc: this.dropoutFunc,\n                           }) as Tensor;\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(prevOutput),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      dropoutFunc: this.dropoutFunc,\n                                    }) as Tensor;\n      }\n      let h: Tensor;\n      const dpMask: Tensor = this.dropoutMask as Tensor;\n      const recDpMask: Tensor = this.recurrentDropoutMask as Tensor;\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      }\n\n      // TODO(cais): Properly set learning phase on output tensor?\n      return [output, output];\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(SimpleRNNCell);\n\nexport declare interface SimpleRNNLayerArgs extends BaseRNNLayerArgs {\n  /**\n   * Positive integer, dimensionality of the output space.\n   */\n  units: number;\n\n  /**\n   * Activation function to use.\n   *\n   * Defaults to  hyperbolic tangent (`tanh`)\n   *\n   * If you pass `null`, no activation will be applied.\n   */\n  activation?: ActivationIdentifier;\n\n  /**\n   * Whether the layer uses a bias vector.\n   */\n  useBias?: boolean;\n\n  /**\n   * Initializer for the `kernel` weights matrix, used for the linear\n   * transformation of the inputs.\n   */\n  kernelInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the `recurrentKernel` weights matrix, used for\n   * linear transformation of the recurrent state.\n   */\n  recurrentInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the bias vector.\n   */\n  biasInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Regularizer function applied to the kernel weights matrix.\n   */\n  kernelRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the recurrentKernel weights matrix.\n   */\n  recurrentRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the bias vector.\n   */\n  biasRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Constraint function applied to the kernel weights matrix.\n   */\n  kernelConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the recurrentKernel weights matrix.\n   */\n  recurrentConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the bias vector.\n   */\n  biasConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the inputs.\n   */\n  dropout?: number;\n\n  /**\n   * Number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the recurrent state.\n   */\n  recurrentDropout?: number;\n\n  /**\n   * This is added for test DI purpose.\n   */\n  dropoutFunc?: Function;\n}\n\n/**\n * RNNLayerConfig is identical to BaseRNNLayerConfig, except it makes the\n * `cell` property required. This interface is to be used with constructors\n * of concrete RNN layer subtypes.\n */\nexport declare interface RNNLayerArgs extends BaseRNNLayerArgs {\n  cell: RNNCell|RNNCell[];\n}\n\nexport class SimpleRNN extends RNN {\n  /** @nocollapse */\n  static className = 'SimpleRNN';\n  constructor(args: SimpleRNNLayerArgs) {\n    args.cell = new SimpleRNNCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    return new cls(config);\n  }\n}\nserialization.registerClass(SimpleRNN);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we extend\n//   that interface instead of repeating the fields.\nexport declare interface GRUCellLayerArgs extends SimpleRNNCellLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n\n  /**\n   * GRU convention (whether to apply reset gate after or before matrix\n   * multiplication). false = \"before\", true = \"after\" (only false is\n   * supported).\n   */\n  resetAfter?: boolean;\n}\n\nexport class GRUCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'GRUCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly recurrentActivation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number;\n  readonly implementation: number;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_RECURRENT_ACTIVATION: ActivationIdentifier = 'hardSigmoid';\n\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n  readonly DEFAULT_BIAS_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  constructor(args: GRUCellLayerArgs) {\n    super(args);\n    if (args.resetAfter) {\n      throw new ValueError(\n          `GRUCell does not support reset_after parameter set to true.`);\n    }\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(\n        args.activation === undefined ? this.DEFAULT_ACTIVATION :\n                                        args.activation);\n    this.recurrentActivation = getActivation(\n        args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight(\n        'kernel', [inputDim, this.units * 3], null, this.kernelInitializer,\n        this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units * 3], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight(\n          'bias', [this.units * 3], null, this.biasInitializer,\n          this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 2) {\n        throw new ValueError(\n            `GRUCell expects 2 input Tensors (inputs, h, c), got ` +\n            `${inputs.length}.`);\n      }\n\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1];  // Previous memory state.\n      inputs = inputs[0];\n\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             count: 3,\n                             dropoutFunc: this.dropoutFunc,\n                           }) as Tensor[];\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(hTMinus1),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      count: 3,\n                                      dropoutFunc: this.dropoutFunc,\n                                    }) as Tensor[];\n      }\n      const dpMask = this.dropoutMask as [Tensor, Tensor, Tensor];\n      const recDpMask = this.recurrentDropoutMask as [Tensor, Tensor, Tensor];\n      let z: Tensor;\n      let r: Tensor;\n      let hh: Tensor;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let matrixX = K.dot(inputs, this.kernel.read());\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      const recurrentKernelValue = this.recurrentKernel.read();\n      const [rk1, rk2] = tfc.split(\n          recurrentKernelValue, [2 * this.units, this.units],\n          recurrentKernelValue.rank - 1);\n      const matrixInner = K.dot(hTMinus1, rk1);\n\n      const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n      const [recurrentZ, recurrentR] =\n          tfc.split(matrixInner, 2, matrixInner.rank - 1);\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n\n      const h =\n          tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h];\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(GRUCell);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we inherit\n//   from that interface instead of repeating the fields here.\nexport declare interface GRULayerArgs extends SimpleRNNLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   * smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n}\n\nexport class GRU extends RNN {\n  /** @nocollapse */\n  static className = 'GRU';\n  constructor(args: GRULayerArgs) {\n    if (args.implementation === 0) {\n      console.warn(\n          '`implementation=0` has been deprecated, and now defaults to ' +\n          '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new GRUCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\nserialization.registerClass(GRU);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we extend\n//   that interface instead of repeating the fields.\nexport declare interface LSTMCellLayerArgs extends SimpleRNNCellLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * If `true`, add 1 to the bias of the forget gate at initialization.\n   * Setting it to `true` will also force `biasInitializer = 'zeros'`.\n   * This is recommended in\n   * [Jozefowicz et\n   * al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n   */\n  unitForgetBias?: boolean;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n}\n\nexport class LSTMCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'LSTMCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly recurrentActivation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n  readonly unitForgetBias: boolean;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number[];\n  readonly implementation: number;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n\n  readonly DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  constructor(args: LSTMCellLayerArgs) {\n    super(args);\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(\n        args.activation === undefined ? this.DEFAULT_ACTIVATION :\n                                        args.activation);\n    this.recurrentActivation = getActivation(\n        args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight(\n        'kernel', [inputDim, this.units * 4], null, this.kernelInitializer,\n        this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units * 4], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    let biasInitializer: Initializer;\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (class CustomInit extends Initializer {\n          /** @nocollapse */\n          static className = 'CustomInit';\n\n          apply(shape: Shape, dtype?: DataType): Tensor {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = (new Ones()).apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(\n                K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n        })();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n      this.bias = this.addWeight(\n          'bias', [this.units * 4], null, biasInitializer, this.biasRegularizer,\n          true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 3) {\n        throw new ValueError(\n            `LSTMCell expects 3 input Tensors (inputs, h, c), got ` +\n            `${inputs.length}.`);\n      }\n      let hTMinus1 = inputs[1];    // Previous memory state.\n      const cTMinus1 = inputs[2];  // Previous carry state.\n      inputs = inputs[0];\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             count: 4,\n                             dropoutFunc: this.dropoutFunc\n                           }) as Tensor[];\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(hTMinus1),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      count: 4,\n                                      dropoutFunc: this.dropoutFunc\n                                    }) as Tensor[];\n      }\n      const dpMask = this.dropoutMask as [Tensor, Tensor, Tensor, Tensor];\n      const recDpMask =\n          this.recurrentDropoutMask as [Tensor, Tensor, Tensor, Tensor];\n\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n      let i: Tensor;\n      let f: Tensor;\n      let c: Tensor;\n      let o: Tensor;\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let z = K.dot(inputs, this.kernel.read());\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n\n      const h = tfc.mul(o, this.activation.apply(c));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h, c];\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(LSTMCell);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we inherit\n//   from that interface instead of repeating the fields here.\nexport declare interface LSTMLayerArgs extends SimpleRNNLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * If `true`, add 1 to the bias of the forget gate at initialization.\n   * Setting it to `true` will also force `biasInitializer = 'zeros'`.\n   * This is recommended in\n   * [Jozefowicz et\n   * al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n   */\n  unitForgetBias?: boolean;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *   Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions, whereas mode 2 will\n   *   batch them into fewer, larger operations. These modes will\n   *   have different performance profiles on different hardware and\n   *   for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this config field.\n   */\n  implementation?: number;\n}\n\nexport class LSTM extends RNN {\n  /** @nocollapse */\n  static className = 'LSTM';\n  constructor(args: LSTMLayerArgs) {\n    if (args.implementation === 0) {\n      console.warn(\n          '`implementation=0` has been deprecated, and now defaults to ' +\n          '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new LSTMCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\nserialization.registerClass(LSTM);\n\nexport declare interface StackedRNNCellsArgs extends LayerArgs {\n  /**\n   * A `Array` of `RNNCell` instances.\n   */\n  cells: RNNCell[];\n}\n\nexport class StackedRNNCells extends RNNCell {\n  /** @nocollapse */\n  static className = 'StackedRNNCells';\n  protected cells: RNNCell[];\n\n  constructor(args: StackedRNNCellsArgs) {\n    super(args);\n    this.cells = args.cells;\n  }\n\n  get stateSize(): number[] {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize: number[] = [];\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n    return stateSize;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      let states = inputs.slice(1);\n\n      // Recover per-cell states.\n      const nestedStates: Tensor[][] = [];\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n      nestedStates.reverse();\n\n      // Call the cells in order and store the returned states.\n      const newNestedStates: Tensor[][] = [];\n      let callInputs: Tensor[];\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i];\n        // TODO(cais): Take care of constants.\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n        callInputs = cell.call(callInputs, kwargs) as Tensor[];\n        newNestedStates.push(callInputs.slice(1));\n      }\n\n      // Format the new states as a flat list in reverse cell order.\n      states = [];\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n      return [callInputs[0]].concat(states);\n    });\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n    let outputDim: number;\n    this.cells.forEach((cell, i) => {\n      nameScope(`RNNCell_${i}`, () => {\n        // TODO(cais): Take care of input constants.\n\n        cell.build(inputShape);\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n        inputShape = [inputShape[0], outputDim] as Shape;\n      });\n    });\n    this.built = true;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const getCellConfig = (cell: RNNCell) => {\n      return {\n        'className': cell.getClassName(),\n        'config': cell.getConfig(),\n      };\n    };\n\n    const cellConfigs = this.cells.map(getCellConfig);\n\n    const config = {'cells': cellConfigs};\n\n    return {...baseConfig, ...config};\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const cells: RNNCell[] = [];\n    for (const cellConfig of (config['cells'] as serialization.ConfigDict[])) {\n      cells.push(deserialize(cellConfig, customObjects) as RNNCell);\n    }\n    return new cls({cells});\n  }\n\n  get trainableWeights(): LayerVariable[] {\n    if (!this.trainable) {\n      return [];\n    }\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n    return weights;\n  }\n\n  get nonTrainableWeights(): LayerVariable[] {\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n    if (!this.trainable) {\n      const trainableWeights: LayerVariable[] = [];\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n      return trainableWeights.concat(weights);\n    }\n    return weights;\n  }\n\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n  getWeights(): Tensor[] {\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n    return batchGetValue(weights);\n  }\n\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n  setWeights(weights: Tensor[]): void {\n    const tuples: Array<[LayerVariable, Tensor]> = [];\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n    batchSetValue(tuples);\n  }\n\n  // TODO(cais): Maybe implemnt `losses` and `getLossesFor`.\n}\nserialization.registerClass(StackedRNNCells);\n\nexport function generateDropoutMask(args: {\n  ones: () => tfc.Tensor,\n  rate: number,\n  training?: boolean,\n  count?: number,\n  dropoutFunc?: Function,\n}): tfc.Tensor|tfc.Tensor[] {\n  const {ones, rate, training = false, count = 1, dropoutFunc} = args;\n\n  const droppedInputs = () =>\n      dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n\n  const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n\n  // just in case count is provided with null or undefined\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n\n  const masks = Array(count).fill(undefined).map(createMask);\n\n  return masks.map(m => tfc.keep(m.clone()));\n}\n"]},"metadata":{},"sourceType":"module"}