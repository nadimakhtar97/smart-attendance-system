{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\n\nexport class Activation extends serialization.Serializable {\n  getConfig() {\n    return {};\n  }\n\n}\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\n\nexport class Elu extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x) {\n    let alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return K.elu(x, alpha);\n  }\n\n}\n/** @nocollapse */\n\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\n\nexport class Selu extends Activation {\n  apply(x) {\n    return tfc.selu(x);\n  }\n\n}\n/** @nocollapse */\n\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\n\nexport class Relu extends Activation {\n  apply(x) {\n    return tfc.relu(x);\n  }\n\n}\n/** @nocollapse */\n\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\n\nexport class Relu6 extends Activation {\n  apply(x) {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n\n}\n/** @nocollapse */\n\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6); //* Linear activation (no-op) */\n\nexport class Linear extends Activation {\n  apply(x) {\n    return x;\n  }\n\n}\n/** @nocollapse */\n\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\n\nexport class Sigmoid extends Activation {\n  apply(x) {\n    return tfc.sigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\n\nexport class HardSigmoid extends Activation {\n  apply(x) {\n    return K.hardSigmoid(x);\n  }\n\n}\n/** @nocollapse */\n\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\n\nexport class Softplus extends Activation {\n  apply(x) {\n    return tfc.softplus(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\n\nexport class Softsign extends Activation {\n  apply(x) {\n    return K.softsign(x);\n  }\n\n}\n/** @nocollapse */\n\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\n\nexport class Tanh extends Activation {\n  apply(x) {\n    return tfc.tanh(x);\n  }\n\n}\n/** @nocollapse */\n\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\n\nexport class Softmax extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x) {\n    let axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return tfc.softmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\n\nexport class LogSoftmax extends Activation {\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x) {\n    let axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return tfc.logSoftmax(x, axis);\n  }\n\n}\n/** @nocollapse */\n\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\n\nexport class Swish extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x) {\n    let alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n  }\n\n}\n/** @nocollapse */\n\nSwish.className = 'swish';\nserialization.registerClass(Swish);\n/**\n * Mish activation function\n */\n\nexport class Mish extends Activation {\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x) {\n    return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n  }\n\n}\n/** @nocollapse */\n\nMish.className = 'mish';\nserialization.registerClass(Mish);\nexport function serializeActivation(activation) {\n  return activation.getClassName();\n}\nexport function deserializeActivation(config) {\n  let customObjects = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n  if (identifier == null) {\n    const config = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n\n  if (typeof identifier === 'string') {\n    const config = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;AAUA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAQC,aAAR,EAA+BC,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAKC,CAAZ,MAAmB,wBAAnB;AAEA,SAAQC,sBAAR,QAAqC,uBAArC;AAEA;;;;;;;;AAOA,OAAM,MAAgBC,UAAhB,SAAmCJ,aAAa,CAACK,YAAjD,CAA6D;AAEjEC,WAAS;AACP,WAAO,EAAP;AACD;;AAJgE;AAOnE;;;;;AAIA,OAAM,MAAOC,GAAP,SAAmBH,UAAnB,CAA6B;AAGjC;;;;;;;AAOAI,OAAK,CAACC,CAAD,EAAqB;AAAA,QAATC,KAAS,uEAAD,CAAC;AACxB,WAAOR,CAAC,CAACS,GAAF,CAAMF,CAAN,EAASC,KAAT,CAAP;AACD;;AAZgC;AACjC;;AACgBH,gBAAY,KAAZ;AAYlBP,aAAa,CAACY,aAAd,CAA4BL,GAA5B;AAEA;;;;;;;;AAOA,OAAM,MAAOM,IAAP,SAAoBT,UAApB,CAA8B;AAGlCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOV,GAAG,CAACe,IAAJ,CAASL,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgBI,iBAAY,MAAZ;AAKlBb,aAAa,CAACY,aAAd,CAA4BC,IAA5B;AAEA;;;;AAGA,OAAM,MAAOE,IAAP,SAAoBX,UAApB,CAA8B;AAGlCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOV,GAAG,CAACiB,IAAJ,CAASP,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgBM,iBAAY,MAAZ;AAKlBf,aAAa,CAACY,aAAd,CAA4BG,IAA5B;AAEA;;;;AAGA,OAAM,MAAOE,KAAP,SAAqBb,UAArB,CAA+B;AAGnCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOR,IAAI,CAAC,MAAMF,GAAG,CAACmB,OAAJ,CAAY,GAAZ,EAAiBnB,GAAG,CAACiB,IAAJ,CAASP,CAAT,CAAjB,CAAP,CAAX;AACD;;AALkC;AACnC;;AACgBQ,kBAAY,OAAZ;AAKlBjB,aAAa,CAACY,aAAd,CAA4BK,KAA5B,E,CAEA;;AACA,OAAM,MAAOE,MAAP,SAAsBf,UAAtB,CAAgC;AAGpCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOA,CAAP;AACD;;AALmC;AACpC;;AACgBU,mBAAY,QAAZ;AAKlBnB,aAAa,CAACY,aAAd,CAA4BO,MAA5B;AAEA;;;;AAGA,OAAM,MAAOC,OAAP,SAAuBhB,UAAvB,CAAiC;AAGrCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOV,GAAG,CAACsB,OAAJ,CAAYZ,CAAZ,CAAP;AACD;;AALoC;AACrC;;AACgBW,oBAAY,SAAZ;AAKlBpB,aAAa,CAACY,aAAd,CAA4BQ,OAA5B;AAEA;;;;AAGA,OAAM,MAAOE,WAAP,SAA2BlB,UAA3B,CAAqC;AAGzCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOP,CAAC,CAACqB,WAAF,CAAcd,CAAd,CAAP;AACD;;AALwC;AACzC;;AACgBa,wBAAY,aAAZ;AAKlBtB,aAAa,CAACY,aAAd,CAA4BU,WAA5B;AAEA;;;;AAGA,OAAM,MAAOE,QAAP,SAAwBpB,UAAxB,CAAkC;AAGtCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOV,GAAG,CAAC0B,QAAJ,CAAahB,CAAb,CAAP;AACD;;AALqC;AACtC;;AACgBe,qBAAY,UAAZ;AAKlBxB,aAAa,CAACY,aAAd,CAA4BY,QAA5B;AAEA;;;;AAGA,OAAM,MAAOE,QAAP,SAAwBtB,UAAxB,CAAkC;AAGtCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOP,CAAC,CAACyB,QAAF,CAAWlB,CAAX,CAAP;AACD;;AALqC;AACtC;;AACgBiB,qBAAY,UAAZ;AAKlB1B,aAAa,CAACY,aAAd,CAA4Bc,QAA5B;AAEA;;;;AAGA,OAAM,MAAOE,IAAP,SAAoBxB,UAApB,CAA8B;AAGlCI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOV,GAAG,CAAC8B,IAAJ,CAASpB,CAAT,CAAP;AACD;;AALiC;AAClC;;AACgBmB,iBAAY,MAAZ;AAKlB5B,aAAa,CAACY,aAAd,CAA4BgB,IAA5B;AAEA;;;;AAGA,OAAM,MAAOE,OAAP,SAAuB1B,UAAvB,CAAiC;AAGrC;;;;;;;;;;;;AAYAI,OAAK,CAACC,CAAD,EAA+B;AAAA,QAAnBsB,IAAmB,uEAAH,CAAC,CAAE;AAClC,WAAOhC,GAAG,CAACiC,OAAJ,CAAYvB,CAAZ,EAAesB,IAAf,CAAP;AACD;;AAjBoC;AACrC;;AACgBD,oBAAY,SAAZ;AAiBlB9B,aAAa,CAACY,aAAd,CAA4BkB,OAA5B;AAEA;;;;AAGA,OAAM,MAAOG,UAAP,SAA0B7B,UAA1B,CAAoC;AAGxC;;;;;;;;;;;;;AAaAI,OAAK,CAACC,CAAD,EAA+B;AAAA,QAAnBsB,IAAmB,uEAAH,CAAC,CAAE;AAClC,WAAOhC,GAAG,CAACmC,UAAJ,CAAezB,CAAf,EAAkBsB,IAAlB,CAAP;AACD;;AAlBuC;AACxC;;AACgBE,uBAAY,YAAZ;AAkBlBjC,aAAa,CAACY,aAAd,CAA4BqB,UAA5B;AAEA;;;;AAGA,OAAM,MAAOE,KAAP,SAAqB/B,UAArB,CAA+B;AAGnC;;;;;;;AAOAI,OAAK,CAACC,CAAD,EAAqB;AAAA,QAATC,KAAS,uEAAD,CAAC;AACxB,WAAOT,IAAI,CAAC,MAAMF,GAAG,CAACqC,GAAJ,CAAQrC,GAAG,CAACsB,OAAJ,CAAYtB,GAAG,CAACqC,GAAJ,CAAQ3B,CAAR,EAAWC,KAAX,CAAZ,CAAR,EAAwCD,CAAxC,CAAP,CAAX;AACD;;AAZkC;AACnC;;AACgB0B,kBAAY,OAAZ;AAYlBnC,aAAa,CAACY,aAAd,CAA4BuB,KAA5B;AAEA;;;;AAGA,OAAM,MAAOE,IAAP,SAAoBjC,UAApB,CAA8B;AAGlC;;;;;;AAMAI,OAAK,CAACC,CAAD,EAAU;AACb,WAAOR,IAAI,CAAC,MAAMF,GAAG,CAACqC,GAAJ,CAAQ3B,CAAR,EAAWV,GAAG,CAAC8B,IAAJ,CAAS9B,GAAG,CAAC0B,QAAJ,CAAahB,CAAb,CAAT,CAAX,CAAP,CAAX;AACD;;AAXiC;AAClC;;AACgB4B,iBAAY,MAAZ;AAWlBrC,aAAa,CAACY,aAAd,CAA4ByB,IAA5B;AAEA,OAAM,SAAUC,mBAAV,CAA8BC,UAA9B,EAAoD;AACxD,SAAOA,UAAU,CAACC,YAAX,EAAP;AACD;AAED,OAAM,SAAUC,qBAAV,CACFC,MADE,EAE0C;AAAA,MAA5CC,aAA4C,uEAAF,EAAE;AAC9C,SAAOxC,sBAAsB,CACzBuC,MADyB,EACjB1C,aAAa,CAAC4C,gBAAd,CAA+BC,MAA/B,GAAwCC,YADvB,EAEzBH,aAFyB,EAEV,YAFU,CAA7B;AAGD;AAED,OAAM,SAAUI,aAAV,CAAwBC,UAAxB,EAC2D;AAC/D,MAAIA,UAAU,IAAI,IAAlB,EAAwB;AACtB,UAAMN,MAAM,GAA6B,EAAzC;AACAA,UAAM,CAAC,WAAD,CAAN,GAAsB,QAAtB;AACAA,UAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAOD,qBAAqB,CAACC,MAAD,CAA5B;AACD;;AACD,MAAI,OAAOM,UAAP,KAAsB,QAA1B,EAAoC;AAClC,UAAMN,MAAM,GAA6B,EAAzC;AACAA,UAAM,CAAC,WAAD,CAAN,GAAsBM,UAAtB;AACAN,UAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAOD,qBAAqB,CAACC,MAAD,CAA5B;AACD,GALD,MAKO,IAAIM,UAAU,YAAY5C,UAA1B,EAAsC;AAC3C,WAAO4C,UAAP;AACD,GAFM,MAEA;AACL,WAAOP,qBAAqB,CAACO,UAAD,CAA5B;AACD;AACF","names":["tfc","serialization","tidy","K","deserializeKerasObject","Activation","Serializable","getConfig","Elu","apply","x","alpha","elu","registerClass","Selu","selu","Relu","relu","Relu6","minimum","Linear","Sigmoid","sigmoid","HardSigmoid","hardSigmoid","Softplus","softplus","Softsign","softsign","Tanh","tanh","Softmax","axis","softmax","LogSoftmax","logSoftmax","Swish","mul","Mish","serializeActivation","activation","getClassName","deserializeActivation","config","customObjects","SerializationMap","getMap","classNameMap","getActivation","identifier"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/activations.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport {ActivationIdentifier} from './keras_format/activation_config';\nimport {deserializeKerasObject} from './utils/generic_utils';\n\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport abstract class Activation extends serialization.Serializable {\n  abstract apply(tensor: Tensor, axis?: number): Tensor;\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'elu';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return K.elu(x, alpha);\n  }\n}\nserialization.registerClass(Elu);\n\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'selu';\n  apply(x: Tensor): Tensor {\n    return tfc.selu(x);\n  }\n}\nserialization.registerClass(Selu);\n\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu';\n  apply(x: Tensor): Tensor {\n    return tfc.relu(x);\n  }\n}\nserialization.registerClass(Relu);\n\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu6';\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n}\nserialization.registerClass(Relu6);\n\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n  /** @nocollapse */\n  static readonly className = 'linear';\n  apply(x: Tensor): Tensor {\n    return x;\n  }\n}\nserialization.registerClass(Linear);\n\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'sigmoid';\n  apply(x: Tensor): Tensor {\n    return tfc.sigmoid(x);\n  }\n}\nserialization.registerClass(Sigmoid);\n\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'hardSigmoid';\n  apply(x: Tensor): Tensor {\n    return K.hardSigmoid(x);\n  }\n}\nserialization.registerClass(HardSigmoid);\n\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softplus';\n  apply(x: Tensor): Tensor {\n    return tfc.softplus(x);\n  }\n}\nserialization.registerClass(Softplus);\n\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softsign';\n  apply(x: Tensor): Tensor {\n    return K.softsign(x);\n  }\n}\nserialization.registerClass(Softsign);\n\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n  /** @nocollapse */\n  static readonly className = 'tanh';\n  apply(x: Tensor): Tensor {\n    return tfc.tanh(x);\n  }\n}\nserialization.registerClass(Tanh);\n\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softmax';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.softmax(x, axis);\n  }\n}\nserialization.registerClass(Softmax);\n\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'logSoftmax';\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.logSoftmax(x, axis);\n  }\n}\nserialization.registerClass(LogSoftmax);\n\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'swish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n  }\n}\nserialization.registerClass(Swish);\n\n/**\n * Mish activation function\n */\nexport class Mish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'mish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n  }\n}\nserialization.registerClass(Mish);\n\nexport function serializeActivation(activation: Activation): string {\n  return activation.getClassName();\n}\n\nexport function deserializeActivation(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Activation {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'activation');\n}\n\nexport function getActivation(identifier: ActivationIdentifier|\n                              serialization.ConfigDict|Activation): Activation {\n  if (identifier == null) {\n    const config: serialization.ConfigDict = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n  if (typeof identifier === 'string') {\n    const config: serialization.ConfigDict = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}\n"]},"metadata":{},"sourceType":"module"}