{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Prelu } from '../kernel_names';\nimport { getReductionAxes } from '../ops/broadcast_util';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const preluGradConfig = {\n  kernelName: Prelu,\n  inputsToSave: ['x', 'alpha'],\n  gradFunc: (dy, saved) => {\n    const [x, alpha] = saved;\n    const mask = greater(x, 0);\n    return {\n      x: () => where(mask, dy, mul(dy, alpha)),\n      alpha: () => {\n        let res = where(mask, zerosLike(dy), mul(dy, x));\n        const reduceAxes = getReductionAxes(alpha.shape, dy.shape);\n\n        if (reduceAxes.length > 0) {\n          res = sum(res, reduceAxes);\n        }\n\n        return reshape(res, alpha.shape);\n      }\n    };\n  }\n};","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAgBA,SAAQA,KAAR,QAAoB,iBAApB;AAEA,SAAQC,gBAAR,QAA+B,uBAA/B;AACA,SAAQC,OAAR,QAAsB,gBAAtB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,OAAR,QAAsB,gBAAtB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,KAAR,QAAoB,cAApB;AACA,SAAQC,SAAR,QAAwB,mBAAxB;AAGA,OAAO,MAAMC,eAAe,GAAe;AACzCC,YAAU,EAAET,KAD6B;AAEzCU,cAAY,EAAE,CAAC,GAAD,EAAM,OAAN,CAF2B;AAGzCC,UAAQ,EAAE,CAACC,EAAD,EAAaC,KAAb,KAAgC;AACxC,UAAM,CAACC,CAAD,EAAIC,KAAJ,IAAaF,KAAnB;AACA,UAAMG,IAAI,GAAGd,OAAO,CAACY,CAAD,EAAI,CAAJ,CAApB;AAEA,WAAO;AACLA,OAAC,EAAE,MAAMR,KAAK,CAACU,IAAD,EAAOJ,EAAP,EAAWT,GAAG,CAACS,EAAD,EAAKG,KAAL,CAAd,CADT;AAELA,WAAK,EAAE,MAAK;AACV,YAAIE,GAAG,GAAGX,KAAK,CAACU,IAAD,EAAOT,SAAS,CAACK,EAAD,CAAhB,EAAsBT,GAAG,CAACS,EAAD,EAAKE,CAAL,CAAzB,CAAf;AACA,cAAMI,UAAU,GAAGjB,gBAAgB,CAACc,KAAK,CAACI,KAAP,EAAcP,EAAE,CAACO,KAAjB,CAAnC;;AACA,YAAID,UAAU,CAACE,MAAX,GAAoB,CAAxB,EAA2B;AACzBH,aAAG,GAAGZ,GAAG,CAACY,GAAD,EAAMC,UAAN,CAAT;AACD;;AACD,eAAOd,OAAO,CAACa,GAAD,EAAMF,KAAK,CAACI,KAAZ,CAAd;AACD;AATI,KAAP;AAWD;AAlBwC,CAApC","names":["Prelu","getReductionAxes","greater","mul","reshape","sum","where","zerosLike","preluGradConfig","kernelName","inputsToSave","gradFunc","dy","saved","x","alpha","mask","res","reduceAxes","shape","length"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/gradients/Prelu_grad.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {Prelu} from '../kernel_names';\nimport {GradConfig} from '../kernel_registry';\nimport {getReductionAxes} from '../ops/broadcast_util';\nimport {greater} from '../ops/greater';\nimport {mul} from '../ops/mul';\nimport {reshape} from '../ops/reshape';\nimport {sum} from '../ops/sum';\nimport {where} from '../ops/where';\nimport {zerosLike} from '../ops/zeros_like';\nimport {Tensor} from '../tensor';\n\nexport const preluGradConfig: GradConfig = {\n  kernelName: Prelu,\n  inputsToSave: ['x', 'alpha'],\n  gradFunc: (dy: Tensor, saved: Tensor[]) => {\n    const [x, alpha] = saved;\n    const mask = greater(x, 0);\n\n    return {\n      x: () => where(mask, dy, mul(dy, alpha)),\n      alpha: () => {\n        let res = where(mask, zerosLike(dy), mul(dy, x));\n        const reduceAxes = getReductionAxes(alpha.shape, dy.shape);\n        if (reduceAxes.length > 0) {\n          res = sum(res, reduceAxes);\n        }\n        return reshape(res, alpha.shape);\n      }\n    };\n  }\n};\n"]},"metadata":{},"sourceType":"module"}