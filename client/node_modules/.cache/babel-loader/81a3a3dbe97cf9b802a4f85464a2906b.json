{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  let xs;\n  let ys;\n  const iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + `${iteratorOut}`);\n  const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  const batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` + `provides ${flattenedXs.length} inputs.  (Expected input keys: ` + `${JSON.stringify(model.inputNames)})`);\n  tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` + `provides ${flattenedYs.length} outputs.  (Expected output keys: ` + `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` + `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` + `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` + `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result = []; // Check that all the required keys are available.\n\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(`The feature data generated by the dataset lacks the required ` + `${inputOrOutput} key '${name}'.`);\n      }\n\n      result.push(values[name]);\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport async function fitDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).');\n  tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` + `but it is not provided in this call.`);\n  tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` + `integer, but got ${args.epochs}`);\n  tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` + `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert( // tslint:disable-next-line:no-any\n  args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error('Cannot start training because another fit() call is ongoing.');\n  }\n\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs;\n    let valYs;\n\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), () => `For fitDataset() with dataset-based validation, ` + `config.validationBatches is expected not to be provided, ` + `or to be a positive integer, ` + `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(args.validationData);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames();\n    let callbackMetrics;\n\n    if (doValidation) {\n      callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {\n      callbackList,\n      history\n    } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n    doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n    let dataIterator = await dataset.iterator();\n\n    while (epoch < args.epochs) {\n      const epochLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next(); // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn('You provided `batchesPerEpoch` as ' + `${args.batchesPerEpoch}, ` + 'but your dataset iterator ran out of data after ' + `${stepsDone} batches; ` + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + `${args.batchesPerEpoch * args.epochs} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {\n            xs,\n            ys\n          } = standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n          const sampleWeights = [];\n\n          if (args.classWeight != null) {\n            const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n            }\n          } // Train on batch.\n\n\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts;\n\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(args.validationData, {\n                batches: args.validationBatches\n              }));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          } // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n\n\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport async function evaluateDataset( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs = [];\n\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), () => 'Test loop expects `batches` to be a positive integer, but ' + `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ? dataset : await dataset.iterator(); // Keeps track of number of examples used in this evaluation.\n\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {\n          xs,\n          ys\n        } = standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] = tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n        ++batch;\n      }\n\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + `batches (in this case, ${args.batches} batches). ` + 'You may need to use the repeat() function when building ' + 'your dataset.');\n      }\n\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAQC,MAAR,QAAqB,uBAArB;AACA,SAAsBC,kBAAtB,EAA8FC,oBAA9F,QAA4I,mBAA5I;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AACA,SAAQC,oBAAR,QAAmD,SAAnD;AAEA,SAAQC,gBAAR,EAA0BC,MAA1B,QAAuC,wBAAvC;AAGA,SAAqCC,uBAArC,EAA8DC,kBAA9D,QAAuF,kBAAvF,C,CAiKA;;AACA,MAAMC,6BAA6B,GAAG,EAAtC;AAEA;;;;;;;;;;;;;;;AAcA,SAASC,6BAAT,EACI;AACA;AACA;AACAC,KAJJ,EAIgBC,WAJhB,EAI+B;AAC7B,MAAIC,EAAJ;AACA,MAAIC,EAAJ;AAEA,QAAMC,cAAc,GAAGH,WAAvB;AACAC,IAAE,GAAGE,cAAc,CAAC,IAAD,CAAnB;AACAD,IAAE,GAAGC,cAAc,CAAC,IAAD,CAAnB;AACAjB,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACIJ,EAAE,IAAI,IAAN,IAAcC,EAAE,IAAI,IADxB,EAEI,MAAM,iEACF,4DADE,GAEF,8DAFE,GAGF,4DAHE,GAIF,GAAGF,WAAW,EANtB;AAQA,QAAMM,WAAW,GACbC,yBAAyB,CAAC,OAAD,EAAUR,KAAK,CAACS,UAAhB,EAA4BP,EAA5B,CAD7B;AAEA,QAAMQ,WAAW,GACbF,yBAAyB,CAAC,QAAD,EAAWR,KAAK,CAACW,WAAjB,EAA8BR,EAA9B,CAD7B;AAGA,QAAMS,SAAS,GAAWL,WAAW,CAAC,CAAD,CAAX,CAAeM,KAAf,CAAqB,CAArB,CAA1B;AAEA1B,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACIC,WAAW,CAACO,MAAZ,KAAuBd,KAAK,CAACe,MAAN,CAAaD,MADxC,EAEI,MAAM,mBAAmBd,KAAK,CAACe,MAAN,CAAaD,MAAM,2BAAtC,GACF,YAAYP,WAAW,CAACO,MAAM,kCAD5B,GAEF,GAAGE,IAAI,CAACC,SAAL,CAAejB,KAAK,CAACS,UAArB,CAAgC,GAJ3C;AAMAtB,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACII,WAAW,CAACI,MAAZ,KAAuBd,KAAK,CAACkB,OAAN,CAAcJ,MADzC,EAEI,MACI,mBAAmBd,KAAK,CAACkB,OAAN,CAAcJ,MAAM,4BAAvC,GACA,YAAYJ,WAAW,CAACI,MAAM,oCAD9B,GAEA,GAAGE,IAAI,CAACC,SAAL,CAAejB,KAAK,CAACW,WAArB,CAAiC,GAL5C;;AAOA,OAAK,IAAIQ,MAAM,GAAG,CAAlB,EAAqBA,MAAM,GAAGZ,WAAW,CAACO,MAA1C,EAAkDK,MAAM,EAAxD,EAA4D;AAC1DhC,OAAG,CAACkB,IAAJ,CAASC,MAAT,CACIC,WAAW,CAACY,MAAD,CAAX,CAAoBN,KAApB,CAA0B,CAA1B,MAAiCD,SADrC,EAEI,MAAM,gCACF,GAAGZ,KAAK,CAACS,UAAN,CAAiBU,MAAjB,CAAwB,QACrBZ,WAAW,CAACY,MAAD,CAAX,CAAoBN,KAApB,CAA0B,CAA1B,CAA4B,IAFhC,GAGF,aAAaD,SAAS,mBAAmBZ,KAAK,CAACS,UAAN,CAAiB,CAAjB,CAAmB,GALpE;AAMD;;AAED,OAAK,IAAIW,MAAM,GAAG,CAAlB,EAAqBA,MAAM,GAAGV,WAAW,CAACI,MAA1C,EAAkDM,MAAM,EAAxD,EAA4D;AAC1DjC,OAAG,CAACkB,IAAJ,CAASC,MAAT,CACII,WAAW,CAACU,MAAD,CAAX,CAAoBP,KAApB,CAA0B,CAA1B,MAAiCD,SADrC,EAEI,MAAM,iCACF,GAAGZ,KAAK,CAACW,WAAN,CAAkBS,MAAlB,CAAyB,QACtBV,WAAW,CAACU,MAAD,CAAX,CAAoBP,KAApB,CAA0B,CAA1B,CAA4B,IAFhC,GAGF,aAAaD,SAAS,mBAAmBZ,KAAK,CAACS,UAAN,CAAiB,CAAjB,CAAmB,GALpE;AAMD;;AAED,SAAO;AAACP,MAAE,EAAEK,WAAL;AAAkBJ,MAAE,EAAEO;AAAtB,GAAP;AACD;;AAED,SAASF,yBAAT,CACIa,aADJ,EAC2BC,KAD3B,EAC4CC,MAD5C,EACsE;AACpE,MAAIA,MAAM,YAAYpC,GAAG,CAACqC,MAA1B,EAAkC;AAChC,WAAO,CAACD,MAAD,CAAP;AACD,GAFD,MAEO,IAAIE,KAAK,CAACC,OAAN,CAAcH,MAAd,CAAJ,EAA2B;AAChCpC,OAAG,CAACkB,IAAJ,CAASC,MAAT,CACIiB,MAAM,CAACT,MAAP,KAAkBQ,KAAK,CAACR,MAD5B,EAEI,MAAM,wBAAwBS,MAAM,CAACT,MAAM,0BACvCQ,KAAK,CAACR,MAAM,iBAAiBO,aAAa,SAASC,KAAK,GAHhE;AAIA,WAAOC,MAAP;AACD,GANM,MAMA;AACL,UAAMI,MAAM,GAAiB,EAA7B,CADK,CAEL;;AACA,SAAK,MAAMC,IAAX,IAAmBN,KAAnB,EAA0B;AACxB,UAAIC,MAAM,CAACK,IAAD,CAAN,IAAgB,IAApB,EAA0B;AACxB,cAAM,IAAIpC,UAAJ,CACF,kEACA,GAAG6B,aAAa,SAASO,IAAI,IAF3B,CAAN;AAGD;;AACDD,YAAM,CAACE,IAAP,CAAYN,MAAM,CAACK,IAAD,CAAlB;AACD;;AACD,WAAOD,MAAP;AACD;AACF;;AAED,SAASG,+BAAT,CACIC,IADJ,EAKqC;AAEnC,MAAIA,IAAI,CAACjB,MAAL,KAAgB,CAApB,EAAuB;AACrB,UAAM,IAAIvB,mBAAJ,CACF,wDADE,CAAN;AAED;;AACD,SAAO;AAACW,MAAE,EAAE6B,IAAI,CAAC,CAAD,CAAT;AAAc5B,MAAE,EAAE4B,IAAI,CAAC,CAAD;AAAtB,GAAP;AACD;;AAED,OAAO,eAAeC,UAAf,EACH;AACA;AACA;AACAhC,KAJG,EAISiC,OAJT,EAKHC,IALG,EAKyB;AAC9B,QAAMC,kBAAkB,GAAGD,IAAI,CAACE,eAAL,IAAwB,IAAnD;AACAjD,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACIN,KAAK,CAACqC,SAAN,IAAmB,IADvB,EAEI,MAAM,2DACF,0CAHR;AAKAlD,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACI4B,IAAI,IAAI,IADZ,EAEI,MAAM,8DACF,sCAHR;AAIA/C,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACI4B,IAAI,CAACI,MAAL,IAAe,IAAf,IAAuBJ,IAAI,CAACI,MAAL,GAAc,CAArC,IAA0CC,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACI,MAAtB,CAD9C,EAEI,MAAM,kEACF,oBAAoBJ,IAAI,CAACI,MAAM,EAHvC;AAIAnD,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACI,CAAC6B,kBAAD,IACKD,IAAI,CAACE,eAAL,GAAuB,CAAvB,IAA4BG,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACE,eAAtB,CAFrC,EAGI,MAAM,kEACF,0CAA0CF,IAAI,CAACE,eAAe,EAJtE;AAKAjD,KAAG,CAACkB,IAAJ,CAASC,MAAT,EACI;AACC4B,MAAY,CAAC,iBAAD,CAAZ,IAAmC,IAFxC,EAGI,MAAM,2DACF,6BAJR;;AAMA,MAAIlC,KAAK,CAACyC,UAAV,EAAsB;AACpB,UAAM,IAAIC,KAAJ,CACF,8DADE,CAAN;AAED;;AACD1C,OAAK,CAACyC,UAAN,GAAmB,IAAnB;;AAEA,MAAI;AACF,UAAME,YAAY,GAAGT,IAAI,CAACU,cAAL,IAAuB,IAA5C;AACA,QAAIC,KAAJ;AACA,QAAIC,KAAJ;;AACA,QAAIH,YAAJ,EAAkB;AAChB,UAAII,eAAe,CAACb,IAAI,CAACU,cAAN,CAAnB,EAA0C;AACxCzD,WAAG,CAACkB,IAAJ,CAASC,MAAT,CACI4B,IAAI,CAACc,iBAAL,IAA0B,IAA1B,IACKd,IAAI,CAACc,iBAAL,GAAyB,CAAzB,IACAT,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAACc,iBAAtB,CAHT,EAII,MAAM,qDACF,2DADE,GAEF,+BAFE,GAGF,WAAWd,IAAI,CAACc,iBAAiB,EAPzC;AAQD,OATD,MASO;AACL,cAAMJ,cAAc,GAAGd,+BAA+B,CAClDI,IAAI,CAACU,cAD6C,CAAtD;AAOAC,aAAK,GAAGD,cAAc,CAAC1C,EAAvB;AACA4C,aAAK,GAAGF,cAAc,CAACzC,EAAvB;AACD;AACF;;AAED,UAAM8C,aAAa,GAAGjD,KAAK,CAACkD,iBAAN,EAAtB;AACA,UAAMC,SAAS,GAAGnD,KAAK,CAACoD,sBAAN,EAAlB;AAEA,QAAIC,eAAJ;;AACA,QAAIV,YAAJ,EAAkB;AAChBU,qBAAe,GACXF,SAAS,CAACG,KAAV,GAAkBC,MAAlB,CAAyBJ,SAAS,CAACK,GAAV,CAAcC,CAAC,IAAI,SAASA,CAA5B,CAAzB,CADJ;AAED,KAHD,MAGO;AACLJ,qBAAe,GAAGF,SAAS,CAACG,KAAV,EAAlB;AACD;;AAED,UAAMI,SAAS,GAAGpE,oBAAoB,CAAC4C,IAAI,CAACwB,SAAN,EAAiBxB,IAAI,CAACyB,UAAtB,CAAtC;AACA,UAAMC,OAAO,GAAG1B,IAAI,CAAC0B,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B1B,IAAI,CAAC0B,OAAhD;AACA,UAAM;AAACC,kBAAD;AAAeC;AAAf,QAA0BzE,kBAAkB,CAC9CqE,SAD8C,EACnCE,OADmC,EAC1B1B,IAAI,CAACI,MADqB,EACb,IADa,EACP,IADO,EAE9CyB,gBAAgB,CAAC9B,OAAD,EAAUC,IAAV,CAF8B,EAG9C,IAH8C,EAGvC;AACPS,gBAJ8C,EAIhCU,eAJgC,CAAlD;AAKAQ,gBAAY,CAACG,QAAb,CAAsBhE,KAAtB;AACAA,SAAK,CAAC8D,OAAN,GAAgBA,OAAhB;AAEA,UAAMD,YAAY,CAACI,YAAb,EAAN;AACAjE,SAAK,CAACkE,aAAN,GAAsB,KAAtB;AACA,QAAIC,KAAK,GAAGjC,IAAI,CAACkC,YAAL,IAAqB,IAArB,GAA4B,CAA5B,GAAgClC,IAAI,CAACkC,YAAjD;AAEA,QAAIC,YAAY,GAAG,MAAMpC,OAAO,CAACqC,QAAR,EAAzB;;AACA,WAAOH,KAAK,GAAGjC,IAAI,CAACI,MAApB,EAA4B;AAC1B,YAAMiC,SAAS,GAAmB,EAAlC;AACA,YAAMV,YAAY,CAACW,YAAb,CAA0BL,KAA1B,CAAN;AACA,UAAIM,SAAS,GAAG,CAAhB;AACA,UAAIC,UAAU,GAAG,CAAjB;;AACA,UAAI,CAACvC,kBAAL,EAAyB;AACvBkC,oBAAY,GAAG,MAAMpC,OAAO,CAACqC,QAAR,EAArB;AACD;;AACD,aAAOnC,kBAAkB,GAAGsC,SAAS,GAAGvC,IAAI,CAACE,eAApB,GAAsC,IAA/D,EAAqE;AACnE,cAAMnC,WAAW,GAAG,MAAMoE,YAAY,CAACM,IAAb,EAA1B,CADmE,CAGnE;AACA;;AACA,YAAIxC,kBAAkB,IAAIlC,WAAW,CAAC2E,IAAtC,EAA4C;AAC1CC,iBAAO,CAACC,IAAR,CACI,uCACA,GAAG5C,IAAI,CAACE,eAAe,IADvB,GAEA,kDAFA,GAGA,GAAGqC,SAAS,YAHZ,GAIA,6CAJA,GAKA,2DALA,GAMA,yBANA,GAOA,GAAGvC,IAAI,CAACE,eAAL,GAAuBF,IAAI,CAACI,MAAM,aAPrC,GAQA,0DARA,GASA,eAVJ;AAWA;AACD;;AAED,YAAIrC,WAAW,CAAC8E,KAAZ,IAAqB,IAAzB,EAA+B;AAC7B,gBAAM;AAAC7E,cAAD;AAAKC;AAAL,cACFJ,6BAA6B,CAACC,KAAD,EAAQC,WAAW,CAAC8E,KAApB,CADjC;AAEA,gBAAMC,SAAS,GAAmB,EAAlC;AACAA,mBAAS,CAAC,OAAD,CAAT,GAAqBN,UAArB;AACAM,mBAAS,CAAC,MAAD,CAAT,GAAoB9E,EAAE,CAAC,CAAD,CAAF,CAAMW,KAAN,CAAY,CAAZ,CAApB;AAEA,gBAAMgD,YAAY,CAACoB,YAAb,CAA0BP,UAA1B,EAAsCM,SAAtC,CAAN;AAEA,gBAAME,aAAa,GAAiB,EAApC;;AACA,cAAIhD,IAAI,CAACiD,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,kBAAMC,oBAAoB,GACtBxF,uBAAuB,CAACsC,IAAI,CAACiD,WAAN,EAAmBnF,KAAK,CAACW,WAAzB,CAD3B;;AAEA,iBAAK,IAAI0E,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGD,oBAAoB,CAACtE,MAAzC,EAAiD,EAAEuE,CAAnD,EAAsD;AACpDH,2BAAa,CAACrD,IAAd,CAAmB,MAAMhC,kBAAkB,CACvCM,EAAE,CAACkF,CAAD,CADqC,EAChC,IADgC,EAC1BD,oBAAoB,CAACC,CAAD,CADM,CAA3C;AAED;AACF,WAjB4B,CAmB7B;;;AACA,gBAAMC,GAAG,GAAGpF,EAAE,CAACqD,MAAH,CAAUpD,EAAV,EAAcoD,MAAd,CAAqB2B,aAArB,CAAZ;AACA,gBAAMK,IAAI,GAAGtC,aAAa,CAACqC,GAAD,CAA1B;AACAnG,aAAG,CAACqG,OAAJ,CAAYF,GAAZ;;AACA,eAAK,IAAID,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGlC,SAAS,CAACrC,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;AACzC,kBAAMI,KAAK,GAAGtC,SAAS,CAACkC,CAAD,CAAvB;AACA,kBAAMK,GAAG,GAAGH,IAAI,CAACF,CAAD,CAAhB;AACAL,qBAAS,CAACS,KAAD,CAAT,GAAmBC,GAAnB;AACAvG,eAAG,CAACwG,IAAJ,CAASD,GAAT;AACD;;AAED,gBAAM7B,YAAY,CAAC+B,UAAb,CAAwBlB,UAAxB,EAAoCM,SAApC,CAAN;AACAvF,8BAAoB,CAACuF,SAAD,CAApB;AAEAN,oBAAU;AACVD,mBAAS;AACV;;AAED,YAAItC,kBAAkB,GAAGsC,SAAS,IAAIvC,IAAI,CAACE,eAArB,GACGnC,WAAW,CAAC2E,IADrC,EAC2C;AACzC;AACA,cAAIjC,YAAJ,EAAkB;AAChB,gBAAIkD,OAAJ;;AACA,gBAAI9C,eAAe,CAACb,IAAI,CAACU,cAAN,CAAnB,EAA0C;AACxCiD,qBAAO,GAAGlG,MAAM,CAAC,MAAMK,KAAK,CAAC8F,eAAN,CACnB5D,IAAI,CAACU,cADc,EACE;AAACmD,uBAAO,EAAE7D,IAAI,CAACc;AAAf,eADF,CAAP,CAAhB;AAED,aAHD,MAGO;AACL6C,qBAAO,GAAGlG,MAAM,CAACK,KAAK,CAACgG,QAAN,CAAenD,KAAf,EAAsBC,KAAtB,EAA6B;AAC5ClC,yBAAS,EAAEsB,IAAI,CAAC+D,mBAAL,IAA4B,IAA5B,GACPnG,6BADO,GAEPoC,IAAI,CAAC+D,mBAHmC;AAI5CrC,uBAAO,EAAE;AAJmC,eAA7B,CAAD,CAAhB;AAMD;;AACD,iBAAK,IAAIyB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGrF,KAAK,CAACkG,YAAN,CAAmBpF,MAAvC,EAA+C,EAAEuE,CAAjD,EAAoD;AAClDd,uBAAS,CAAC,OAAOvE,KAAK,CAACkG,YAAN,CAAmBb,CAAnB,CAAqB,EAA7B,CAAT,GAA4CQ,OAAO,CAACR,CAAD,CAAnD;AACD;AACF,WAlBwC,CAmBzC;AACA;AACA;AACA;AACA;;;AACA;AACD;;AAED,YAAIrF,KAAK,CAACkE,aAAV,EAAyB;AACvB;AACD;AACF;;AACD,YAAML,YAAY,CAACsC,UAAb,CAAwBhC,KAAxB,EAA+BI,SAA/B,CAAN;AACAJ,WAAK;;AACL,UAAInE,KAAK,CAACkE,aAAV,EAAyB;AACvB;AACD;AACF;;AACD,UAAML,YAAY,CAACuC,UAAb,EAAN;AACA,UAAMpG,KAAK,CAAC8D,OAAN,CAAcuC,QAAd,EAAN;AACA,WAAOrG,KAAK,CAAC8D,OAAb;AACD,GA/JD,SA+JU;AACR9D,SAAK,CAACyC,UAAN,GAAmB,KAAnB;AACD;AACF;AAED;;AACA,SAASsB,gBAAT,CACI9B,OADJ,EACyBC,IADzB,EACqD;AACnD;AACA,MAAIoE,aAAa,GAAW,IAA5B;;AACA,MAAIpE,IAAI,CAACE,eAAL,IAAwB,IAA5B,EAAkC;AAChCkE,iBAAa,GAAGpE,IAAI,CAACE,eAArB;AACD,GAFD,MAEO,IAAIG,MAAM,CAACgE,QAAP,CAAgBtE,OAAO,CAACuE,IAAxB,CAAJ,EAAmC;AACxCF,iBAAa,GAAGrE,OAAO,CAACuE,IAAxB;AACD;;AACD,SAAOF,aAAP;AACD,C,CAED;AACA;;;AACA,SAASvD,eAAT,CACId,OADJ,EAKc;AACZ,SAAQ,OAAQA,OAAsB,CAACqC,QAA/B,KAA4C,UAApD;AACD,C,CAED;AACA;;;AACA,SAASmC,oBAAT,CAAiCnC,QAAjC,EACgD;AAC9C,SAAQ,OAAQA,QAA4B,CAACK,IAArC,KAA8C,UAAtD;AACD;;AAED,OAAO,eAAemB,eAAf,EACH;AACA;AACA;AACA9F,KAJG,EAISiC,OAJT,EAKHC,IALG,EAK2B;AAChCA,MAAI,GAAGA,IAAI,IAAI,EAAf;AACA,QAAMwE,UAAU,GAAGxE,IAAI,CAAC6D,OAAL,IAAgB,IAAnC;AACA,QAAMY,CAAC,GAAG3G,KAAK,CAAC4G,YAAhB;AACA,MAAIrB,IAAI,GAAiB,EAAzB;;AACA,MAAIrD,IAAI,CAAC0B,OAAL,GAAe,CAAnB,EAAsB;AACpB,UAAM,IAAIrE,mBAAJ,CAAwB,sCAAxB,CAAN;AACD;;AAEDJ,KAAG,CAACkB,IAAJ,CAASC,MAAT,CACI,CAACoG,UAAD,IAAgBxE,IAAI,CAAC6D,OAAL,GAAe,CAAf,IAAoBxD,MAAM,CAACC,SAAP,CAAiBN,IAAI,CAAC6D,OAAtB,CADxC,EAEI,MAAM,+DACF,YAAY/E,IAAI,CAACC,SAAL,CAAeiB,IAAI,CAAC6D,OAApB,CAA4B,EAHhD;AAIA,QAAM1B,YAAY,GAAGoC,oBAAoB,CAACxE,OAAD,CAApB,GACjBA,OADiB,GAEjB,MAAOA,OAAsB,CAACqC,QAAvB,EAFX,CAbgC,CAgBhC;;AACA,MAAIuC,WAAW,GAAG,CAAlB;AACA,MAAIC,KAAK,GAAG,CAAZ;;AAEA,SAAOJ,UAAU,GAAGI,KAAK,GAAG5E,IAAI,CAAC6D,OAAhB,GAA0B,IAA3C,EAAiD;AAC/C,UAAM9F,WAAW,GAAG,MAAMoE,YAAY,CAACM,IAAb,EAA1B;AACAY,QAAI,GAAGpG,GAAG,CAAC4H,IAAJ,CAAS,MAAK;AACnB,UAAI9G,WAAW,CAAC8E,KAAhB,EAAuB;AACrB;AACA;AACA,cAAM;AAAC7E,YAAD;AAAKC;AAAL,YACFJ,6BAA6B,CAACC,KAAD,EAAQC,WAAW,CAAC8E,KAApB,CADjC;AAEA,cAAMiC,OAAO,GAAG9G,EAAE,CAACqD,MAAH,CAAUpD,EAAV,CAAhB;AACA,cAAM8G,SAAS,GAAG9H,GAAG,CAAC4H,IAAJ,CAAS,MAAMJ,CAAC,CAACK,OAAD,CAAhB,CAAlB;AACA7H,WAAG,CAACqG,OAAJ,CAAYwB,OAAZ;;AAEA,YAAIF,KAAK,KAAK,CAAd,EAAiB;AACf,eAAK,IAAIzB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG4B,SAAS,CAACnG,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;AACzCE,gBAAI,CAAC1D,IAAL,CAAUzC,MAAM,CAAC,CAAD,CAAhB;AACD;AACF;;AAED,cAAMwB,SAAS,GAAGoG,OAAO,CAAC,CAAD,CAAP,CAAWnG,KAAX,CAAiB,CAAjB,CAAlB;;AACA,aAAK,IAAIwE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG4B,SAAS,CAACnG,MAA9B,EAAsC,EAAEuE,CAAxC,EAA2C;AACzC,gBAAM6B,QAAQ,GAAGD,SAAS,CAAC5B,CAAD,CAA1B;AACA,gBAAM8B,SAAS,GAAG5B,IAAI,CAACF,CAAD,CAAtB;AACAE,cAAI,CAACF,CAAD,CAAJ,GACIlG,GAAG,CAAC4H,IAAJ,CAAS,MAAM5H,GAAG,CAACiI,GAAJ,CAAQ7B,IAAI,CAACF,CAAD,CAAZ,EAAiBlG,GAAG,CAACkI,GAAJ,CAAQzG,SAAR,EAAmBsG,QAAnB,CAAjB,CAAf,CADJ;;AAEA,cAAIJ,KAAK,GAAG,CAAZ,EAAe;AACb3H,eAAG,CAACqG,OAAJ,CAAY2B,SAAZ;AACD;AACF;;AACDhI,WAAG,CAACqG,OAAJ,CAAYyB,SAAZ;AACAJ,mBAAW,IAAIjG,SAAf;AAEA,UAAEkG,KAAF;AACD;;AACD,aAAOvB,IAAP;AACD,KAhCM,CAAP;;AAkCA,QAAItF,WAAW,CAAC2E,IAAhB,EAAsB;AACpB,UAAI8B,UAAJ,EAAgB;AACd7B,eAAO,CAACC,IAAR,CACI,qEACA,8CADA,GAEA,0CAFA,GAGA,0BAA0B5C,IAAI,CAAC6D,OAAO,aAHtC,GAIA,0DAJA,GAKA,eANJ;AAOD;;AACD;AACD;AACF;;AAED,OAAK,IAAIV,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGE,IAAI,CAACzE,MAAzB,EAAiC,EAAEuE,CAAnC,EAAsC;AACpC,UAAM8B,SAAS,GAAG5B,IAAI,CAACF,CAAD,CAAtB;AACAE,QAAI,CAACF,CAAD,CAAJ,GAAUlG,GAAG,CAACmI,GAAJ,CAAQ/B,IAAI,CAACF,CAAD,CAAZ,EAAiBwB,WAAjB,CAAV;AACA1H,OAAG,CAACqG,OAAJ,CAAY2B,SAAZ;AACD;;AAED,SAAOzH,gBAAgB,CAAC6F,IAAD,CAAvB;AACD","names":["tfc","scalar","configureCallbacks","standardizeCallbacks","NotImplementedError","ValueError","disposeTensorsInLogs","singletonOrArray","toList","standardizeClassWeights","standardizeWeights","DEFAULT_VALIDATION_BATCH_SIZE","standardizeDataIteratorOutput","model","iteratorOut","xs","ys","iteratorOutObj","util","assert","flattenedXs","flattenTensorOrArrayOrMap","inputNames","flattenedYs","outputNames","batchSize","shape","length","inputs","JSON","stringify","outputs","xIndex","yIndex","inputOrOutput","names","values","Tensor","Array","isArray","result","name","push","standardizeTensorValidationData","data","fitDataset","dataset","args","hasBatchesPerEpoch","batchesPerEpoch","optimizer","epochs","Number","isInteger","isTraining","Error","doValidation","validationData","valXs","valYs","isDatasetObject","validationBatches","trainFunction","makeTrainFunction","outLabels","getDedupedMetricsNames","callbackMetrics","slice","concat","map","n","callbacks","yieldEvery","verbose","callbackList","history","getStepsPerEpoch","setModel","onTrainBegin","stopTraining_","epoch","initialEpoch","dataIterator","iterator","epochLogs","onEpochBegin","stepsDone","batchIndex","next","done","console","warn","value","batchLogs","onBatchBegin","sampleWeights","classWeight","standardClassWeights","i","ins","outs","dispose","label","out","keep","onBatchEnd","valOuts","evaluateDataset","batches","evaluate","validationBatchSize","metricsNames","onEpochEnd","onTrainEnd","syncData","stepsPerEpoch","isFinite","size","isLazyIteratorObject","hasBatches","f","testFunction","numExamples","batch","tidy","xsAndYs","batchOuts","batchOut","oldScalar","add","mul","div"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/engine/training_dataset.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {scalar} from '@tensorflow/tfjs-core';\nimport {BaseCallback, configureCallbacks, CustomCallbackArgs, History, ModelLoggingVerbosity, standardizeCallbacks, YieldEveryOptions} from '../base_callbacks';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {disposeTensorsInLogs, UnresolvedLogs} from '../logs';\nimport {TensorOrArrayOrMap} from '../types';\nimport {singletonOrArray, toList} from '../utils/generic_utils';\n\nimport {Dataset, LazyIterator} from './dataset_stub';\nimport {ClassWeight, ClassWeightMap, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Interface for configuring model training based on a dataset object.\n */\nexport interface ModelFitDatasetArgs<T> {\n  /**\n   * (Optional) Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. It should\n   * typically be equal to the number of samples of your dataset divided by\n   * the batch size, so that `fitDataset`() call can utilize the entire dataset.\n   * If it is not provided, use `done` return value in `iterator.next()` as\n   * signal to finish an epoch.\n   */\n  batchesPerEpoch?: number;\n\n  /**\n   * Integer number of times to iterate over the training dataset.\n   */\n  epochs: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be any of the following:\n   *\n   *   - An array `[xVal, yVal]`, where the two values may be `tf.Tensor`,\n   *     an array of Tensors, or a map of string to Tensor.\n   *   - Similarly, an array ` [xVal, yVal, valSampleWeights]`\n   *     (not implemented yet).\n   *   - a `Dataset` object with elements of the form `{xs: xVal, ys: yVal}`,\n   *     where `xs` and `ys` are the feature and label tensors, respectively.\n   *\n   * If `validationData` is an Array of Tensor objects, each `tf.Tensor` will be\n   * sliced into batches during validation, using the parameter\n   * `validationBatchSize` (which defaults to 32). The entirety of the\n   * `tf.Tensor` objects will be used in the validation.\n   *\n   * If `validationData` is a dataset object, and the `validationBatches`\n   * parameter is specified, the validation will use `validationBatches` batches\n   * drawn from the dataset object. If `validationBatches` parameter is not\n   * specified, the validation will stop when the dataset is exhausted.\n   *\n   * The model will not be trained on this data.\n   */\n  validationData?: [\n    TensorOrArrayOrMap, TensorOrArrayOrMap\n  ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|Dataset<T>;\n\n  /**\n   * Optional batch size for validation.\n   *\n   * Used only if `validationData` is an array of `tf.Tensor` objects, i.e., not\n   * a dataset object.\n   *\n   * If not specified, its value defaults to 32.\n   */\n  validationBatchSize?: number;\n\n  /**\n   * (Optional) Only relevant if `validationData` is specified and is a dataset\n   * object.\n   *\n   * Total number of batches of samples to draw from `validationData` for\n   * validation purpose before stopping at the end of every epoch. If not\n   * specified, `evaluateDataset` will use `iterator.next().done` as signal to\n   * stop validation.\n   */\n  validationBatches?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - a `number`: Will yield every `number` milliseconds.\n   *   - `'never'`: never yield. (But yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run). When this is used, `epochs` is the index of the \"final epoch\".\n   * The model is not trained for a number of iterations given by `epochs`,\n   * but merely until the epoch of index `epochs` is reached.\n   */\n  initialEpoch?: number;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or a object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n}\n\nexport interface FitDatasetElement {\n  xs: TensorOrArrayOrMap;\n  ys: TensorOrArrayOrMap;\n}\n\n/**\n * Interface for configuring model evaluation based on a dataset object.\n */\nexport interface ModelEvaluateDatasetArgs {\n  /**\n   * Number of batches to draw from the dataset object before ending the\n   * evaluation.\n   */\n  batches?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n}\n\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, iteratorOut: {}): {xs: tfc.Tensor[], ys: tfc.Tensor[]} {\n  let xs: TensorOrArrayOrMap;\n  let ys: TensorOrArrayOrMap;\n\n  const iteratorOutObj = iteratorOut as FitDatasetElement;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(\n      xs != null && ys != null,\n      () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n          'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n          'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n          'string to Tensor.  The provided Dataset instead generates ' +\n          `${iteratorOut}`);\n\n  const flattenedXs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n\n  const batchSize: number = flattenedXs[0].shape[0];\n\n  tfc.util.assert(\n      flattenedXs.length === model.inputs.length,\n      () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n          `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n          `${JSON.stringify(model.inputNames)})`);\n\n  tfc.util.assert(\n      flattenedYs.length === model.outputs.length,\n      () =>\n          `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n          `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n          `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(\n        flattenedXs[xIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${\n                  flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(\n        flattenedYs[yIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${\n                  flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {xs: flattenedXs, ys: flattenedYs};\n}\n\nfunction flattenTensorOrArrayOrMap(\n    inputOrOutput: string, names: string[], values: TensorOrArrayOrMap) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(\n        values.length === names.length,\n        () => `Received an array of ${values.length} Tensors, but expected ${\n            names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result: tfc.Tensor[] = [];\n    // Check that all the required keys are available.\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(\n            `The feature data generated by the dataset lacks the required ` +\n            `${inputOrOutput} key '${name}'.`);\n      }\n      result.push(values[name]);\n    }\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData<T>(\n    data:\n        [\n          tfc.Tensor|tfc.Tensor[], tfc.Tensor|tfc.Tensor[]\n        ]|[tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n           tfc.Tensor | tfc.Tensor[]]):\n    {xs: tfc.Tensor|tfc.Tensor[], ys: tfc.Tensor|tfc.Tensor[]} {\n  if (data.length === 3) {\n    throw new NotImplementedError(\n        'Validation with sample weights is not implemented yet.');\n  }\n  return {xs: data[0], ys: data[1]};\n}\n\nexport async function fitDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>,\n    args: ModelFitDatasetArgs<T>): Promise<History> {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(\n      model.optimizer != null,\n      () => 'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileConfig).');\n\n  tfc.util.assert(\n      args != null,\n      () => `For fitDataset(), the 2nd argument (config) is required, ` +\n          `but it is not provided in this call.`);\n  tfc.util.assert(\n      args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs),\n      () => `For fitDataset(), config.epochs is expected to be a positive ` +\n          `integer, but got ${args.epochs}`);\n  tfc.util.assert(\n      !hasBatchesPerEpoch ||\n          (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)),\n      () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n          `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert(\n      // tslint:disable-next-line:no-any\n      (args as any)['validationSplit'] == null,\n      () => '`validationSplit` is not supported by `fitDataset()`. ' +\n          'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error(\n        'Cannot start training because another fit() call is ongoing.');\n  }\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs: tfc.Tensor|tfc.Tensor[];\n    let valYs: tfc.Tensor|tfc.Tensor[];\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(\n            args.validationBatches == null ||\n                (args.validationBatches > 0 &&\n                 Number.isInteger(args.validationBatches)),\n            () => `For fitDataset() with dataset-based validation, ` +\n                `config.validationBatches is expected not to be provided, ` +\n                `or to be a positive integer, ` +\n                `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(\n            args.validationData as\n                    [tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[]] |\n            [\n              tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n              tfc.Tensor | tfc.Tensor[]\n            ]);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames() as string[];\n\n    let callbackMetrics: string[];\n    if (doValidation) {\n      callbackMetrics =\n          outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, args.epochs, null, null,\n        getStepsPerEpoch(dataset, args),\n        null,  // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n\n    let dataIterator = await dataset.iterator();\n    while (epoch < args.epochs) {\n      const epochLogs: UnresolvedLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next();\n\n        // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn(\n              'You provided `batchesPerEpoch` as ' +\n              `${args.batchesPerEpoch}, ` +\n              'but your dataset iterator ran out of data after ' +\n              `${stepsDone} batches; ` +\n              'interrupting training. Make sure that your ' +\n              'dataset can generate at least `batchesPerEpoch * epochs` ' +\n              'batches (in this case, ' +\n              `${args.batchesPerEpoch * args.epochs} batches). ` +\n              'You may need to use the repeat() function when building ' +\n              'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {xs, ys} =\n              standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs: UnresolvedLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          const sampleWeights: tfc.Tensor[] = [];\n          if (args.classWeight != null) {\n            const standardClassWeights =\n                standardizeClassWeights(args.classWeight, model.outputNames);\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(\n                  ys[i], null, standardClassWeights[i]));\n            }\n          }\n\n          // Train on batch.\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                                 iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts: tfc.Scalar[];\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(\n                  args.validationData, {batches: args.validationBatches}));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ?\n                    DEFAULT_VALIDATION_BATCH_SIZE :\n                    args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          }\n          // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch<T>(\n    dataset: Dataset<T>, args: ModelFitDatasetArgs<T>): number {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch: number = null;\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n  return stepsPerEpoch;\n}\n\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject<T>(\n    dataset:\n        [\n          TensorOrArrayOrMap, TensorOrArrayOrMap\n        ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|\n    Dataset<T>): boolean {\n  return (typeof (dataset as Dataset<T>).iterator === 'function');\n}\n\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject<T>(iterator: Dataset<T>|\n                                 LazyIterator<T>): boolean {\n  return (typeof (iterator as LazyIterator<T>).next === 'function');\n}\n\nexport async function evaluateDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>|LazyIterator<T>,\n    args: ModelEvaluateDatasetArgs): Promise<tfc.Scalar|tfc.Scalar[]> {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs: tfc.Scalar[] = [];\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(\n      !hasBatches || (args.batches > 0 && Number.isInteger(args.batches)),\n      () => 'Test loop expects `batches` to be a positive integer, but ' +\n          `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ?\n      dataset as LazyIterator<T>:\n      await (dataset as Dataset<T>).iterator();\n  // Keeps track of number of examples used in this evaluation.\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {xs, ys} =\n            standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] =\n              tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n\n        ++batch;\n      }\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn(\n            'Your dataset iterator ran out of data during evaluateDataset(). ' +\n            'Interrupting evalution. Make sure that your ' +\n            'dataset can generate at least `batches` ' +\n            `batches (in this case, ${args.batches} batches). ` +\n            'You may need to use the repeat() function when building ' +\n            'your dataset.');\n      }\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}\n"]},"metadata":{},"sourceType":"module"}