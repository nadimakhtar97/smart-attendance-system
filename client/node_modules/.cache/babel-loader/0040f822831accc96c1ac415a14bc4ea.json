{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using tf.Tensor objects.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { Tensor, tensor1d, util } from '@tensorflow/tfjs-core';\nimport { expandDims, gather, sliceAlongFirstAxis } from '../backend/tfjs_backend';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { range } from '../utils/math_utils';\nexport function checkBatchSize(batchSize) {\n  tfc.util.assert(batchSize > 0 && Number.isInteger(batchSize), () => `batchSize is required to be a positive integer, but got ${batchSize}`);\n}\n/**\n * Slice a Tensor or an Array of Tensors, by start and stop indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArraysByIndices()` together.\n *\n * @param arrays: the input.\n * @param start: the starting index (inclusive).\n * @param stop: the stopping index (exclusive).\n * @returns The result of the slicing. If `arrays` is an `Array` of\n *   `tf.Tensor`s, the slicing will be applied to all elements of the `Array`\n *   in the same way.\n */\n\nexport function sliceArrays(arrays, start, stop) {\n  if (arrays == null) {\n    return [null];\n  } else if (Array.isArray(arrays)) {\n    return arrays.map(array => sliceAlongFirstAxis(array, start, stop - start));\n  } else {\n    // Tensor.\n    return sliceAlongFirstAxis(arrays, start, stop - start);\n  }\n}\n/**\n * Slice a Tensor or an Array of Tensors, by random-order indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArrays()` together.\n *\n * @param arrays The input `tf.Tensor` or `Array` of `tf.Tensor`s to slice.\n *   If an `Array` of `tf.Tensor`s, all `tf.Tensor`s will be sliced in the\n *   same fashion.\n * @param indices The indices to use for slicing along the first (batch)\n *   dimension.\n * @returns Result(s) of the slicing.\n */\n\nexport function sliceArraysByIndices(arrays, indices) {\n  return tfc.tidy(() => {\n    if (arrays == null) {\n      return null;\n    } else if (Array.isArray(arrays)) {\n      return arrays.map(array => sliceArraysByIndices(array, indices));\n    } else {\n      // TODO(cais): indices should be a pre-constructed Tensor1D to avoid\n      //   tensor1d() calls.\n      return gather(arrays, indices.dtype === 'int32' ? indices : tfc.cast(indices, 'int32'));\n    }\n  });\n}\n/**\n * Returns a list of batch indices (tuples of indices).\n * @param size: Integer, total size of the data to slice into batches.\n * @param batchSize: Integer, batch size.\n * @returns An Array of [batchStart, batchEnd] tuples. batchStart is\n *   inclusive; batchEnd is exclusive. I.e., each batch consists of indices x\n *   that satisfy batchStart <= x < batchEnd.\n */\n\nexport function makeBatches(size, batchSize) {\n  const output = [];\n  let batchStart = 0;\n  let batchEnd = null;\n\n  while (batchStart < size) {\n    batchEnd = batchStart + batchSize;\n\n    if (batchEnd >= size) {\n      batchEnd = size;\n    }\n\n    output.push([batchStart, batchEnd]);\n    batchStart = batchEnd;\n  }\n\n  return output;\n}\n/**\n * Abstract fit function for `f(ins)`.\n * @param f A Function returning a list of tensors. For training, this\n *   function is expected to perform the updates to the variables.\n * @param ins List of tensors to be fed to `f`.\n * @param outLabels List of strings, display names of the outputs of `f`.\n * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n * @param epochs Number of times to iterate over the data. Default : 1.\n * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n * @param callbacks List of callbacks to be called during training.\n * @param valF Function to call for validation.\n * @param valIns List of tensors to be fed to `valF`.\n * @param shuffle Whether to shuffle the data at the beginning of every\n * epoch. Default : true.\n * @param callbackMetrics List of strings, the display names of the metrics\n *   passed to the callbacks. They should be the concatenation of the\n *   display names of the outputs of `f` and the list of display names\n *   of the outputs of `valF`.\n * @param initialEpoch Epoch at which to start training (useful for\n *   resuming a previous training run). Default : 0.\n * @param stepsPerEpoch Total number of steps (batches on samples) before\n *   declaring one epoch finished and starting the next epoch. Ignored with\n *   the default value of `undefined` or `null`.\n * @param validationSteps Number of steps to run validation for (only if\n *   doing validation from data tensors). Not applicable for tfjs-layers.\n * @returns A `History` object.\n */\n\nasync function fitLoop( // Type `model` as `any` here to avoid circular dependency w/ training.ts.\n// tslint:disable-next-line:no-any\nmodel, f, ins, outLabels, batchSize, epochs, verbose, callbacks, valF, valIns, shuffle, callbackMetrics, initialEpoch, stepsPerEpoch, validationSteps) {\n  if (batchSize == null) {\n    batchSize = 32;\n  }\n\n  if (epochs == null) {\n    epochs = 1;\n  }\n\n  if (shuffle == null) {\n    shuffle = true;\n  }\n\n  if (initialEpoch == null) {\n    initialEpoch = 0;\n  } // TODO(cais): Change const to let below when implementing validation.\n\n\n  let doValidation = false;\n\n  if (valF != null && valIns != null) {\n    doValidation = true; // TODO(cais): verbose message.\n  }\n\n  if (validationSteps != null) {\n    doValidation = true;\n\n    if (stepsPerEpoch == null) {\n      throw new ValueError('Can only use `validationSteps` when doing step-wise training, ' + 'i.e., `stepsPerEpoch` must be set.');\n    }\n  }\n\n  const numTrainSamples = model.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n  let indexArray;\n\n  if (numTrainSamples != null) {\n    indexArray = range(0, numTrainSamples);\n  }\n\n  if (verbose == null) {\n    verbose = 1;\n  }\n\n  const {\n    callbackList,\n    history\n  } = configureCallbacks(callbacks, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n  callbackList.setModel(model);\n  model.history = history;\n  await callbackList.onTrainBegin();\n  model.stopTraining_ = false; // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n  // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n\n  for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n    await callbackList.onEpochBegin(epoch);\n    const epochLogs = {};\n\n    if (stepsPerEpoch != null) {\n      throw new NotImplementedError('stepsPerEpoch mode is not implemented yet.');\n    } else {\n      if (shuffle === 'batch') {\n        throw new NotImplementedError('batch shuffling is not implemneted yet');\n      } else if (shuffle) {\n        util.shuffle(indexArray);\n      } // Convert the potentially shuffled indices to Tensor1D, to avoid the\n      // cost of repeated creation of Array1Ds later on.\n\n\n      const epochIndexArray1D = tensor1d(indexArray);\n      const batches = makeBatches(numTrainSamples, batchSize);\n\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchLogs = {};\n        await callbackList.onBatchBegin(batchIndex, batchLogs);\n        tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds = sliceAlongFirstAxis(epochIndexArray1D, batchStart, batchEnd - batchStart);\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = batchEnd - batchStart; // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n\n          const insBatch = sliceArraysByIndices(ins, batchIds);\n          const outs = f(insBatch);\n\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out); // TODO(cais): Use scope() to avoid ownership.\n          }\n\n          if (batchIndex === batches.length - 1) {\n            // Last batch.\n            if (doValidation) {\n              const valOuts = model.testLoop(valF, valIns, batchSize); // Porting Notes: In tfjs-layers, valOuts is always an Array.\n\n              for (let i = 0; i < outLabels.length; ++i) {\n                const label = outLabels[i];\n                const out = valOuts[i];\n                tfc.keep(out); // TODO(cais): Use scope() to avoid ownership.\n\n                epochLogs['val_' + label] = out;\n              }\n            }\n          }\n        });\n        await callbackList.onBatchEnd(batchIndex, batchLogs);\n        disposeTensorsInLogs(batchLogs);\n\n        if (model.stopTraining_) {\n          break;\n        } // TODO(cais): return outs as list of Tensor.\n\n      }\n\n      epochIndexArray1D.dispose();\n    } // TODO(cais): Run validation at the end of the epoch.\n\n\n    await callbackList.onEpochEnd(epoch, epochLogs);\n\n    if (model.stopTraining_) {\n      break;\n    }\n  }\n\n  await callbackList.onTrainEnd();\n  await model.history.syncData();\n  return model.history;\n}\n\nexport async function fitTensors( // Type `model` as `any` here to avoid circular dependency w/ training.ts.\n// tslint:disable-next-line:no-any\nmodel, x, y) {\n  let args = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};\n\n  if (model.isTraining) {\n    throw new Error('Cannot start training because another fit() call is ongoing.');\n  }\n\n  model.isTraining = true;\n  let inputs;\n  let targets;\n  let originalInputs;\n  let originalTargets;\n  let inputValX;\n  let inputValY;\n  let valX;\n  let valY;\n  let sampleWeights;\n\n  try {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize); // Validate user data.\n    // TODO(cais): Support sampleWeight.\n\n    const checkBatchAxis = false;\n    const standardizedOuts = await model.standardizeUserData(x, y, args.sampleWeight, args.classWeight, checkBatchAxis, batchSize);\n    inputs = standardizedOuts[0];\n    targets = standardizedOuts[1];\n    sampleWeights = standardizedOuts[2]; // Prepare validation data.\n\n    let doValidation = false;\n    let valIns;\n\n    if (args.validationData != null && args.validationData.length > 0) {\n      doValidation = true;\n\n      if (args.validationData.length === 2) {\n        // config.validationData consists of valX and valY.\n        inputValX = args.validationData[0];\n        inputValY = args.validationData[1];\n      } else if (args.validationData.length === 3) {\n        throw new NotImplementedError('validationData including sample weights is not supported yet.');\n      } else {\n        throw new ValueError(`When passing validation data, it must contain 2 (valX, valY) ` + `or 3 (valX, valY, valSampleWeight) items; ` + `${args.validationData} is invalid.`);\n      }\n\n      const checkBatchAxis = true;\n      const valStandardized = await model.standardizeUserData(inputValX, inputValY, null,\n      /** Unused sample weights. */\n      null,\n      /** Unused class weights. */\n      checkBatchAxis, batchSize);\n      valX = valStandardized[0];\n      valY = valStandardized[1];\n      valIns = valX.concat(valY); // TODO(cais): Add useLearningPhase data properly.\n    } else if (args.validationSplit != null && args.validationSplit > 0 && args.validationSplit < 1) {\n      doValidation = true; // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n\n      const splitAt = Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n      const originalBatchSize = inputs[0].shape[0];\n      valX = sliceArrays(inputs, splitAt, originalBatchSize);\n      originalInputs = inputs;\n      inputs = sliceArrays(inputs, 0, splitAt);\n      valY = sliceArrays(targets, splitAt, originalBatchSize);\n      originalTargets = targets;\n      targets = sliceArrays(targets, 0, splitAt); // TODO(cais): Once sampleWeights becomes available, slice it to get\n      //   valSampleWeights.\n\n      valIns = valX.concat(valY); // TODO(cais): Add useLearningPhase data properly.\n    } else if (args.validationSteps != null) {\n      doValidation = true; // TODO(cais): Add useLearningPhase.\n    }\n\n    const ins = inputs.concat(targets).concat(sampleWeights);\n    model.checkTrainableWeightsConsistency(); // TODO(cais): Handle use_learning_phase and learning_phase?\n    // Porting Note: Here we see a key deviation of tfjs-layers from\n    // Keras.\n    //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n    //  we do not construct symbolic computation graphs to embody the\n    //  training process. Instead, we define a function that performs the\n    //  training action. In PyKeras, the data (inputs and targets) are fed\n    //  through graph placeholders. In tfjs-layers, the data are fed as\n    //  function arguments. Since the function are defined below in the\n    //  scope, we don't have equivalents of PyKeras's\n    //  `_make_train_funciton`.\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames();\n    let valFunction;\n    let callbackMetrics;\n\n    if (doValidation) {\n      model.makeTestFunction();\n      valFunction = model.testFunction;\n      callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      valFunction = null;\n      valIns = [];\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const out = await fitLoop(model, trainFunction, ins, outLabels, batchSize, args.epochs, args.verbose, callbacks, valFunction, valIns, args.shuffle, callbackMetrics, args.initialEpoch, null, null);\n    return out;\n  } finally {\n    model.isTraining = false; // Memory clean up.\n\n    disposeNewTensors(inputs, x);\n    disposeNewTensors(targets, y);\n    disposeNewTensors(originalInputs, x);\n    disposeNewTensors(originalTargets, y);\n    disposeNewTensors(valX, inputValX);\n    disposeNewTensors(valY, inputValY);\n\n    if (sampleWeights != null) {\n      tfc.dispose(sampleWeights);\n    }\n  } // TODO(cais): Add value to outLabels.\n\n}\n/**\n * Ensure tensors all have a rank of at least 2.\n *\n * If a tensor has a rank of 1, it is dimension-expanded to rank 2.\n * If any tensor has a rank of 0 (i.e., is a scalar), an error will be thrown.\n */\n\nexport function ensureTensorsRank2OrHigher(tensors) {\n  const outs = [];\n\n  if (tensors instanceof Tensor) {\n    tensors = [tensors];\n  } // Make Tensors at least 2D.\n\n\n  for (let i = 0; i < tensors.length; ++i) {\n    const tensor = tensors[i];\n\n    if (tensor.rank === 1) {\n      outs.push(expandDims(tensor, 1));\n    } else if (tensor.rank === 0) {\n      throw new Error('Expected tensor to be at least 1D, but received a 0D tensor ' + '(scalar).');\n    } else {\n      outs.push(tensor);\n    }\n  }\n\n  return outs;\n}\n/**\n * Compare a set of tensors with a reference (old) set, discard the ones\n * in the new set that are not present in the reference set.\n *\n * This method is used for memory clenaup during calls such as\n * LayersModel.fit().\n *\n * @param tensors New set which may contain Tensors not present in\n *   `refTensors`.\n * @param refTensors Reference Tensor set.\n */\n// TODO(cais, kangyizhang): Deduplicate with tfjs-data.\n\nexport function disposeNewTensors(tensors, refTensors) {\n  if (tensors == null) {\n    return;\n  }\n\n  const oldTensorIds = [];\n\n  if (refTensors instanceof Tensor) {\n    oldTensorIds.push(refTensors.id);\n  } else if (Array.isArray(refTensors)) {\n    refTensors.forEach(t => oldTensorIds.push(t.id));\n  } else if (refTensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in refTensors) {\n      const oldTensor = refTensors[name];\n      oldTensorIds.push(oldTensor.id);\n    }\n  }\n\n  const tensorsToDispose = [];\n\n  if (tensors instanceof Tensor) {\n    if (oldTensorIds.indexOf(tensors.id) === -1) {\n      tensorsToDispose.push(tensors);\n    }\n  } else if (Array.isArray(tensors)) {\n    tensors.forEach(t => {\n      if (oldTensorIds.indexOf(t.id) === -1) {\n        tensorsToDispose.push(t);\n      }\n    });\n  } else if (tensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in tensors) {\n      const tensor = tensors[name];\n\n      if (oldTensorIds.indexOf(tensor.id) === -1) {\n        tensorsToDispose.push(tensor);\n      }\n    }\n  }\n\n  tensorsToDispose.forEach(t => {\n    if (!t.isDisposed) {\n      t.dispose();\n    }\n  });\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAgBC,MAAhB,EAAkCC,QAAlC,EAA4CC,IAA5C,QAAuD,uBAAvD;AAEA,SAAQC,UAAR,EAAoBC,MAApB,EAA4BC,mBAA5B,QAAsD,yBAAtD;AACA,SAAsBC,kBAAtB,EAA8FC,oBAA9F,QAA4I,mBAA5I;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AACA,SAAQC,oBAAR,QAAmD,SAAnD;AACA,SAAQC,KAAR,QAAoB,qBAApB;AA4IA,OAAM,SAAUC,cAAV,CAAyBC,SAAzB,EAA0C;AAC9Cd,KAAG,CAACG,IAAJ,CAASY,MAAT,CACID,SAAS,GAAG,CAAZ,IAAiBE,MAAM,CAACC,SAAP,CAAiBH,SAAjB,CADrB,EAEI,MAAM,2DACFA,SAAS,EAHjB;AAID;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUI,WAAV,CACFC,MADE,EACuBC,KADvB,EACsCC,IADtC,EACkD;AACtD,MAAIF,MAAM,IAAI,IAAd,EAAoB;AAClB,WAAO,CAAC,IAAD,CAAP;AACD,GAFD,MAEO,IAAIG,KAAK,CAACC,OAAN,CAAcJ,MAAd,CAAJ,EAA2B;AAChC,WAAOA,MAAM,CAACK,GAAP,CAAWC,KAAK,IAAInB,mBAAmB,CAACmB,KAAD,EAAQL,KAAR,EAAeC,IAAI,GAAGD,KAAtB,CAAvC,CAAP;AACD,GAFM,MAEA;AAAG;AACR,WAAOd,mBAAmB,CAACa,MAAD,EAASC,KAAT,EAAgBC,IAAI,GAAGD,KAAvB,CAA1B;AACD;AACF;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUM,oBAAV,CACFP,MADE,EACuBQ,OADvB,EACwC;AAC5C,SAAO3B,GAAG,CAAC4B,IAAJ,CAAS,MAAK;AACnB,QAAIT,MAAM,IAAI,IAAd,EAAoB;AAClB,aAAO,IAAP;AACD,KAFD,MAEO,IAAIG,KAAK,CAACC,OAAN,CAAcJ,MAAd,CAAJ,EAA2B;AAChC,aAAOA,MAAM,CAACK,GAAP,CACHC,KAAK,IAAKC,oBAAoB,CAACD,KAAD,EAAQE,OAAR,CAD3B,CAAP;AAED,KAHM,MAGA;AACL;AACA;AACA,aAAOtB,MAAM,CACTc,MADS,EAETQ,OAAO,CAACE,KAAR,KAAkB,OAAlB,GAA4BF,OAA5B,GAAsC3B,GAAG,CAAC8B,IAAJ,CAASH,OAAT,EAAkB,OAAlB,CAF7B,CAAb;AAGD;AACF,GAbM,CAAP;AAcD;AAED;;;;;;;;;AAQA,OAAM,SAAUI,WAAV,CACFC,IADE,EACYlB,SADZ,EAC6B;AACjC,QAAMmB,MAAM,GAA4B,EAAxC;AACA,MAAIC,UAAU,GAAG,CAAjB;AACA,MAAIC,QAAQ,GAAW,IAAvB;;AACA,SAAOD,UAAU,GAAGF,IAApB,EAA0B;AACxBG,YAAQ,GAAGD,UAAU,GAAGpB,SAAxB;;AACA,QAAIqB,QAAQ,IAAIH,IAAhB,EAAsB;AACpBG,cAAQ,GAAGH,IAAX;AACD;;AACDC,UAAM,CAACG,IAAP,CAAY,CAACF,UAAD,EAAaC,QAAb,CAAZ;AACAD,cAAU,GAAGC,QAAb;AACD;;AACD,SAAOF,MAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA,eAAeI,OAAf,EACI;AACA;AACAC,KAHJ,EAGgBC,CAHhB,EAGiDC,GAHjD,EAIIC,SAJJ,EAI0B3B,SAJ1B,EAI8C4B,MAJ9C,EAI+DC,OAJ/D,EAKIC,SALJ,EAKgCC,IALhC,EAMIC,MANJ,EAMuBC,OANvB,EAMiDC,eANjD,EAOIC,YAPJ,EAO2BC,aAP3B,EAQIC,eARJ,EAQ4B;AAC1B,MAAIrC,SAAS,IAAI,IAAjB,EAAuB;AACrBA,aAAS,GAAG,EAAZ;AACD;;AACD,MAAI4B,MAAM,IAAI,IAAd,EAAoB;AAClBA,UAAM,GAAG,CAAT;AACD;;AACD,MAAIK,OAAO,IAAI,IAAf,EAAqB;AACnBA,WAAO,GAAG,IAAV;AACD;;AACD,MAAIE,YAAY,IAAI,IAApB,EAA0B;AACxBA,gBAAY,GAAG,CAAf;AACD,GAZyB,CAc1B;;;AACA,MAAIG,YAAY,GAAG,KAAnB;;AACA,MAAIP,IAAI,IAAI,IAAR,IAAgBC,MAAM,IAAI,IAA9B,EAAoC;AAClCM,gBAAY,GAAG,IAAf,CADkC,CAElC;AACD;;AACD,MAAID,eAAe,IAAI,IAAvB,EAA6B;AAC3BC,gBAAY,GAAG,IAAf;;AACA,QAAIF,aAAa,IAAI,IAArB,EAA2B;AACzB,YAAM,IAAIxC,UAAJ,CACF,mEACA,oCAFE,CAAN;AAGD;AACF;;AAED,QAAM2C,eAAe,GACjBf,KAAK,CAACgB,eAAN,CAAsBd,GAAtB,EAA2B1B,SAA3B,EAAsCoC,aAAtC,EAAqD,iBAArD,CADJ;AAEA,MAAIK,UAAJ;;AACA,MAAIF,eAAe,IAAI,IAAvB,EAA6B;AAC3BE,cAAU,GAAG3C,KAAK,CAAC,CAAD,EAAIyC,eAAJ,CAAlB;AACD;;AAED,MAAIV,OAAO,IAAI,IAAf,EAAqB;AACnBA,WAAO,GAAG,CAAV;AACD;;AAED,QAAM;AAACa,gBAAD;AAAeC;AAAf,MAA0BlD,kBAAkB,CAC9CqC,SAD8C,EACnCD,OADmC,EAC1BD,MAD0B,EAClBO,YADkB,EACJI,eADI,EACaH,aADb,EAE9CpC,SAF8C,EAEnCsC,YAFmC,EAErBJ,eAFqB,CAAlD;AAGAQ,cAAY,CAACE,QAAb,CAAsBpB,KAAtB;AACAA,OAAK,CAACmB,OAAN,GAAgBA,OAAhB;AACA,QAAMD,YAAY,CAACG,YAAb,EAAN;AACArB,OAAK,CAACsB,aAAN,GAAsB,KAAtB,CA9C0B,CA+C1B;AACA;;AAEA,OAAK,IAAIC,KAAK,GAAGZ,YAAjB,EAA+BY,KAAK,GAAGnB,MAAvC,EAA+C,EAAEmB,KAAjD,EAAwD;AACtD,UAAML,YAAY,CAACM,YAAb,CAA0BD,KAA1B,CAAN;AACA,UAAME,SAAS,GAAmB,EAAlC;;AACA,QAAIb,aAAa,IAAI,IAArB,EAA2B;AACzB,YAAM,IAAIzC,mBAAJ,CACF,4CADE,CAAN;AAED,KAHD,MAGO;AACL,UAAIsC,OAAO,KAAK,OAAhB,EAAyB;AACvB,cAAM,IAAItC,mBAAJ,CAAwB,wCAAxB,CAAN;AACD,OAFD,MAEO,IAAIsC,OAAJ,EAAa;AAClB5C,YAAI,CAAC4C,OAAL,CAAaQ,UAAb;AACD,OALI,CAML;AACA;;;AACA,YAAMS,iBAAiB,GAAG9D,QAAQ,CAACqD,UAAD,CAAlC;AAEA,YAAMU,OAAO,GAAGlC,WAAW,CAACsB,eAAD,EAAkBvC,SAAlB,CAA3B;;AACA,WAAK,IAAIoD,UAAU,GAAG,CAAtB,EAAyBA,UAAU,GAAGD,OAAO,CAACE,MAA9C,EAAsD,EAAED,UAAxD,EAAoE;AAClE,cAAME,SAAS,GAAmB,EAAlC;AACA,cAAMZ,YAAY,CAACa,YAAb,CAA0BH,UAA1B,EAAsCE,SAAtC,CAAN;AAEApE,WAAG,CAAC4B,IAAJ,CAAS,MAAK;AACZ,gBAAMM,UAAU,GAAG+B,OAAO,CAACC,UAAD,CAAP,CAAoB,CAApB,CAAnB;AACA,gBAAM/B,QAAQ,GAAG8B,OAAO,CAACC,UAAD,CAAP,CAAoB,CAApB,CAAjB;AACA,gBAAMI,QAAQ,GAAGhE,mBAAmB,CACf0D,iBADe,EACI9B,UADJ,EAEfC,QAAQ,GAAGD,UAFI,CAApC;AAGAkC,mBAAS,CAAC,OAAD,CAAT,GAAqBF,UAArB;AACAE,mBAAS,CAAC,MAAD,CAAT,GAAoBjC,QAAQ,GAAGD,UAA/B,CAPY,CASZ;AACA;;AACA,gBAAMqC,QAAQ,GAAG7C,oBAAoB,CAACc,GAAD,EAAM8B,QAAN,CAArC;AACA,gBAAME,IAAI,GAAGjC,CAAC,CAACgC,QAAD,CAAd;;AACA,eAAK,IAAIE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGhC,SAAS,CAAC0B,MAA9B,EAAsC,EAAEM,CAAxC,EAA2C;AACzC,kBAAMC,KAAK,GAAGjC,SAAS,CAACgC,CAAD,CAAvB;AACA,kBAAME,GAAG,GAAGH,IAAI,CAACC,CAAD,CAAhB;AACAL,qBAAS,CAACM,KAAD,CAAT,GAAmBC,GAAnB;AACA3E,eAAG,CAAC4E,IAAJ,CAASD,GAAT,EAJyC,CAKzC;AACD;;AAED,cAAIT,UAAU,KAAKD,OAAO,CAACE,MAAR,GAAiB,CAApC,EAAuC;AAAG;AACxC,gBAAIf,YAAJ,EAAkB;AAChB,oBAAMyB,OAAO,GAAGvC,KAAK,CAACwC,QAAN,CAAejC,IAAf,EAAqBC,MAArB,EAA6BhC,SAA7B,CAAhB,CADgB,CAEhB;;AACA,mBAAK,IAAI2D,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGhC,SAAS,CAAC0B,MAA9B,EAAsC,EAAEM,CAAxC,EAA2C;AACzC,sBAAMC,KAAK,GAAGjC,SAAS,CAACgC,CAAD,CAAvB;AACA,sBAAME,GAAG,GAAGE,OAAO,CAACJ,CAAD,CAAnB;AACAzE,mBAAG,CAAC4E,IAAJ,CAASD,GAAT,EAHyC,CAIzC;;AACAZ,yBAAS,CAAC,SAASW,KAAV,CAAT,GAA4BC,GAA5B;AACD;AACF;AACF;AACF,SAlCD;AAoCA,cAAMnB,YAAY,CAACuB,UAAb,CAAwBb,UAAxB,EAAoCE,SAApC,CAAN;AACAzD,4BAAoB,CAACyD,SAAD,CAApB;;AAEA,YAAI9B,KAAK,CAACsB,aAAV,EAAyB;AACvB;AACD,SA7CiE,CA8ClE;;AACD;;AAEDI,uBAAiB,CAACgB,OAAlB;AACD,KAnEqD,CAoEtD;;;AACA,UAAMxB,YAAY,CAACyB,UAAb,CAAwBpB,KAAxB,EAA+BE,SAA/B,CAAN;;AACA,QAAIzB,KAAK,CAACsB,aAAV,EAAyB;AACvB;AACD;AACF;;AACD,QAAMJ,YAAY,CAAC0B,UAAb,EAAN;AAEA,QAAM5C,KAAK,CAACmB,OAAN,CAAc0B,QAAd,EAAN;AACA,SAAO7C,KAAK,CAACmB,OAAb;AACD;;AAED,OAAO,eAAe2B,UAAf,EACH;AACA;AACA9C,KAHG,EAGS+C,CAHT,EAIHC,CAJG,EAKoB;AAAA,MAAvBC,IAAuB,uEAAF,EAAE;;AACzB,MAAIjD,KAAK,CAACkD,UAAV,EAAsB;AACpB,UAAM,IAAIC,KAAJ,CACF,8DADE,CAAN;AAED;;AACDnD,OAAK,CAACkD,UAAN,GAAmB,IAAnB;AACA,MAAIE,MAAJ;AACA,MAAIC,OAAJ;AACA,MAAIC,cAAJ;AACA,MAAIC,eAAJ;AACA,MAAIC,SAAJ;AACA,MAAIC,SAAJ;AACA,MAAIC,IAAJ;AACA,MAAIC,IAAJ;AACA,MAAIC,aAAJ;;AACA,MAAI;AACF,UAAMpF,SAAS,GAAGyE,IAAI,CAACzE,SAAL,IAAkB,IAAlB,GAAyB,EAAzB,GAA8ByE,IAAI,CAACzE,SAArD;AACAD,kBAAc,CAACC,SAAD,CAAd,CAFE,CAIF;AACA;;AACA,UAAMqF,cAAc,GAAG,KAAvB;AACA,UAAMC,gBAAgB,GAClB,MAAM9D,KAAK,CAAC+D,mBAAN,CACFhB,CADE,EACCC,CADD,EACIC,IAAI,CAACe,YADT,EACuBf,IAAI,CAACgB,WAD5B,EACyCJ,cADzC,EAEFrF,SAFE,CADV;AAIA4E,UAAM,GAAGU,gBAAgB,CAAC,CAAD,CAAzB;AACAT,WAAO,GAAGS,gBAAgB,CAAC,CAAD,CAA1B;AACAF,iBAAa,GAAGE,gBAAgB,CAAC,CAAD,CAAhC,CAbE,CAeF;;AACA,QAAIhD,YAAY,GAAG,KAAnB;AACA,QAAIN,MAAJ;;AACA,QAAIyC,IAAI,CAACiB,cAAL,IAAuB,IAAvB,IAA+BjB,IAAI,CAACiB,cAAL,CAAoBrC,MAApB,GAA6B,CAAhE,EAAmE;AACjEf,kBAAY,GAAG,IAAf;;AACA,UAAImC,IAAI,CAACiB,cAAL,CAAoBrC,MAApB,KAA+B,CAAnC,EAAsC;AACpC;AACA2B,iBAAS,GAAGP,IAAI,CAACiB,cAAL,CAAoB,CAApB,CAAZ;AACAT,iBAAS,GAAGR,IAAI,CAACiB,cAAL,CAAoB,CAApB,CAAZ;AACD,OAJD,MAIO,IAAIjB,IAAI,CAACiB,cAAL,CAAoBrC,MAApB,KAA+B,CAAnC,EAAsC;AAC3C,cAAM,IAAI1D,mBAAJ,CACF,+DADE,CAAN;AAED,OAHM,MAGA;AACL,cAAM,IAAIC,UAAJ,CACF,kEACA,4CADA,GAEA,GAAG6E,IAAI,CAACiB,cAAc,cAHpB,CAAN;AAID;;AAED,YAAML,cAAc,GAAG,IAAvB;AACA,YAAMM,eAAe,GACjB,MAAMnE,KAAK,CAAC+D,mBAAN,CACFP,SADE,EACSC,SADT,EACoB,IADpB;AAC0B;AAC5B,UAFE;AAE0B;AAC5BI,oBAHE,EAGcrF,SAHd,CADV;AAKAkF,UAAI,GAAGS,eAAe,CAAC,CAAD,CAAtB;AACAR,UAAI,GAAGQ,eAAe,CAAC,CAAD,CAAtB;AACA3D,YAAM,GAAGkD,IAAI,CAACU,MAAL,CAAYT,IAAZ,CAAT,CAxBiE,CAyBjE;AACD,KA1BD,MA0BO,IACHV,IAAI,CAACoB,eAAL,IAAwB,IAAxB,IAAgCpB,IAAI,CAACoB,eAAL,GAAuB,CAAvD,IACApB,IAAI,CAACoB,eAAL,GAAuB,CAFpB,EAEuB;AAC5BvD,kBAAY,GAAG,IAAf,CAD4B,CAE5B;;AACA,YAAMwD,OAAO,GACTC,IAAI,CAACC,KAAL,CAAWpB,MAAM,CAAC,CAAD,CAAN,CAAUqB,KAAV,CAAgB,CAAhB,KAAsB,IAAIxB,IAAI,CAACoB,eAA/B,CAAX,CADJ;AAEA,YAAMK,iBAAiB,GAAGtB,MAAM,CAAC,CAAD,CAAN,CAAUqB,KAAV,CAAgB,CAAhB,CAA1B;AACAf,UAAI,GAAG9E,WAAW,CAACwE,MAAD,EAASkB,OAAT,EAAkBI,iBAAlB,CAAlB;AACApB,oBAAc,GAAGF,MAAjB;AACAA,YAAM,GAAGxE,WAAW,CAACwE,MAAD,EAAS,CAAT,EAAYkB,OAAZ,CAApB;AACAX,UAAI,GAAG/E,WAAW,CAACyE,OAAD,EAAUiB,OAAV,EAAmBI,iBAAnB,CAAlB;AACAnB,qBAAe,GAAGF,OAAlB;AACAA,aAAO,GAAGzE,WAAW,CAACyE,OAAD,EAAU,CAAV,EAAaiB,OAAb,CAArB,CAX4B,CAY5B;AACA;;AACA9D,YAAM,GAAGkD,IAAI,CAACU,MAAL,CAAYT,IAAZ,CAAT,CAd4B,CAgB5B;AACD,KAnBM,MAmBA,IAAIV,IAAI,CAACpC,eAAL,IAAwB,IAA5B,EAAkC;AACvCC,kBAAY,GAAG,IAAf,CADuC,CAEvC;AACD;;AAED,UAAMZ,GAAG,GAAGkD,MAAM,CAACgB,MAAP,CAAcf,OAAd,EAAuBe,MAAvB,CAA8BR,aAA9B,CAAZ;AAEA5D,SAAK,CAAC2E,gCAAN,GAtEE,CAwEF;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,UAAMC,aAAa,GAAG5E,KAAK,CAAC6E,iBAAN,EAAtB;AACA,UAAM1E,SAAS,GAAGH,KAAK,CAAC8E,sBAAN,EAAlB;AAEA,QAAIC,WAAJ;AACA,QAAIrE,eAAJ;;AACA,QAAII,YAAJ,EAAkB;AAChBd,WAAK,CAACgF,gBAAN;AACAD,iBAAW,GAAG/E,KAAK,CAACiF,YAApB;AACAvE,qBAAe,GACXP,SAAS,CAAC+E,KAAV,GAAkBd,MAAlB,CAAyBjE,SAAS,CAACjB,GAAV,CAAciG,CAAC,IAAI,SAASA,CAA5B,CAAzB,CADJ;AAED,KALD,MAKO;AACLJ,iBAAW,GAAG,IAAd;AACAvE,YAAM,GAAG,EAAT;AACAE,qBAAe,GAAGP,SAAS,CAAC+E,KAAV,EAAlB;AACD;;AAED,UAAM5E,SAAS,GAAGpC,oBAAoB,CAAC+E,IAAI,CAAC3C,SAAN,EAAiB2C,IAAI,CAACmC,UAAtB,CAAtC;AACA,UAAM/C,GAAG,GAAG,MAAMtC,OAAO,CACrBC,KADqB,EACd4E,aADc,EACC1E,GADD,EACMC,SADN,EACiB3B,SADjB,EAC4ByE,IAAI,CAAC7C,MADjC,EAErB6C,IAAI,CAAC5C,OAFgB,EAEPC,SAFO,EAEIyE,WAFJ,EAEiBvE,MAFjB,EAEyByC,IAAI,CAACxC,OAF9B,EAGrBC,eAHqB,EAGJuC,IAAI,CAACtC,YAHD,EAGe,IAHf,EAGqB,IAHrB,CAAzB;AAIA,WAAO0B,GAAP;AACD,GA1GD,SA0GU;AACRrC,SAAK,CAACkD,UAAN,GAAmB,KAAnB,CADQ,CAER;;AACAmC,qBAAiB,CAACjC,MAAD,EAASL,CAAT,CAAjB;AACAsC,qBAAiB,CAAChC,OAAD,EAAUL,CAAV,CAAjB;AACAqC,qBAAiB,CAAC/B,cAAD,EAAiBP,CAAjB,CAAjB;AACAsC,qBAAiB,CAAC9B,eAAD,EAAkBP,CAAlB,CAAjB;AACAqC,qBAAiB,CAAC3B,IAAD,EAAmBF,SAAnB,CAAjB;AACA6B,qBAAiB,CAAC1B,IAAD,EAAmBF,SAAnB,CAAjB;;AACA,QAAIG,aAAa,IAAI,IAArB,EAA2B;AACzBlG,SAAG,CAACgF,OAAJ,CAAYkB,aAAZ;AACD;AACF,GArIwB,CAsIzB;;AACD;AAED;;;;;;;AAMA,OAAM,SAAU0B,0BAAV,CAAqCC,OAArC,EAA6D;AACjE,QAAMrD,IAAI,GAAa,EAAvB;;AACA,MAAIqD,OAAO,YAAY5H,MAAvB,EAA+B;AAC7B4H,WAAO,GAAG,CAACA,OAAD,CAAV;AACD,GAJgE,CAMjE;;;AACA,OAAK,IAAIpD,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGoD,OAAO,CAAC1D,MAA5B,EAAoC,EAAEM,CAAtC,EAAyC;AACvC,UAAMqD,MAAM,GAAGD,OAAO,CAACpD,CAAD,CAAtB;;AACA,QAAIqD,MAAM,CAACC,IAAP,KAAgB,CAApB,EAAuB;AACrBvD,UAAI,CAACpC,IAAL,CAAUhC,UAAU,CAAC0H,MAAD,EAAS,CAAT,CAApB;AACD,KAFD,MAEO,IAAIA,MAAM,CAACC,IAAP,KAAgB,CAApB,EAAuB;AAC5B,YAAM,IAAItC,KAAJ,CACF,iEACA,WAFE,CAAN;AAGD,KAJM,MAIA;AACLjB,UAAI,CAACpC,IAAL,CAAU0F,MAAV;AACD;AACF;;AACD,SAAOtD,IAAP;AACD;AAED;;;;;;;;;;;AAWA;;AACA,OAAM,SAAUmD,iBAAV,CACFE,OADE,EAEFG,UAFE,EAEuD;AAC3D,MAAIH,OAAO,IAAI,IAAf,EAAqB;AACnB;AACD;;AACD,QAAMI,YAAY,GAAa,EAA/B;;AACA,MAAID,UAAU,YAAY/H,MAA1B,EAAkC;AAChCgI,gBAAY,CAAC7F,IAAb,CAAkB4F,UAAU,CAACE,EAA7B;AACD,GAFD,MAEO,IAAI5G,KAAK,CAACC,OAAN,CAAcyG,UAAd,CAAJ,EAA+B;AACpCA,cAAU,CAACG,OAAX,CAAmBC,CAAC,IAAIH,YAAY,CAAC7F,IAAb,CAAkBgG,CAAC,CAACF,EAApB,CAAxB;AACD,GAFM,MAEA,IAAIF,UAAU,IAAI,IAAlB,EAAwB;AAC7B;AACA,SAAK,MAAMK,IAAX,IAAmBL,UAAnB,EAA+B;AAC7B,YAAMM,SAAS,GAAGN,UAAU,CAACK,IAAD,CAA5B;AACAJ,kBAAY,CAAC7F,IAAb,CAAkBkG,SAAS,CAACJ,EAA5B;AACD;AACF;;AAED,QAAMK,gBAAgB,GAAa,EAAnC;;AACA,MAAIV,OAAO,YAAY5H,MAAvB,EAA+B;AAC7B,QAAIgI,YAAY,CAACO,OAAb,CAAqBX,OAAO,CAACK,EAA7B,MAAqC,CAAC,CAA1C,EAA6C;AAC3CK,sBAAgB,CAACnG,IAAjB,CAAsByF,OAAtB;AACD;AACF,GAJD,MAIO,IAAIvG,KAAK,CAACC,OAAN,CAAcsG,OAAd,CAAJ,EAA4B;AACjCA,WAAO,CAACM,OAAR,CAAgBC,CAAC,IAAG;AAClB,UAAIH,YAAY,CAACO,OAAb,CAAqBJ,CAAC,CAACF,EAAvB,MAA+B,CAAC,CAApC,EAAuC;AACrCK,wBAAgB,CAACnG,IAAjB,CAAsBgG,CAAtB;AACD;AACF,KAJD;AAKD,GANM,MAMA,IAAIP,OAAO,IAAI,IAAf,EAAqB;AAC1B;AACA,SAAK,MAAMQ,IAAX,IAAmBR,OAAnB,EAA4B;AAC1B,YAAMC,MAAM,GAAGD,OAAO,CAACQ,IAAD,CAAtB;;AACA,UAAIJ,YAAY,CAACO,OAAb,CAAqBV,MAAM,CAACI,EAA5B,MAAoC,CAAC,CAAzC,EAA4C;AAC1CK,wBAAgB,CAACnG,IAAjB,CAAsB0F,MAAtB;AACD;AACF;AACF;;AAEDS,kBAAgB,CAACJ,OAAjB,CAAyBC,CAAC,IAAG;AAC3B,QAAI,CAACA,CAAC,CAACK,UAAP,EAAmB;AACjBL,OAAC,CAACpD,OAAF;AACD;AACF,GAJD;AAKD","names":["tfc","Tensor","tensor1d","util","expandDims","gather","sliceAlongFirstAxis","configureCallbacks","standardizeCallbacks","NotImplementedError","ValueError","disposeTensorsInLogs","range","checkBatchSize","batchSize","assert","Number","isInteger","sliceArrays","arrays","start","stop","Array","isArray","map","array","sliceArraysByIndices","indices","tidy","dtype","cast","makeBatches","size","output","batchStart","batchEnd","push","fitLoop","model","f","ins","outLabels","epochs","verbose","callbacks","valF","valIns","shuffle","callbackMetrics","initialEpoch","stepsPerEpoch","validationSteps","doValidation","numTrainSamples","checkNumSamples","indexArray","callbackList","history","setModel","onTrainBegin","stopTraining_","epoch","onEpochBegin","epochLogs","epochIndexArray1D","batches","batchIndex","length","batchLogs","onBatchBegin","batchIds","insBatch","outs","i","label","out","keep","valOuts","testLoop","onBatchEnd","dispose","onEpochEnd","onTrainEnd","syncData","fitTensors","x","y","args","isTraining","Error","inputs","targets","originalInputs","originalTargets","inputValX","inputValY","valX","valY","sampleWeights","checkBatchAxis","standardizedOuts","standardizeUserData","sampleWeight","classWeight","validationData","valStandardized","concat","validationSplit","splitAt","Math","floor","shape","originalBatchSize","checkTrainableWeightsConsistency","trainFunction","makeTrainFunction","getDedupedMetricsNames","valFunction","makeTestFunction","testFunction","slice","n","yieldEvery","disposeNewTensors","ensureTensorsRank2OrHigher","tensors","tensor","rank","refTensors","oldTensorIds","id","forEach","t","name","oldTensor","tensorsToDispose","indexOf","isDisposed"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/engine/training_tensors.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using tf.Tensor objects.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Scalar, Tensor, Tensor1D, tensor1d, util} from '@tensorflow/tfjs-core';\n\nimport {expandDims, gather, sliceAlongFirstAxis} from '../backend/tfjs_backend';\nimport {BaseCallback, configureCallbacks, CustomCallbackArgs, History, ModelLoggingVerbosity, standardizeCallbacks, YieldEveryOptions} from '../base_callbacks';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {disposeTensorsInLogs, UnresolvedLogs} from '../logs';\nimport {range} from '../utils/math_utils';\nimport {ClassWeight, ClassWeightMap} from './training_utils';\n\n/**\n * Interface configuration model training based on data as `tf.Tensor`s.\n */\nexport interface ModelFitArgs {\n  /**\n   * Number of samples per gradient update. If unspecified, it\n   * will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Integer number of times to iterate over the training data arrays.\n   */\n  epochs?: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Float between 0 and 1: fraction of the training data\n   * to be used as validation data. The model will set apart this fraction of\n   * the training data, will not train on it, and will evaluate the loss and\n   * any model metrics on this data at the end of each epoch.\n   * The validation data is selected from the last samples in the `x` and `y`\n   * data provided, before shuffling.\n   */\n  validationSplit?: number;\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be a tuple [xVal, yVal] or a tuple [xVal, yVal,\n   * valSampleWeights]. The model will not be trained on this data.\n   * `validationData` will override `validationSplit`.\n   */\n  validationData?: [\n    Tensor|Tensor[], Tensor|Tensor[]\n  ]|[Tensor | Tensor[], Tensor|Tensor[], Tensor|Tensor[]];\n\n  /**\n   * Whether to shuffle the training data before each epoch. Has\n   * no effect when `stepsPerEpoch` is not `null`.\n   */\n  shuffle?: boolean;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or a object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n\n  /**\n   * Optional array of the same length as x, containing\n   * weights to apply to the model's loss for each sample. In the case of\n   * temporal data, you can pass a 2D array with shape (samples,\n   * sequenceLength), to apply a different weight to every timestep of every\n   * sample. In this case you should make sure to specify\n   * sampleWeightMode=\"temporal\" in compile().\n   */\n  sampleWeight?: Tensor;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run). When this is used, `epochs` is the index of the \"final epoch\".\n   * The model is not trained for a number of iterations given by `epochs`,\n   * but merely until the epoch of index `epochs` is reached.\n   */\n  initialEpoch?: number;\n\n  /**\n   * Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. When training\n   * with Input Tensors such as TensorFlow data tensors, the default `null` is\n   * equal to the number of unique samples in your dataset divided by the\n   * batch size, or 1 if that cannot be determined.\n   */\n  stepsPerEpoch?: number;\n\n  /**\n   * Only relevant if `stepsPerEpoch` is specified. Total number of steps\n   * (batches of samples) to validate before stopping.\n   */\n  validationSteps?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - any `number`: yield every `number` milliseconds.\n   *   - `'never'`: never yield. (yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n}\n\nexport function checkBatchSize(batchSize: number) {\n  tfc.util.assert(\n      batchSize > 0 && Number.isInteger(batchSize),\n      () => `batchSize is required to be a positive integer, but got ${\n          batchSize}`);\n}\n\n/**\n * Slice a Tensor or an Array of Tensors, by start and stop indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArraysByIndices()` together.\n *\n * @param arrays: the input.\n * @param start: the starting index (inclusive).\n * @param stop: the stopping index (exclusive).\n * @returns The result of the slicing. If `arrays` is an `Array` of\n *   `tf.Tensor`s, the slicing will be applied to all elements of the `Array`\n *   in the same way.\n */\nexport function sliceArrays(\n    arrays: Tensor|Tensor[], start: number, stop: number): Tensor|Tensor[] {\n  if (arrays == null) {\n    return [null];\n  } else if (Array.isArray(arrays)) {\n    return arrays.map(array => sliceAlongFirstAxis(array, start, stop - start));\n  } else {  // Tensor.\n    return sliceAlongFirstAxis(arrays, start, stop - start);\n  }\n}\n\n/**\n * Slice a Tensor or an Array of Tensors, by random-order indices.\n *\n * Porting Note: The `_slice_arrays` function in PyKeras is covered by this\n *   function and `sliceArrays()` together.\n *\n * @param arrays The input `tf.Tensor` or `Array` of `tf.Tensor`s to slice.\n *   If an `Array` of `tf.Tensor`s, all `tf.Tensor`s will be sliced in the\n *   same fashion.\n * @param indices The indices to use for slicing along the first (batch)\n *   dimension.\n * @returns Result(s) of the slicing.\n */\nexport function sliceArraysByIndices(\n    arrays: Tensor|Tensor[], indices: Tensor1D): Tensor|Tensor[] {\n  return tfc.tidy(() => {\n    if (arrays == null) {\n      return null;\n    } else if (Array.isArray(arrays)) {\n      return arrays.map(\n          array => (sliceArraysByIndices(array, indices) as Tensor));\n    } else {\n      // TODO(cais): indices should be a pre-constructed Tensor1D to avoid\n      //   tensor1d() calls.\n      return gather(\n          arrays,\n          indices.dtype === 'int32' ? indices : tfc.cast(indices, 'int32'));\n    }\n  });\n}\n\n/**\n * Returns a list of batch indices (tuples of indices).\n * @param size: Integer, total size of the data to slice into batches.\n * @param batchSize: Integer, batch size.\n * @returns An Array of [batchStart, batchEnd] tuples. batchStart is\n *   inclusive; batchEnd is exclusive. I.e., each batch consists of indices x\n *   that satisfy batchStart <= x < batchEnd.\n */\nexport function makeBatches(\n    size: number, batchSize: number): Array<[number, number]> {\n  const output: Array<[number, number]> = [];\n  let batchStart = 0;\n  let batchEnd: number = null;\n  while (batchStart < size) {\n    batchEnd = batchStart + batchSize;\n    if (batchEnd >= size) {\n      batchEnd = size;\n    }\n    output.push([batchStart, batchEnd]);\n    batchStart = batchEnd;\n  }\n  return output;\n}\n\n/**\n * Abstract fit function for `f(ins)`.\n * @param f A Function returning a list of tensors. For training, this\n *   function is expected to perform the updates to the variables.\n * @param ins List of tensors to be fed to `f`.\n * @param outLabels List of strings, display names of the outputs of `f`.\n * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n * @param epochs Number of times to iterate over the data. Default : 1.\n * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n * @param callbacks List of callbacks to be called during training.\n * @param valF Function to call for validation.\n * @param valIns List of tensors to be fed to `valF`.\n * @param shuffle Whether to shuffle the data at the beginning of every\n * epoch. Default : true.\n * @param callbackMetrics List of strings, the display names of the metrics\n *   passed to the callbacks. They should be the concatenation of the\n *   display names of the outputs of `f` and the list of display names\n *   of the outputs of `valF`.\n * @param initialEpoch Epoch at which to start training (useful for\n *   resuming a previous training run). Default : 0.\n * @param stepsPerEpoch Total number of steps (batches on samples) before\n *   declaring one epoch finished and starting the next epoch. Ignored with\n *   the default value of `undefined` or `null`.\n * @param validationSteps Number of steps to run validation for (only if\n *   doing validation from data tensors). Not applicable for tfjs-layers.\n * @returns A `History` object.\n */\nasync function fitLoop(\n    // Type `model` as `any` here to avoid circular dependency w/ training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, f: (data: Tensor[]) => Scalar[], ins: Tensor[],\n    outLabels?: string[], batchSize?: number, epochs?: number, verbose?: number,\n    callbacks?: BaseCallback[], valF?: (data: Tensor[]) => Scalar[],\n    valIns?: Tensor[], shuffle?: boolean|string, callbackMetrics?: string[],\n    initialEpoch?: number, stepsPerEpoch?: number,\n    validationSteps?: number): Promise<History> {\n  if (batchSize == null) {\n    batchSize = 32;\n  }\n  if (epochs == null) {\n    epochs = 1;\n  }\n  if (shuffle == null) {\n    shuffle = true;\n  }\n  if (initialEpoch == null) {\n    initialEpoch = 0;\n  }\n\n  // TODO(cais): Change const to let below when implementing validation.\n  let doValidation = false;\n  if (valF != null && valIns != null) {\n    doValidation = true;\n    // TODO(cais): verbose message.\n  }\n  if (validationSteps != null) {\n    doValidation = true;\n    if (stepsPerEpoch == null) {\n      throw new ValueError(\n          'Can only use `validationSteps` when doing step-wise training, ' +\n          'i.e., `stepsPerEpoch` must be set.');\n    }\n  }\n\n  const numTrainSamples =\n      model.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n  let indexArray: number[];\n  if (numTrainSamples != null) {\n    indexArray = range(0, numTrainSamples);\n  }\n\n  if (verbose == null) {\n    verbose = 1;\n  }\n\n  const {callbackList, history} = configureCallbacks(\n      callbacks, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch,\n      batchSize, doValidation, callbackMetrics);\n  callbackList.setModel(model);\n  model.history = history;\n  await callbackList.onTrainBegin();\n  model.stopTraining_ = false;\n  // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n  // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n\n  for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n    await callbackList.onEpochBegin(epoch);\n    const epochLogs: UnresolvedLogs = {};\n    if (stepsPerEpoch != null) {\n      throw new NotImplementedError(\n          'stepsPerEpoch mode is not implemented yet.');\n    } else {\n      if (shuffle === 'batch') {\n        throw new NotImplementedError('batch shuffling is not implemneted yet');\n      } else if (shuffle) {\n        util.shuffle(indexArray);\n      }\n      // Convert the potentially shuffled indices to Tensor1D, to avoid the\n      // cost of repeated creation of Array1Ds later on.\n      const epochIndexArray1D = tensor1d(indexArray);\n\n      const batches = makeBatches(numTrainSamples, batchSize);\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchLogs: UnresolvedLogs = {};\n        await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n        tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds = sliceAlongFirstAxis(\n                               epochIndexArray1D, batchStart,\n                               batchEnd - batchStart) as Tensor1D;\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = batchEnd - batchStart;\n\n          // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n          const insBatch = sliceArraysByIndices(ins, batchIds) as Tensor[];\n          const outs = f(insBatch);\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n            // TODO(cais): Use scope() to avoid ownership.\n          }\n\n          if (batchIndex === batches.length - 1) {  // Last batch.\n            if (doValidation) {\n              const valOuts = model.testLoop(valF, valIns, batchSize);\n              // Porting Notes: In tfjs-layers, valOuts is always an Array.\n              for (let i = 0; i < outLabels.length; ++i) {\n                const label = outLabels[i];\n                const out = valOuts[i];\n                tfc.keep(out);\n                // TODO(cais): Use scope() to avoid ownership.\n                epochLogs['val_' + label] = out;\n              }\n            }\n          }\n        });\n\n        await callbackList.onBatchEnd(batchIndex, batchLogs);\n        disposeTensorsInLogs(batchLogs);\n\n        if (model.stopTraining_) {\n          break;\n        }\n        // TODO(cais): return outs as list of Tensor.\n      }\n\n      epochIndexArray1D.dispose();\n    }\n    // TODO(cais): Run validation at the end of the epoch.\n    await callbackList.onEpochEnd(epoch, epochLogs);\n    if (model.stopTraining_) {\n      break;\n    }\n  }\n  await callbackList.onTrainEnd();\n\n  await model.history.syncData();\n  return model.history;\n}\n\nexport async function fitTensors(\n    // Type `model` as `any` here to avoid circular dependency w/ training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n    y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n    args: ModelFitArgs = {}): Promise<History> {\n  if (model.isTraining) {\n    throw new Error(\n        'Cannot start training because another fit() call is ongoing.');\n  }\n  model.isTraining = true;\n  let inputs: Tensor[];\n  let targets: Tensor[];\n  let originalInputs: Tensor[];\n  let originalTargets: Tensor[];\n  let inputValX: Tensor|Tensor[];\n  let inputValY: Tensor|Tensor[];\n  let valX: Tensor|Tensor[];\n  let valY: Tensor|Tensor[];\n  let sampleWeights: Tensor[];\n  try {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize);\n\n    // Validate user data.\n    // TODO(cais): Support sampleWeight.\n    const checkBatchAxis = false;\n    const standardizedOuts =\n        await model.standardizeUserData(\n            x, y, args.sampleWeight, args.classWeight, checkBatchAxis,\n            batchSize) as [Tensor[], Tensor[], Tensor[]];\n    inputs = standardizedOuts[0];\n    targets = standardizedOuts[1];\n    sampleWeights = standardizedOuts[2];\n\n    // Prepare validation data.\n    let doValidation = false;\n    let valIns: Tensor[];\n    if (args.validationData != null && args.validationData.length > 0) {\n      doValidation = true;\n      if (args.validationData.length === 2) {\n        // config.validationData consists of valX and valY.\n        inputValX = args.validationData[0];\n        inputValY = args.validationData[1];\n      } else if (args.validationData.length === 3) {\n        throw new NotImplementedError(\n            'validationData including sample weights is not supported yet.');\n      } else {\n        throw new ValueError(\n            `When passing validation data, it must contain 2 (valX, valY) ` +\n            `or 3 (valX, valY, valSampleWeight) items; ` +\n            `${args.validationData} is invalid.`);\n      }\n\n      const checkBatchAxis = true;\n      const valStandardized =\n          await model.standardizeUserData(\n              inputValX, inputValY, null, /** Unused sample weights. */\n              null,                       /** Unused class weights. */\n              checkBatchAxis, batchSize) as [Tensor[], Tensor[], Tensor[]];\n      valX = valStandardized[0];\n      valY = valStandardized[1];\n      valIns = valX.concat(valY);\n      // TODO(cais): Add useLearningPhase data properly.\n    } else if (\n        args.validationSplit != null && args.validationSplit > 0 &&\n        args.validationSplit < 1) {\n      doValidation = true;\n      // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n      const splitAt =\n          Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n      const originalBatchSize = inputs[0].shape[0];\n      valX = sliceArrays(inputs, splitAt, originalBatchSize) as Tensor[];\n      originalInputs = inputs;\n      inputs = sliceArrays(inputs, 0, splitAt) as Tensor[];\n      valY = sliceArrays(targets, splitAt, originalBatchSize) as Tensor[];\n      originalTargets = targets;\n      targets = sliceArrays(targets, 0, splitAt) as Tensor[];\n      // TODO(cais): Once sampleWeights becomes available, slice it to get\n      //   valSampleWeights.\n      valIns = valX.concat(valY);\n\n      // TODO(cais): Add useLearningPhase data properly.\n    } else if (args.validationSteps != null) {\n      doValidation = true;\n      // TODO(cais): Add useLearningPhase.\n    }\n\n    const ins = inputs.concat(targets).concat(sampleWeights);\n\n    model.checkTrainableWeightsConsistency();\n\n    // TODO(cais): Handle use_learning_phase and learning_phase?\n\n    // Porting Note: Here we see a key deviation of tfjs-layers from\n    // Keras.\n    //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n    //  we do not construct symbolic computation graphs to embody the\n    //  training process. Instead, we define a function that performs the\n    //  training action. In PyKeras, the data (inputs and targets) are fed\n    //  through graph placeholders. In tfjs-layers, the data are fed as\n    //  function arguments. Since the function are defined below in the\n    //  scope, we don't have equivalents of PyKeras's\n    //  `_make_train_funciton`.\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames() as string[];\n\n    let valFunction: (data: Tensor[]) => Scalar[];\n    let callbackMetrics: string[];\n    if (doValidation) {\n      model.makeTestFunction();\n      valFunction = model.testFunction;\n      callbackMetrics =\n          outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      valFunction = null;\n      valIns = [];\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const out = await fitLoop(\n        model, trainFunction, ins, outLabels, batchSize, args.epochs,\n        args.verbose, callbacks, valFunction, valIns, args.shuffle,\n        callbackMetrics, args.initialEpoch, null, null);\n    return out;\n  } finally {\n    model.isTraining = false;\n    // Memory clean up.\n    disposeNewTensors(inputs, x);\n    disposeNewTensors(targets, y);\n    disposeNewTensors(originalInputs, x);\n    disposeNewTensors(originalTargets, y);\n    disposeNewTensors(valX as Tensor[], inputValX);\n    disposeNewTensors(valY as Tensor[], inputValY);\n    if (sampleWeights != null) {\n      tfc.dispose(sampleWeights);\n    }\n  }\n  // TODO(cais): Add value to outLabels.\n}\n\n/**\n * Ensure tensors all have a rank of at least 2.\n *\n * If a tensor has a rank of 1, it is dimension-expanded to rank 2.\n * If any tensor has a rank of 0 (i.e., is a scalar), an error will be thrown.\n */\nexport function ensureTensorsRank2OrHigher(tensors: Tensor|Tensor[]): Tensor[] {\n  const outs: Tensor[] = [];\n  if (tensors instanceof Tensor) {\n    tensors = [tensors];\n  }\n\n  // Make Tensors at least 2D.\n  for (let i = 0; i < tensors.length; ++i) {\n    const tensor = tensors[i];\n    if (tensor.rank === 1) {\n      outs.push(expandDims(tensor, 1));\n    } else if (tensor.rank === 0) {\n      throw new Error(\n          'Expected tensor to be at least 1D, but received a 0D tensor ' +\n          '(scalar).');\n    } else {\n      outs.push(tensor);\n    }\n  }\n  return outs;\n}\n\n/**\n * Compare a set of tensors with a reference (old) set, discard the ones\n * in the new set that are not present in the reference set.\n *\n * This method is used for memory clenaup during calls such as\n * LayersModel.fit().\n *\n * @param tensors New set which may contain Tensors not present in\n *   `refTensors`.\n * @param refTensors Reference Tensor set.\n */\n// TODO(cais, kangyizhang): Deduplicate with tfjs-data.\nexport function disposeNewTensors(\n    tensors: Tensor|Tensor[]|{[inputName: string]: Tensor},\n    refTensors: Tensor|Tensor[]|{[inputName: string]: Tensor}): void {\n  if (tensors == null) {\n    return;\n  }\n  const oldTensorIds: number[] = [];\n  if (refTensors instanceof Tensor) {\n    oldTensorIds.push(refTensors.id);\n  } else if (Array.isArray(refTensors)) {\n    refTensors.forEach(t => oldTensorIds.push(t.id));\n  } else if (refTensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in refTensors) {\n      const oldTensor = refTensors[name];\n      oldTensorIds.push(oldTensor.id);\n    }\n  }\n\n  const tensorsToDispose: Tensor[] = [];\n  if (tensors instanceof Tensor) {\n    if (oldTensorIds.indexOf(tensors.id) === -1) {\n      tensorsToDispose.push(tensors);\n    }\n  } else if (Array.isArray(tensors)) {\n    tensors.forEach(t => {\n      if (oldTensorIds.indexOf(t.id) === -1) {\n        tensorsToDispose.push(t);\n      }\n    });\n  } else if (tensors != null) {\n    // `oldTensors` is a map from string name to Tensor.\n    for (const name in tensors) {\n      const tensor = tensors[name];\n      if (oldTensorIds.indexOf(tensor.id) === -1) {\n        tensorsToDispose.push(tensor);\n      }\n    }\n  }\n\n  tensorsToDispose.forEach(t => {\n    if (!t.isDisposed) {\n      t.dispose();\n    }\n  });\n}\n"]},"metadata":{},"sourceType":"module"}