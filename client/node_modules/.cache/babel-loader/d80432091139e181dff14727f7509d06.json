{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { broadcast_util, upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose'; // Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\n\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl(_ref) {\n  let {\n    a,\n    b,\n    transposeA,\n    transposeB,\n    backend,\n    bias = null,\n    preluActivationWeights = null,\n    leakyreluAlpha = 0,\n    activation = null\n  } = _ref;\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${a.shape} and ` + `${b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB]; // The rest of the implementation is designed to operate on rank-3 tensors\n\n  const a3d = reshape({\n    inputs: {\n      x: a\n    },\n    backend,\n    attrs: {\n      shape: a3dShape\n    }\n  });\n  const b3d = reshape({\n    inputs: {\n      x: b\n    },\n    backend,\n    attrs: {\n      shape: b3dShape\n    }\n  });\n  const intermediates = [a3d, b3d];\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation != null ? mapActivationToShaderProgram(activation, true) : null;\n  const containsFusedOps = hasBias || hasPreluActivationWeights || hasLeakyreluAlpha || fusedActivation != null;\n  let out; // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n\n  if ((outerShapeA === 1 || outerShapeB === 1) && sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    let aVec = a3d;\n    let bVec = b3d;\n\n    if (transposeA) {\n      aVec = transpose({\n        inputs: {\n          x: a3d\n        },\n        backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(aVec);\n    }\n\n    if (transposeB) {\n      bVec = transpose({\n        inputs: {\n          x: b3d\n        },\n        backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(bVec);\n    }\n\n    const shouldReshapeA = outerShapeB !== 1;\n    const shouldReshapeB = outerShapeB === 1;\n    let aVec3d = aVec;\n\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {\n          x: aVec\n        },\n        backend,\n        attrs: {\n          shape: [batchDim, sharedDim, 1]\n        }\n      });\n      intermediates.push(aVec3d);\n    }\n\n    const axis = outerShapeB === 1 ? 2 : 1;\n    let bVec3d = bVec;\n\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {\n          x: bVec\n        },\n        backend,\n        attrs: {\n          shape: [batchDim, 1, sharedDim]\n        }\n      });\n      intermediates.push(bVec3d);\n    }\n\n    const product = multiply({\n      inputs: {\n        a: aVec3d,\n        b: bVec3d\n      },\n      backend\n    });\n    out = sum({\n      inputs: {\n        x: product\n      },\n      backend,\n      attrs: {\n        axis,\n        keepDims: true\n      }\n    });\n    intermediates.push(product);\n  } else {\n    const dtype = upcastType(a.dtype, b.dtype);\n    const program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    const inputs = [a3d, b3d];\n\n    if (bias != null) {\n      inputs.push(bias);\n    }\n\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n\n    if (hasLeakyreluAlpha) {\n      const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  const outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(out);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return outReshaped;\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsBA,cAAtB,EAAkDC,UAAlD,EAA8DC,IAA9D,QAAyE,uBAAzE;AAGA,SAAQC,4BAAR,QAA2C,oCAA3C;AACA,SAAQC,mBAAR,QAAkC,sBAAlC;AAEA,SAAQC,QAAR,QAAuB,YAAvB;AACA,SAAQC,OAAR,QAAsB,WAAtB;AACA,SAAQC,GAAR,QAAkB,OAAlB;AACA,SAAQC,SAAR,QAAwB,aAAxB,C,CAEA;AACA;AACA;;AACA,OAAO,MAAMC,2BAA2B,GAAG,IAApC;AAcP,OAAM,SAAUC,eAAV,OAUc;AAAA,MAVY;AAC9BC,KAD8B;AAE9BC,KAF8B;AAG9BC,cAH8B;AAI9BC,cAJ8B;AAK9BC,WAL8B;AAM9BC,QAAI,GAAG,IANuB;AAO9BC,0BAAsB,GAAG,IAPK;AAQ9BC,kBAAc,GAAG,CARa;AAS9BC,cAAU,GAAG;AATiB,GAUZ;AAClB,QAAMC,KAAK,GAAGT,CAAC,CAACU,KAAF,CAAQC,MAAtB;AACA,QAAMC,KAAK,GAAGX,CAAC,CAACS,KAAF,CAAQC,MAAtB;AAEA,QAAME,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAH,GAAwBT,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAtD;AACA,QAAMK,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAH,GAAwBX,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAtD;AAEA,QAAMG,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAH,GAAwBT,CAAC,CAACU,KAAF,CAAQD,KAAK,GAAG,CAAhB,CAAtD;AACA,QAAMO,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAH,GAAwBX,CAAC,CAACS,KAAF,CAAQE,KAAK,GAAG,CAAhB,CAAtD;AAEA,QAAMK,UAAU,GAAGjB,CAAC,CAACU,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;AACA,QAAMC,UAAU,GAAGlB,CAAC,CAACS,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CAAnB;AAEA,QAAME,SAAS,GAAG7B,IAAI,CAAC8B,aAAL,CAAmBJ,UAAnB,CAAlB;AACA,QAAMK,SAAS,GAAG/B,IAAI,CAAC8B,aAAL,CAAmBF,UAAnB,CAAlB;AAEA,QAAMI,iBAAiB,GAAGlC,cAAc,CAACmC,0BAAf,CACtBxB,CAAC,CAACU,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CADsB,EACAjB,CAAC,CAACS,KAAF,CAAQQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAlB,CADA,CAA1B;AAEA,QAAMO,QAAQ,GAAGF,iBAAiB,CAACG,MAAlB,CAAyB,CAACX,WAAD,EAAcC,WAAd,CAAzB,CAAjB;AAEAzB,MAAI,CAACoC,MAAL,CACId,WAAW,KAAKC,WADpB,EAEI,MAAM,kCAAkCD,WAAW,SAA7C,GACF,GAAGC,WAAW,4BAA4Bd,CAAC,CAACU,KAAK,OAD/C,GAEF,GAAGT,CAAC,CAACS,KAAK,mBAAmBR,UAAU,EAFrC,GAGF,mBAAmBC,UAAU,cALrC;AAOA,QAAMyB,QAAQ,GAA6B1B,UAAU,GACjD,CAACkB,SAAD,EAAYP,WAAZ,EAAyBE,WAAzB,CADiD,GAEjD,CAACK,SAAD,EAAYL,WAAZ,EAAyBF,WAAzB,CAFJ;AAGA,QAAMgB,QAAQ,GAA6B1B,UAAU,GACjD,CAACmB,SAAD,EAAYN,WAAZ,EAAyBF,WAAzB,CADiD,GAEjD,CAACQ,SAAD,EAAYR,WAAZ,EAAyBE,WAAzB,CAFJ,CA9BkB,CAkClB;;AACA,QAAMc,GAAG,GAAGnC,OAAO,CAAC;AAACoC,UAAM,EAAE;AAACC,OAAC,EAAEhC;AAAJ,KAAT;AAAiBI,WAAjB;AAA0B6B,SAAK,EAAE;AAACvB,WAAK,EAAEkB;AAAR;AAAjC,GAAD,CAAnB;AACA,QAAMM,GAAG,GAAGvC,OAAO,CAAC;AAACoC,UAAM,EAAE;AAACC,OAAC,EAAE/B;AAAJ,KAAT;AAAiBG,WAAjB;AAA0B6B,SAAK,EAAE;AAACvB,WAAK,EAAEmB;AAAR;AAAjC,GAAD,CAAnB;AAEA,QAAMM,aAAa,GAAiB,CAACL,GAAD,EAAMI,GAAN,CAApC;AAEA,QAAME,QAAQ,GAAGC,IAAI,CAACC,GAAL,CAASlB,SAAT,EAAoBE,SAApB,CAAjB;AACA,QAAMiB,SAAS,GAAGrC,UAAU,GAAG4B,GAAG,CAACpB,KAAJ,CAAU,CAAV,CAAH,GAAkBoB,GAAG,CAACpB,KAAJ,CAAU,CAAV,CAA9C;AAEA,QAAM8B,OAAO,GAAGnC,IAAI,IAAI,IAAxB;AACA,QAAMoC,yBAAyB,GAAGnC,sBAAsB,IAAI,IAA5D;AACA,QAAMoC,iBAAiB,GAAGlC,UAAU,KAAK,WAAzC;AACA,QAAMmC,eAAe,GAAGnC,UAAU,IAAI,IAAd,GACpBhB,4BAA4B,CAACgB,UAAD,EAAa,IAAb,CADR,GAEpB,IAFJ;AAGA,QAAMoC,gBAAgB,GAAGJ,OAAO,IAAIC,yBAAX,IACrBC,iBADqB,IACAC,eAAe,IAAI,IAD5C;AAEA,MAAIE,GAAJ,CAnDkB,CAqDlB;AACA;;AACA,MAAI,CAAC9B,WAAW,KAAK,CAAhB,IAAqBC,WAAW,KAAK,CAAtC,KACAuB,SAAS,GAAGzC,2BADZ,IAC2C8C,gBAAgB,KAAK,KADpE,EAC2E;AACzE,QAAIE,IAAI,GAAGhB,GAAX;AACA,QAAIiB,IAAI,GAAGb,GAAX;;AACA,QAAIhC,UAAJ,EAAgB;AACd4C,UAAI,GAAGjD,SAAS,CAAC;AAACkC,cAAM,EAAE;AAACC,WAAC,EAAEF;AAAJ,SAAT;AAAmB1B,eAAnB;AAA4B6B,aAAK,EAAE;AAACe,cAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;AAAP;AAAnC,OAAD,CAAhB;AACAb,mBAAa,CAACc,IAAd,CAAmBH,IAAnB;AACD;;AACD,QAAI3C,UAAJ,EAAgB;AACd4C,UAAI,GAAGlD,SAAS,CAAC;AAACkC,cAAM,EAAE;AAACC,WAAC,EAAEE;AAAJ,SAAT;AAAmB9B,eAAnB;AAA4B6B,aAAK,EAAE;AAACe,cAAI,EAAE,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP;AAAP;AAAnC,OAAD,CAAhB;AACAb,mBAAa,CAACc,IAAd,CAAmBF,IAAnB;AACD;;AAED,UAAMG,cAAc,GAAGlC,WAAW,KAAK,CAAvC;AACA,UAAMmC,cAAc,GAAGnC,WAAW,KAAK,CAAvC;AAEA,QAAIoC,MAAM,GAAGN,IAAb;;AACA,QAAII,cAAJ,EAAoB;AAClBE,YAAM,GAAGzD,OAAO,CAAC;AACfoC,cAAM,EAAE;AAACC,WAAC,EAAEc;AAAJ,SADO;AAEf1C,eAFe;AAGf6B,aAAK,EAAE;AAACvB,eAAK,EAAE,CAAC0B,QAAD,EAAWG,SAAX,EAAsB,CAAtB;AAAR;AAHQ,OAAD,CAAhB;AAMAJ,mBAAa,CAACc,IAAd,CAAmBG,MAAnB;AACD;;AAED,UAAMC,IAAI,GAAGrC,WAAW,KAAK,CAAhB,GAAoB,CAApB,GAAwB,CAArC;AAEA,QAAIsC,MAAM,GAAGP,IAAb;;AACA,QAAII,cAAJ,EAAoB;AAClBG,YAAM,GAAG3D,OAAO,CAAC;AACfoC,cAAM,EAAE;AAACC,WAAC,EAAEe;AAAJ,SADO;AAEf3C,eAFe;AAGf6B,aAAK,EAAE;AAACvB,eAAK,EAAE,CAAC0B,QAAD,EAAW,CAAX,EAAcG,SAAd;AAAR;AAHQ,OAAD,CAAhB;AAMAJ,mBAAa,CAACc,IAAd,CAAmBK,MAAnB;AACD;;AAED,UAAMC,OAAO,GAAG7D,QAAQ,CAAC;AAACqC,YAAM,EAAE;AAAC/B,SAAC,EAAEoD,MAAJ;AAAYnD,SAAC,EAAEqD;AAAf,OAAT;AAAiClD;AAAjC,KAAD,CAAxB;AACAyC,OAAG,GAAGjD,GAAG,CAAC;AAACmC,YAAM,EAAE;AAACC,SAAC,EAAEuB;AAAJ,OAAT;AAAuBnD,aAAvB;AAAgC6B,WAAK,EAAE;AAACoB,YAAD;AAAOG,gBAAQ,EAAE;AAAjB;AAAvC,KAAD,CAAT;AACArB,iBAAa,CAACc,IAAd,CAAmBM,OAAnB;AACD,GA3CD,MA2CO;AACL,UAAME,KAAK,GAAGnE,UAAU,CAACU,CAAC,CAACyD,KAAH,EAAUxD,CAAC,CAACwD,KAAZ,CAAxB;AAEA,UAAMC,OAAO,GAAG,IAAIjE,mBAAJ,CACZmC,QADY,EACFC,QADE,EACQ,CAACO,QAAD,EAAWrB,WAAX,EAAwBC,WAAxB,CADR,EAC8Cd,UAD9C,EAEZC,UAFY,EAEAqC,OAFA,EAESG,eAFT,EAE0BF,yBAF1B,EAGZC,iBAHY,CAAhB;AAKA,UAAMX,MAAM,GAAiB,CAACD,GAAD,EAAMI,GAAN,CAA7B;;AACA,QAAI7B,IAAI,IAAI,IAAZ,EAAkB;AAChB0B,YAAM,CAACkB,IAAP,CAAY5C,IAAZ;AACD;;AACD,QAAIoC,yBAAJ,EAA+B;AAC7BV,YAAM,CAACkB,IAAP,CAAY3C,sBAAZ;AACD;;AACD,QAAIoC,iBAAJ,EAAuB;AACrB,YAAMiB,eAAe,GAAGvD,OAAO,CAACwD,cAAR,CACpB,EADoB,EAChB,SADgB,EAEpBrE,IAAI,CAACsE,iBAAL,CAAuBtD,cAAvB,EAA0D,SAA1D,CAFoB,CAAxB;AAGAwB,YAAM,CAACkB,IAAP,CAAYU,eAAZ;AACAxB,mBAAa,CAACc,IAAd,CAAmBU,eAAnB;AACD;;AAEDd,OAAG,GAAGzC,OAAO,CAAC0D,eAAR,CAAwBJ,OAAxB,EAAiC3B,MAAjC,EAAyC0B,KAAzC,CAAN;AACD;;AAED,QAAMM,WAAW,GACbpE,OAAO,CAAC;AAACoC,UAAM,EAAE;AAACC,OAAC,EAAEa;AAAJ,KAAT;AAAmBzC,WAAnB;AAA4B6B,SAAK,EAAE;AAACvB,WAAK,EAAEe;AAAR;AAAnC,GAAD,CADX;AAEAU,eAAa,CAACc,IAAd,CAAmBJ,GAAnB;;AACA,OAAK,MAAMmB,CAAX,IAAgB7B,aAAhB,EAA+B;AAC7B/B,WAAO,CAAC6D,6BAAR,CAAsCD,CAAtC;AACD;;AACD,SAAOD,WAAP;AACD","names":["broadcast_util","upcastType","util","mapActivationToShaderProgram","MatMulPackedProgram","multiply","reshape","sum","transpose","MATMUL_SHARED_DIM_THRESHOLD","batchMatMulImpl","a","b","transposeA","transposeB","backend","bias","preluActivationWeights","leakyreluAlpha","activation","aRank","shape","length","bRank","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","outShapeOuterDims","assertAndGetBroadcastShape","outShape","concat","assert","a3dShape","b3dShape","a3d","inputs","x","attrs","b3d","intermediates","batchDim","Math","max","sharedDim","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","containsFusedOps","out","aVec","bVec","perm","push","shouldReshapeA","shouldReshapeB","aVec3d","axis","bVec3d","product","keepDims","dtype","program","$leakyreluAlpha","makeTensorInfo","createScalarValue","runWebGLProgram","outReshaped","i","disposeIntermediateTensorInfo"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-backend-webgl/src/kernels/BatchMatMul_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, broadcast_util, TensorInfo, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\n\nimport {multiply} from './Multiply';\nimport {reshape} from './Reshape';\nimport {sum} from './Sum';\nimport {transpose} from './Transpose';\n\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\n\ntype BatchMatMulConfig = {\n  a: TensorInfo,\n  b: TensorInfo,\n  transposeA: boolean,\n  transposeB: boolean,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: BatchMatMulConfig): TensorInfo {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape: [number, number, number] = transposeA ?\n      [batchDimA, innerShapeA, outerShapeA] :\n      [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape: [number, number, number] = transposeB ?\n      [batchDimB, outerShapeB, innerShapeB] :\n      [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n\n  const intermediates: TensorInfo[] = [a3d, b3d];\n\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation != null ?\n      mapActivationToShaderProgram(activation, true) :\n      null;\n  const containsFusedOps = hasBias || hasPreluActivationWeights ||\n      hasLeakyreluAlpha || fusedActivation != null;\n  let out: TensorInfo;\n\n  // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n  if ((outerShapeA === 1 || outerShapeB === 1) &&\n      sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    let aVec = a3d;\n    let bVec = b3d;\n    if (transposeA) {\n      aVec = transpose({inputs: {x: a3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(aVec);\n    }\n    if (transposeB) {\n      bVec = transpose({inputs: {x: b3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(bVec);\n    }\n\n    const shouldReshapeA = outerShapeB !== 1;\n    const shouldReshapeB = outerShapeB === 1;\n\n    let aVec3d = aVec;\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {x: aVec},\n        backend,\n        attrs: {shape: [batchDim, sharedDim, 1]}\n      });\n\n      intermediates.push(aVec3d);\n    }\n\n    const axis = outerShapeB === 1 ? 2 : 1;\n\n    let bVec3d = bVec;\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {x: bVec},\n        backend,\n        attrs: {shape: [batchDim, 1, sharedDim]}\n      });\n\n      intermediates.push(bVec3d);\n    }\n\n    const product = multiply({inputs: {a: aVec3d, b: bVec3d}, backend});\n    out = sum({inputs: {x: product}, backend, attrs: {axis, keepDims: true}});\n    intermediates.push(product);\n  } else {\n    const dtype = upcastType(a.dtype, b.dtype);\n\n    const program = new MatMulPackedProgram(\n        a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA,\n        transposeB, hasBias, fusedActivation, hasPreluActivationWeights,\n        hasLeakyreluAlpha);\n\n    const inputs: TensorInfo[] = [a3d, b3d];\n    if (bias != null) {\n      inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n      const $leakyreluAlpha = backend.makeTensorInfo(\n          [], 'float32',\n          util.createScalarValue(leakyreluAlpha as {} as 'float32', 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  const outReshaped =\n      reshape({inputs: {x: out}, backend, attrs: {shape: outShape}});\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return outReshaped;\n}\n"]},"metadata":{},"sourceType":"module"}