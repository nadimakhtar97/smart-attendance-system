{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FusedBatchNorm, util } from '@tensorflow/tfjs-core';\nimport { assertNotComplex } from '../cpu_util';\nexport function batchNorm(args) {\n  const {\n    inputs,\n    backend,\n    attrs\n  } = args;\n  const {\n    x,\n    scale,\n    offset,\n    mean,\n    variance\n  } = inputs;\n  util.assert(mean.shape.length === variance.shape.length, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert(offset == null || mean.shape.length === offset.shape.length, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert(scale == null || mean.shape.length === scale.shape.length, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n  let {\n    varianceEpsilon\n  } = attrs;\n\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const xVals = backend.data.get(x.dataId).values;\n  const mVals = backend.data.get(mean.dataId).values;\n  const varVals = backend.data.get(variance.dataId).values;\n  const sVals = scale ? backend.data.get(scale.dataId).values : new Float32Array([1]);\n  const offVals = offset ? backend.data.get(offset.dataId).values : new Float32Array([0]);\n  const outVals = new Float32Array(xVals.length);\n  const offValsLength = offVals.length;\n  const sValsLength = sVals.length;\n  const varValsLength = varVals.length;\n  const mValsLength = mVals.length;\n  let offi = 0;\n  let mi = 0;\n  let si = 0;\n  let vi = 0;\n\n  for (let i = 0; i < xVals.length; ++i) {\n    outVals[i] = offVals[offi++] + (xVals[i] - mVals[mi++]) * sVals[si++] / Math.sqrt(varVals[vi++] + varianceEpsilon);\n\n    if (offi >= offValsLength) {\n      offi = 0;\n    }\n\n    if (mi >= mValsLength) {\n      mi = 0;\n    }\n\n    if (si >= sValsLength) {\n      si = 0;\n    }\n\n    if (vi >= varValsLength) {\n      vi = 0;\n    }\n  }\n\n  return backend.makeTensorInfo(x.shape, x.dtype, outVals);\n}\nexport const batchNormConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'cpu',\n  kernelFunc: batchNorm\n};","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,cAAR,EAAqHC,IAArH,QAAgI,uBAAhI;AAGA,SAAQC,gBAAR,QAA+B,aAA/B;AAEA,OAAM,SAAUC,SAAV,CAAoBC,IAApB,EAIL;AACC,QAAM;AAACC,UAAD;AAASC,WAAT;AAAkBC;AAAlB,MAA2BH,IAAjC;AACA,QAAM;AAACI,KAAD;AAAIC,SAAJ;AAAWC,UAAX;AAAmBC,QAAnB;AAAyBC;AAAzB,MAAqCP,MAA3C;AAEAJ,MAAI,CAACY,MAAL,CACIF,IAAI,CAACG,KAAL,CAAWC,MAAX,KAAsBH,QAAQ,CAACE,KAAT,CAAeC,MADzC,EAEI,MAAM,qEACF,cAHR;AAIAd,MAAI,CAACY,MAAL,CACIH,MAAM,IAAI,IAAV,IAAkBC,IAAI,CAACG,KAAL,CAAWC,MAAX,KAAsBL,MAAM,CAACI,KAAP,CAAaC,MADzD,EAEI,MAAM,mEACF,cAHR;AAIAd,MAAI,CAACY,MAAL,CACIJ,KAAK,IAAI,IAAT,IAAiBE,IAAI,CAACG,KAAL,CAAWC,MAAX,KAAsBN,KAAK,CAACK,KAAN,CAAYC,MADvD,EAEI,MAAM,kEACF,cAHR;AAKAb,kBAAgB,CAAC,CAACM,CAAD,EAAIG,IAAJ,EAAUC,QAAV,EAAoBH,KAApB,EAA2BC,MAA3B,CAAD,EAAqC,WAArC,CAAhB;AAEA,MAAI;AAACM;AAAD,MAAoBT,KAAxB;;AACA,MAAIS,eAAe,IAAI,IAAvB,EAA6B;AAC3BA,mBAAe,GAAG,KAAlB;AACD;;AAED,QAAMC,KAAK,GAAGX,OAAO,CAACY,IAAR,CAAaC,GAAb,CAAiBX,CAAC,CAACY,MAAnB,EAA2BC,MAAzC;AACA,QAAMC,KAAK,GAAGhB,OAAO,CAACY,IAAR,CAAaC,GAAb,CAAiBR,IAAI,CAACS,MAAtB,EAA8BC,MAA5C;AACA,QAAME,OAAO,GAAGjB,OAAO,CAACY,IAAR,CAAaC,GAAb,CAAiBP,QAAQ,CAACQ,MAA1B,EAAkCC,MAAlD;AACA,QAAMG,KAAK,GAAGf,KAAK,GAAGH,OAAO,CAACY,IAAR,CAAaC,GAAb,CAAiBV,KAAK,CAACW,MAAvB,EAA+BC,MAAlC,GACG,IAAII,YAAJ,CAAiB,CAAC,CAAD,CAAjB,CADtB;AAEA,QAAMC,OAAO,GAAGhB,MAAM,GAClBJ,OAAO,CAACY,IAAR,CAAaC,GAAb,CAAiBT,MAAM,CAACU,MAAxB,EAAgCC,MADd,GAElB,IAAII,YAAJ,CAAiB,CAAC,CAAD,CAAjB,CAFJ;AAGA,QAAME,OAAO,GAAG,IAAIF,YAAJ,CAAiBR,KAAK,CAACF,MAAvB,CAAhB;AAEA,QAAMa,aAAa,GAAGF,OAAO,CAACX,MAA9B;AACA,QAAMc,WAAW,GAAGL,KAAK,CAACT,MAA1B;AACA,QAAMe,aAAa,GAAGP,OAAO,CAACR,MAA9B;AACA,QAAMgB,WAAW,GAAGT,KAAK,CAACP,MAA1B;AAEA,MAAIiB,IAAI,GAAG,CAAX;AACA,MAAIC,EAAE,GAAG,CAAT;AACA,MAAIC,EAAE,GAAG,CAAT;AACA,MAAIC,EAAE,GAAG,CAAT;;AACA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGnB,KAAK,CAACF,MAA1B,EAAkC,EAAEqB,CAApC,EAAuC;AACrCT,WAAO,CAACS,CAAD,CAAP,GAAaV,OAAO,CAACM,IAAI,EAAL,CAAP,GACT,CAACf,KAAK,CAACmB,CAAD,CAAL,GAAWd,KAAK,CAACW,EAAE,EAAH,CAAjB,IAA2BT,KAAK,CAACU,EAAE,EAAH,CAAhC,GACIG,IAAI,CAACC,IAAL,CAAUf,OAAO,CAACY,EAAE,EAAH,CAAP,GAAgBnB,eAA1B,CAFR;;AAGA,QAAIgB,IAAI,IAAIJ,aAAZ,EAA2B;AACzBI,UAAI,GAAG,CAAP;AACD;;AACD,QAAIC,EAAE,IAAIF,WAAV,EAAuB;AACrBE,QAAE,GAAG,CAAL;AACD;;AACD,QAAIC,EAAE,IAAIL,WAAV,EAAuB;AACrBK,QAAE,GAAG,CAAL;AACD;;AACD,QAAIC,EAAE,IAAIL,aAAV,EAAyB;AACvBK,QAAE,GAAG,CAAL;AACD;AACF;;AACD,SAAO7B,OAAO,CAACiC,cAAR,CAAuB/B,CAAC,CAACM,KAAzB,EAAgCN,CAAC,CAACgC,KAAlC,EAAyCb,OAAzC,CAAP;AACD;AAED,OAAO,MAAMc,eAAe,GAAiB;AAC3CC,YAAU,EAAE1C,cAD+B;AAE3C2C,aAAW,EAAE,KAF8B;AAG3CC,YAAU,EAAEzC;AAH+B,CAAtC","names":["FusedBatchNorm","util","assertNotComplex","batchNorm","args","inputs","backend","attrs","x","scale","offset","mean","variance","assert","shape","length","varianceEpsilon","xVals","data","get","dataId","values","mVals","varVals","sVals","Float32Array","offVals","outVals","offValsLength","sValsLength","varValsLength","mValsLength","offi","mi","si","vi","i","Math","sqrt","makeTensorInfo","dtype","batchNormConfig","kernelName","backendName","kernelFunc"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-backend-cpu/src/kernels/BatchNorm.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function batchNorm(args: {\n  inputs: FusedBatchNormInputs,\n  backend: MathBackendCPU,\n  attrs: FusedBatchNormAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, scale, offset, mean, variance} = inputs;\n\n  util.assert(\n      mean.shape.length === variance.shape.length,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      offset == null || mean.shape.length === offset.shape.length,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      scale == null || mean.shape.length === scale.shape.length,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n\n  let {varianceEpsilon} = attrs;\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const mVals = backend.data.get(mean.dataId).values as TypedArray;\n  const varVals = backend.data.get(variance.dataId).values as TypedArray;\n  const sVals = scale ? backend.data.get(scale.dataId).values as TypedArray :\n                        new Float32Array([1]);\n  const offVals = offset ?\n      backend.data.get(offset.dataId).values as TypedArray :\n      new Float32Array([0]);\n  const outVals = new Float32Array(xVals.length);\n\n  const offValsLength = offVals.length;\n  const sValsLength = sVals.length;\n  const varValsLength = varVals.length;\n  const mValsLength = mVals.length;\n\n  let offi = 0;\n  let mi = 0;\n  let si = 0;\n  let vi = 0;\n  for (let i = 0; i < xVals.length; ++i) {\n    outVals[i] = offVals[offi++] +\n        (xVals[i] - mVals[mi++]) * sVals[si++] /\n            Math.sqrt(varVals[vi++] + varianceEpsilon);\n    if (offi >= offValsLength) {\n      offi = 0;\n    }\n    if (mi >= mValsLength) {\n      mi = 0;\n    }\n    if (si >= sValsLength) {\n      si = 0;\n    }\n    if (vi >= varValsLength) {\n      vi = 0;\n    }\n  }\n  return backend.makeTensorInfo(x.shape, x.dtype, outVals);\n}\n\nexport const batchNormConfig: KernelConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'cpu',\n  kernelFunc: batchNorm as {} as KernelFunc,\n};\n"]},"metadata":{},"sourceType":"module"}