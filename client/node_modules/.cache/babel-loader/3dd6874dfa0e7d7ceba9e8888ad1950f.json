{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class AdadeltaOptimizer extends Optimizer {\n  constructor(learningRate, rho) {\n    let epsilon = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    super();\n    this.learningRate = learningRate;\n    this.rho = rho;\n    this.epsilon = epsilon;\n    this.accumulatedGrads = [];\n    this.accumulatedUpdates = [];\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n      tidy(() => {\n        const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));\n        const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);\n        const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedUpdates = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n\n}\n/** @nocollapse */\n\nAdadeltaOptimizer.className = 'Adadelta'; // Name matters for Python compatibility.\n\nregisterClass(AdadeltaOptimizer);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,OAAR,EAAiBC,IAAjB,QAA4B,YAA5B;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,IAAR,QAAmB,YAAnB;AACA,SAAQC,MAAR,QAAqB,eAArB;AACA,SAAQC,SAAR,QAAwB,mBAAxB;AACA,SAAoBC,aAApB,QAA+E,kBAA/E;AAGA,SAAQC,SAAR,QAA2C,aAA3C;AAEA;;AACA,OAAM,MAAOC,iBAAP,SAAiCD,SAAjC,CAA0C;AAM9CE,cACcC,YADd,EAC8CC,GAD9C,EAEoC;AAAA,QAAtBC,OAAsB,uEAAJ,IAAI;AAClC;AAFY;AAAgC;AAChC;AALN,4BAAwC,EAAxC;AACA,8BAA0C,EAA1C;;AAON,QAAIA,OAAO,IAAI,IAAf,EAAqB;AACnB,WAAKA,OAAL,GAAef,MAAM,CAACgB,OAAP,CAAeD,OAAf,EAAf;AACD;AACF;;AAEDE,gBAAc,CAACC,iBAAD,EAAkD;AAC9D,UAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAIAC,iBAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAY;AAChC,YAAMC,KAAK,GAAG7B,MAAM,CAAC8B,mBAAP,CAA2BN,IAA3B,CAAd;AACA,YAAMO,SAAS,GAAG,KAAlB;;AACA,UAAI,KAAKC,gBAAL,CAAsBJ,CAAtB,KAA4B,IAAhC,EAAsC;AACpC,aAAKI,gBAAL,CAAsBJ,CAAtB,IAA2B;AACzBK,sBAAY,EAAE,GAAGT,IAAI,aADI;AAEzBU,kBAAQ,EAAEhC,IAAI,CAAC,MAAMM,SAAS,CAACqB,KAAD,CAAT,CAAiBK,QAAjB,CAA0BH,SAA1B,CAAP;AAFW,SAA3B;AAID;;AACD,UAAI,KAAKI,kBAAL,CAAwBP,CAAxB,KAA8B,IAAlC,EAAwC;AACtC,aAAKO,kBAAL,CAAwBP,CAAxB,IAA6B;AAC3BK,sBAAY,EAAE,GAAGT,IAAI,YADM;AAE3BU,kBAAQ,EAAEhC,IAAI,CAAC,MAAMM,SAAS,CAACqB,KAAD,CAAT,CAAiBK,QAAjB,CAA0BH,SAA1B,CAAP;AAFa,SAA7B;AAID;;AAED,YAAMK,QAAQ,GAAGhB,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBS,MADR,GAEbnB,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIY,QAAQ,IAAI,IAAhB,EAAsB;AACpB;AACD;;AAED,YAAME,eAAe,GAAG,KAAKN,gBAAL,CAAsBJ,CAAtB,EAAyBM,QAAjD;AACA,YAAMK,iBAAiB,GAAG,KAAKJ,kBAAL,CAAwBP,CAAxB,EAA2BM,QAArD;AAEAhC,UAAI,CAAC,MAAK;AACR,cAAMsC,kBAAkB,GACpBrC,GAAG,CAACE,GAAG,CAACiC,eAAD,EAAkB,KAAKxB,GAAvB,CAAJ,EACCT,GAAG,CAACE,MAAM,CAAC6B,QAAD,CAAP,EAAmB,IAAI,KAAKtB,GAA5B,CADJ,CADP;AAIA,cAAM2B,OAAO,GACTpC,GAAG,CAACD,GAAG,CAACE,IAAI,CAACH,GAAG,CAACoC,iBAAD,EAAoB,KAAKxB,OAAzB,CAAJ,CAAL,EACCT,IAAI,CAACH,GAAG,CAACmC,eAAD,EAAkB,KAAKvB,OAAvB,CAAJ,CADL,CAAJ,EAECqB,QAFD,CADP;AAKA,cAAMM,oBAAoB,GACtBvC,GAAG,CAACE,GAAG,CAACkC,iBAAD,EAAoB,KAAKzB,GAAzB,CAAJ,EACCT,GAAG,CAACE,MAAM,CAACkC,OAAD,CAAP,EAAkB,IAAI,KAAK3B,GAA3B,CADJ,CADP;AAIAwB,uBAAe,CAACK,MAAhB,CAAuBH,kBAAvB;AACAD,yBAAiB,CAACI,MAAlB,CAAyBD,oBAAzB;AAEA,cAAME,QAAQ,GAAGzC,GAAG,CAACE,GAAG,CAACoC,OAAD,EAAU,CAAC,KAAK5B,YAAhB,CAAJ,EAAmCgB,KAAnC,CAApB;AACAA,aAAK,CAACc,MAAN,CAAaC,QAAb;AACD,OAnBG,CAAJ;AAoBD,KA9CD;AA+CA,SAAKC,mBAAL;AACD;;AAED5C,SAAO;AACL,QAAI,KAAKkC,kBAAL,IAA2B,IAA/B,EAAqC;AACnClC,aAAO,CAAC,KAAK+B,gBAAL,CAAsBV,GAAtB,CAA0BwB,CAAC,IAAIA,CAAC,CAACZ,QAAjC,CAAD,CAAP;AACAjC,aAAO,CAAC,KAAKkC,kBAAL,CAAwBb,GAAxB,CAA4BwB,CAAC,IAAIA,CAAC,CAACZ,QAAnC,CAAD,CAAP;AACD;AACF;;AAEe,QAAVa,UAAU;AACd;AACA,UAAMC,SAAS,GACX,CAAC,GAAG,KAAKhB,gBAAT,EAA2B,GAAG,KAAKG,kBAAnC,CADJ;AAEA,WAAO,CAAC,MAAM,KAAKc,cAAL,EAAP,EAA8BC,MAA9B,CACHF,SAAS,CAAC1B,GAAV,CAAcwB,CAAC,KAAK;AAACtB,UAAI,EAAEsB,CAAC,CAACb,YAAT;AAAuBI,YAAM,EAAES,CAAC,CAACZ;AAAjC,KAAL,CAAf,CADG,CAAP;AAED;;AAEe,QAAViB,UAAU,CAACC,YAAD,EAA4B;AAC1CA,gBAAY,GAAG,MAAM,KAAKC,iBAAL,CAAuBD,YAAvB,CAArB;AACA,UAAME,aAAa,GAAGF,YAAY,CAACG,MAAb,GAAsB,CAA5C;AACA,UAAMxB,SAAS,GAAG,KAAlB;AACA,SAAKC,gBAAL,GACIoB,YAAY,CAACI,KAAb,CAAmB,CAAnB,EAAsBF,aAAtB,EAAqChC,GAArC,CAAyCwB,CAAC,KAAK;AACJb,kBAAY,EAAEa,CAAC,CAACtB,IADZ;AAEJU,cAAQ,EAAEY,CAAC,CAACT,MAAF,CAASH,QAAT,CACNH,SADM;AAFN,KAAL,CAA1C,CADJ;AAMA,SAAKI,kBAAL,GACIiB,YAAY,CAACI,KAAb,CAAmBF,aAAnB,EAAkCA,aAAa,GAAG,CAAlD,EACKhC,GADL,CACSwB,CAAC,KAAK;AACJb,kBAAY,EAAEa,CAAC,CAACtB,IADZ;AAEJU,cAAQ,EAAEY,CAAC,CAACT,MAAF,CAASH,QAAT,CAAkBH,SAAlB;AAFN,KAAL,CADV,CADJ;AAMD;;AAED0B,WAAS;AACP,WAAO;AACL,sBAAgB,KAAK5C,YADhB;AAEL,aAAO,KAAKC,GAFP;AAGL,iBAAW,KAAKC;AAHX,KAAP;AAKD;AAED;;;AACiB,SAAV2C,UAAU,CACbC,GADa,EACoBC,MADpB,EACsC;AACrD,WAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,KAAD,CAAtC,EAA+CA,MAAM,CAAC,SAAD,CAArD,CAAP;AACD;;AApH6C;AAC9C;;AACOjD,8BAAY,UAAZ,C,CAAyB;;AAoHlCF,aAAa,CAACE,iBAAD,CAAb","names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","zerosLike","registerClass","Optimizer","AdadeltaOptimizer","constructor","learningRate","rho","epsilon","backend","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","accumulatedGrads","originalName","variable","accumulatedUpdates","gradient","tensor","accumulatedGrad","accumulatedUpdate","newAccumulatedGrad","updates","newAccumulatedUpdate","assign","newValue","incrementIterations","v","getWeights","variables","saveIterations","concat","setWeights","weightValues","extractIterations","variableCount","length","slice","getConfig","fromConfig","cls","config"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/optimizers/adadelta_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/ops';\nimport {square} from '../ops/square';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'Adadelta';  // Name matters for Python compatibility.\n  private accumulatedGrads: OptimizerVariable[] = [];\n  private accumulatedUpdates: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected rho: number,\n      protected epsilon: number = null) {\n    super();\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad =\n            add(mul(accumulatedGrad, this.rho),\n                mul(square(gradient), 1 - this.rho));\n\n        const updates =\n            mul(div(sqrt(add(accumulatedUpdate, this.epsilon)),\n                    sqrt(add(accumulatedGrad, this.epsilon))),\n                gradient);\n\n        const newAccumulatedUpdate =\n            add(mul(accumulatedUpdate, this.rho),\n                mul(square(updates), 1 - this.rho));\n\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedUpdates =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n}\nregisterClass(AdadeltaOptimizer);\n"]},"metadata":{},"sourceType":"module"}