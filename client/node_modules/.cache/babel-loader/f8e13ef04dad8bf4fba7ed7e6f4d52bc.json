{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Add } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const addGradConfig = {\n  kernelName: Add,\n  inputsToSave: ['a', 'b'],\n  gradFunc: (dy, saved) => {\n    const [a, b] = saved;\n    const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const derA = () => {\n      let res = dy;\n      const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n      }\n\n      return reshape(res, a.shape);\n    };\n\n    const derB = () => {\n      let res = dy;\n      const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n      }\n\n      return reshape(res, b.shape);\n    };\n\n    return {\n      a: derA,\n      b: derB\n    };\n  }\n};","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAgBA,SAAQA,GAAR,QAAkB,iBAAlB;AAEA,OAAO,KAAKC,cAAZ,MAAgC,uBAAhC;AACA,SAAQC,OAAR,QAAsB,gBAAtB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AAGA,OAAO,MAAMC,aAAa,GAAe;AACvCC,YAAU,EAAEL,GAD2B;AAEvCM,cAAY,EAAE,CAAC,GAAD,EAAM,GAAN,CAFyB;AAGvCC,UAAQ,EAAE,CAACC,EAAD,EAAaC,KAAb,KAAgC;AACxC,UAAM,CAACC,CAAD,EAAIC,CAAJ,IAASF,KAAf;AACA,UAAMG,QAAQ,GACVX,cAAc,CAACY,0BAAf,CAA0CH,CAAC,CAACI,KAA5C,EAAmDH,CAAC,CAACG,KAArD,CADJ;;AAGA,UAAMC,IAAI,GAAG,MAAK;AAChB,UAAIC,GAAG,GAAGR,EAAV;AACA,YAAMS,UAAU,GAAGhB,cAAc,CAACiB,gBAAf,CAAgCR,CAAC,CAACI,KAAlC,EAAyCF,QAAzC,CAAnB;;AACA,UAAIK,UAAU,CAACE,MAAX,GAAoB,CAAxB,EAA2B;AACzBH,WAAG,GAAGb,GAAG,CAACa,GAAD,EAAMC,UAAN,CAAT;AACD;;AACD,aAAOf,OAAO,CAACc,GAAD,EAAMN,CAAC,CAACI,KAAR,CAAd;AACD,KAPD;;AAQA,UAAMM,IAAI,GAAG,MAAK;AAChB,UAAIJ,GAAG,GAAGR,EAAV;AACA,YAAMS,UAAU,GAAGhB,cAAc,CAACiB,gBAAf,CAAgCP,CAAC,CAACG,KAAlC,EAAyCF,QAAzC,CAAnB;;AACA,UAAIK,UAAU,CAACE,MAAX,GAAoB,CAAxB,EAA2B;AACzBH,WAAG,GAAGb,GAAG,CAACa,GAAD,EAAMC,UAAN,CAAT;AACD;;AACD,aAAOf,OAAO,CAACc,GAAD,EAAML,CAAC,CAACG,KAAR,CAAd;AACD,KAPD;;AASA,WAAO;AAACJ,OAAC,EAAEK,IAAJ;AAAUJ,OAAC,EAAES;AAAb,KAAP;AACD;AA1BsC,CAAlC","names":["Add","broadcast_util","reshape","sum","addGradConfig","kernelName","inputsToSave","gradFunc","dy","saved","a","b","outShape","assertAndGetBroadcastShape","shape","derA","res","reduceAxes","getReductionAxes","length","derB"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/gradients/Add_grad.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {Add} from '../kernel_names';\nimport {GradConfig} from '../kernel_registry';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport {reshape} from '../ops/reshape';\nimport {sum} from '../ops/sum';\nimport {Tensor} from '../tensor';\n\nexport const addGradConfig: GradConfig = {\n  kernelName: Add,\n  inputsToSave: ['a', 'b'],\n  gradFunc: (dy: Tensor, saved: Tensor[]) => {\n    const [a, b] = saved;\n    const outShape =\n        broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n\n    const derA = () => {\n      let res = dy;\n      const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n      if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n      }\n      return reshape(res, a.shape);\n    };\n    const derB = () => {\n      let res = dy;\n      const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n      if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n      }\n      return reshape(res, b.shape);\n    };\n\n    return {a: derA, b: derB};\n  }\n};\n"]},"metadata":{},"sourceType":"module"}