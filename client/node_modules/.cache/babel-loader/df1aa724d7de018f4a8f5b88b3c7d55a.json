{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class RMSPropOptimizer extends Optimizer {\n  constructor(learningRate) {\n    let decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n    let momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n    let epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    let centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n    super();\n    this.learningRate = learningRate;\n    this.decay = decay;\n    this.momentum = momentum;\n    this.epsilon = epsilon;\n    this.accumulatedMeanSquares = [];\n    this.accumulatedMoments = [];\n    this.accumulatedMeanGrads = [];\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable; // Centered gradient\n\n          const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));\n          const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));\n          const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights() {\n    // Order matters for Python compatibility.\n    const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n\n    return [await this.saveIterations()].concat(variables.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares = weightValues.slice(0, variableCount).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n    this.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n\n    if (this.centered) {\n      this.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map(v => ({\n        originalName: v.name,\n        variable: v.tensor.variable(trainable)\n      }));\n    }\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);\n  }\n\n}\n/** @nocollapse */\n\nRMSPropOptimizer.className = 'RMSProp'; // Note: Name matters for Python compatibility.\n\nregisterClass(RMSPropOptimizer);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,OAAR,EAAiBC,IAAjB,QAA4B,YAA5B;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,IAAR,QAAmB,aAAnB;AACA,SAAQC,MAAR,QAAqB,eAArB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,SAAR,QAAwB,mBAAxB;AACA,SAAoBC,aAApB,QAA+E,kBAA/E;AAGA,SAAQC,SAAR,QAA2C,aAA3C;AAEA;;AACA,OAAM,MAAOC,gBAAP,SAAgCD,SAAhC,CAAyC;AAS7CE,cACcC,YADd,EAGoB;AAAA,QAF0BC,KAE1B,uEAFkC,GAElC;AAAA,QADNC,QACM,uEADK,GACL;AAAA,QADoBC,OACpB,uEADsC,IACtC;AAAA,QAAhBC,QAAgB,uEAAL,KAAK;AAClB;AAHY;AAAgC;AAChC;AAA0B;AANhC,kCAA8C,EAA9C;AACA,8BAA0C,EAA1C;AACA,gCAA4C,EAA5C;AAQN,SAAKA,QAAL,GAAgBA,QAAhB;;AAEA,QAAID,OAAO,IAAI,IAAf,EAAqB;AACnB,WAAKA,OAAL,GAAejB,MAAM,CAACmB,OAAP,CAAeF,OAAf,EAAf;AACD;;AACD,QAAIH,YAAY,IAAI,IAApB,EAA0B;AACxB,YAAM,IAAIM,KAAJ,CAAU,oDAAV,CAAN;AACD;AACF;;AAEDC,gBAAc,CAACC,iBAAD,EAAgD;AAC5D,UAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAIAC,iBAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAY;AAChC,YAAMC,KAAK,GAAGjC,MAAM,CAACkC,mBAAP,CAA2BN,IAA3B,CAAd;AACA,YAAMO,SAAS,GAAG,KAAlB;;AACA,UAAI,KAAKC,sBAAL,CAA4BJ,CAA5B,KAAkC,IAAtC,EAA4C;AAC1C,aAAKI,sBAAL,CAA4BJ,CAA5B,IAAiC;AAC/BK,sBAAY,EAAE,GAAGT,IAAI,MADU;AAE/BU,kBAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAD,CAAT,CAAiBK,QAAjB,CAA0BH,SAA1B,CAAP;AAFiB,SAAjC;AAID;;AACD,UAAI,KAAKI,kBAAL,CAAwBP,CAAxB,KAA8B,IAAlC,EAAwC;AACtC,aAAKO,kBAAL,CAAwBP,CAAxB,IAA6B;AAC3BK,sBAAY,EAAE,GAAGT,IAAI,WADM;AAE3BU,kBAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAD,CAAT,CAAiBK,QAAjB,CAA0BH,SAA1B,CAAP;AAFa,SAA7B;AAID;;AACD,UAAI,KAAKK,oBAAL,CAA0BR,CAA1B,KAAgC,IAAhC,IAAwC,KAAKd,QAAjD,EAA2D;AACzD,aAAKsB,oBAAL,CAA0BR,CAA1B,IAA+B;AAC7BK,sBAAY,EAAE,GAAGT,IAAI,KADQ;AAE7BU,kBAAQ,EAAEpC,IAAI,CAAC,MAAMO,SAAS,CAACwB,KAAD,CAAT,CAAiBK,QAAjB,CAA0BH,SAA1B,CAAP;AAFe,SAA/B;AAID;;AAED,YAAMM,QAAQ,GAAGjB,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBU,MADR,GAEbpB,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIa,QAAQ,IAAI,IAAhB,EAAsB;AACpB;AACD;;AAED,YAAME,qBAAqB,GAAG,KAAKP,sBAAL,CAA4BJ,CAA5B,EAA+BM,QAA7D;AACA,YAAMC,kBAAkB,GAAG,KAAKA,kBAAL,CAAwBP,CAAxB,EAA2BM,QAAtD;AACApC,UAAI,CAAC,MAAK;AACR,cAAM0C,wBAAwB,GAC1BzC,GAAG,CAACE,GAAG,CAACsC,qBAAD,EAAwB,KAAK5B,KAA7B,CAAJ,EACCV,GAAG,CAACE,MAAM,CAACkC,QAAD,CAAP,EAAmB,IAAI,KAAK1B,KAA5B,CADJ,CADP;;AAIA,YAAI,KAAKG,QAAT,EAAmB;AACjB,gBAAM2B,mBAAmB,GAAG,KAAKL,oBAAL,CAA0BR,CAA1B,EAA6BM,QAAzD,CADiB,CAEjB;;AACA,gBAAMQ,sBAAsB,GACxB3C,GAAG,CAACE,GAAG,CAACwC,mBAAD,EAAsB,KAAK9B,KAA3B,CAAJ,EACCV,GAAG,CAACoC,QAAD,EAAW,IAAI,KAAK1B,KAApB,CADJ,CADP;AAIA,gBAAMgC,gBAAgB,GAClB3C,GAAG,CAACC,GAAG,CAACoC,QAAD,EAAW,KAAK3B,YAAhB,CAAJ,EACCR,IAAI,CACAE,GAAG,CAACoC,wBAAD,EACCzC,GAAG,CAACI,MAAM,CAACuC,sBAAD,CAAP,EAAiC,KAAK7B,OAAtC,CADJ,CADH,CADL,CADP;AAKA,gBAAM+B,qBAAqB,GACvB7C,GAAG,CAACE,GAAG,CAACkC,kBAAD,EAAqB,KAAKvB,QAA1B,CAAJ,EAAyC+B,gBAAzC,CADP;AAGAJ,+BAAqB,CAACM,MAAtB,CAA6BL,wBAA7B;AACAC,6BAAmB,CAACI,MAApB,CAA2BH,sBAA3B;AACAP,4BAAkB,CAACU,MAAnB,CAA0BD,qBAA1B;AAEA,gBAAME,QAAQ,GAAG1C,GAAG,CAACyB,KAAD,EAAQe,qBAAR,CAApB;AACAf,eAAK,CAACgB,MAAN,CAAaC,QAAb;AACD,SArBD,MAqBO;AACL;AACA,gBAAMN,wBAAwB,GAC1BzC,GAAG,CAACE,GAAG,CAACsC,qBAAD,EAAwB,KAAK5B,KAA7B,CAAJ,EACCV,GAAG,CAACE,MAAM,CAACkC,QAAD,CAAP,EAAmB,IAAI,KAAK1B,KAA5B,CADJ,CADP;AAIA,gBAAMiC,qBAAqB,GACvB7C,GAAG,CAACE,GAAG,CAACkC,kBAAD,EAAqB,KAAKvB,QAA1B,CAAJ,EACCZ,GAAG,CAACC,GAAG,CAACoC,QAAD,EAAW,KAAK3B,YAAhB,CAAJ,EACCR,IAAI,CAACH,GAAG,CAACyC,wBAAD,EAA2B,KAAK3B,OAAhC,CAAJ,CADL,CADJ,CADP;AAKA0B,+BAAqB,CAACM,MAAtB,CAA6BL,wBAA7B;AACAL,4BAAkB,CAACU,MAAnB,CAA0BD,qBAA1B;AAEA,gBAAME,QAAQ,GAAG1C,GAAG,CAACyB,KAAD,EAAQe,qBAAR,CAApB;AACAf,eAAK,CAACgB,MAAN,CAAaC,QAAb;AACD;AACF,OA3CG,CAAJ;AA4CD,KA3ED;AA4EA,SAAKC,mBAAL;AACD;;AAEDlD,SAAO;AACL,QAAI,KAAKmC,sBAAL,IAA+B,IAAnC,EAAyC;AACvCnC,aAAO,CAAC,KAAKmC,sBAAL,CAA4BV,GAA5B,CAAgC0B,CAAC,IAAIA,CAAC,CAACd,QAAvC,CAAD,CAAP;AACD;;AACD,QAAI,KAAKE,oBAAL,IAA6B,IAA7B,IAAqC,KAAKtB,QAA9C,EAAwD;AACtDjB,aAAO,CAAC,KAAKuC,oBAAL,CAA0Bd,GAA1B,CAA8B0B,CAAC,IAAIA,CAAC,CAACd,QAArC,CAAD,CAAP;AACD;;AACD,QAAI,KAAKC,kBAAL,IAA2B,IAA/B,EAAqC;AACnCtC,aAAO,CAAC,KAAKsC,kBAAL,CAAwBb,GAAxB,CAA4B0B,CAAC,IAAIA,CAAC,CAACd,QAAnC,CAAD,CAAP;AACD;AACF;;AAEe,QAAVe,UAAU;AACd;AACA,UAAMC,SAAS,GACX,CAAC,GAAG,KAAKlB,sBAAT,EAAiC,GAAG,KAAKG,kBAAzC,CADJ;;AAEA,QAAI,KAAKrB,QAAT,EAAmB;AACjBoC,eAAS,CAACC,IAAV,CAAe,GAAG,KAAKf,oBAAvB;AACD;;AACD,WAAO,CAAC,MAAM,KAAKgB,cAAL,EAAP,EAA8BC,MAA9B,CACHH,SAAS,CAAC5B,GAAV,CAAc0B,CAAC,KAAK;AAACxB,UAAI,EAAEwB,CAAC,CAACf,YAAT;AAAuBK,YAAM,EAAEU,CAAC,CAACd;AAAjC,KAAL,CAAf,CADG,CAAP;AAED;;AAEe,QAAVoB,UAAU,CAACC,YAAD,EAA4B;AAC1CA,gBAAY,GAAG,MAAM,KAAKC,iBAAL,CAAuBD,YAAvB,CAArB;AACA,UAAME,aAAa,GACf,KAAK3C,QAAL,GAAgByC,YAAY,CAACG,MAAb,GAAsB,CAAtC,GAA0CH,YAAY,CAACG,MAAb,GAAsB,CADpE;AAEA,UAAM3B,SAAS,GAAG,KAAlB;AACA,SAAKC,sBAAL,GACIuB,YAAY,CAACI,KAAb,CAAmB,CAAnB,EAAsBF,aAAtB,EAAqCnC,GAArC,CAAyC0B,CAAC,KAAK;AACJf,kBAAY,EAAEe,CAAC,CAACxB,IADZ;AAEJU,cAAQ,EAAEc,CAAC,CAACV,MAAF,CAASJ,QAAT,CACNH,SADM;AAFN,KAAL,CAA1C,CADJ;AAMA,SAAKI,kBAAL,GACIoB,YAAY,CAACI,KAAb,CAAmBF,aAAnB,EAAkCA,aAAa,GAAG,CAAlD,EACKnC,GADL,CACS0B,CAAC,KAAK;AACJf,kBAAY,EAAEe,CAAC,CAACxB,IADZ;AAEJU,cAAQ,EAAEc,CAAC,CAACV,MAAF,CAASJ,QAAT,CAAkBH,SAAlB;AAFN,KAAL,CADV,CADJ;;AAMA,QAAI,KAAKjB,QAAT,EAAmB;AACjB,WAAKsB,oBAAL,GACImB,YAAY,CAACI,KAAb,CAAmBF,aAAa,GAAG,CAAnC,EAAsCA,aAAa,GAAG,CAAtD,EACKnC,GADL,CACS0B,CAAC,KAAK;AACJf,oBAAY,EAAEe,CAAC,CAACxB,IADZ;AAEJU,gBAAQ,EAAEc,CAAC,CAACV,MAAF,CAASJ,QAAT,CAAkBH,SAAlB;AAFN,OAAL,CADV,CADJ;AAMD;AACF;;AAED6B,WAAS;AACP,WAAO;AACL,sBAAgB,KAAKlD,YADhB;AAEL,eAAS,KAAKC,KAFT;AAGL,kBAAY,KAAKC,QAHZ;AAIL,iBAAW,KAAKC,OAJX;AAKL,kBAAY,KAAKC;AALZ,KAAP;AAOD;AAED;;;AACiB,SAAV+C,UAAU,CACbC,GADa,EACoBC,MADpB,EACsC;AACrD,WAAO,IAAID,GAAJ,CACHC,MAAM,CAAC,cAAD,CADH,EACqBA,MAAM,CAAC,OAAD,CAD3B,EACsCA,MAAM,CAAC,UAAD,CAD5C,EAEHA,MAAM,CAAC,SAAD,CAFH,EAEgBA,MAAM,CAAC,UAAD,CAFtB,CAAP;AAGD;;AA/K4C;AAC7C;;AACOvD,6BAAY,SAAZ,C,CAAwB;;AA+KjCF,aAAa,CAACE,gBAAD,CAAb","names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","sub","zerosLike","registerClass","Optimizer","RMSPropOptimizer","constructor","learningRate","decay","momentum","epsilon","centered","backend","Error","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","accumulatedMeanSquares","originalName","variable","accumulatedMoments","accumulatedMeanGrads","gradient","tensor","accumulatedMeanSquare","newAccumulatedMeanSquare","accumulatedMeanGrad","newAccumulatedMeanGrad","gradContribution","newAccumulatedMoments","assign","newValue","incrementIterations","v","getWeights","variables","push","saveIterations","concat","setWeights","weightValues","extractIterations","variableCount","length","slice","getConfig","fromConfig","cls","config"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/optimizers/rmsprop_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class RMSPropOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'RMSProp';  // Note: Name matters for Python compatibility.\n  private centered: boolean;\n\n  private accumulatedMeanSquares: OptimizerVariable[] = [];\n  private accumulatedMoments: OptimizerVariable[] = [];\n  private accumulatedMeanGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected decay = 0.9,\n      protected momentum = 0.0, protected epsilon: number = null,\n      centered = false) {\n    super();\n\n    this.centered = centered;\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n    if (learningRate == null) {\n      throw new Error(`learningRate for RMSPropOptimizer must be defined.`);\n    }\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedMeanSquares[i] == null) {\n        this.accumulatedMeanSquares[i] = {\n          originalName: `${name}/rms`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMoments[i] == null) {\n        this.accumulatedMoments[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedMeanGrads[i] == null && this.centered) {\n        this.accumulatedMeanGrads[i] = {\n          originalName: `${name}/mg`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;\n      const accumulatedMoments = this.accumulatedMoments[i].variable;\n      tidy(() => {\n        const newAccumulatedMeanSquare =\n            add(mul(accumulatedMeanSquare, this.decay),\n                mul(square(gradient), 1 - this.decay));\n\n        if (this.centered) {\n          const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;\n          // Centered gradient\n          const newAccumulatedMeanGrad =\n              add(mul(accumulatedMeanGrad, this.decay),\n                  mul(gradient, 1 - this.decay));\n\n          const gradContribution =\n              div(mul(gradient, this.learningRate),\n                  sqrt(\n                      sub(newAccumulatedMeanSquare,\n                          add(square(newAccumulatedMeanGrad), this.epsilon))));\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum), gradContribution);\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMeanGrad.assign(newAccumulatedMeanGrad);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        } else {\n          // Plain gradient\n          const newAccumulatedMeanSquare =\n              add(mul(accumulatedMeanSquare, this.decay),\n                  mul(square(gradient), 1 - this.decay));\n\n          const newAccumulatedMoments =\n              add(mul(accumulatedMoments, this.momentum),\n                  div(mul(gradient, this.learningRate),\n                      sqrt(add(newAccumulatedMeanSquare, this.epsilon))));\n\n          accumulatedMeanSquare.assign(newAccumulatedMeanSquare);\n          accumulatedMoments.assign(newAccumulatedMoments);\n\n          const newValue = sub(value, newAccumulatedMoments);\n          value.assign(newValue);\n        }\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedMeanSquares != null) {\n      dispose(this.accumulatedMeanSquares.map(v => v.variable));\n    }\n    if (this.accumulatedMeanGrads != null && this.centered) {\n      dispose(this.accumulatedMeanGrads.map(v => v.variable));\n    }\n    if (this.accumulatedMoments != null) {\n      dispose(this.accumulatedMoments.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedMeanSquares, ...this.accumulatedMoments];\n    if (this.centered) {\n      variables.push(...this.accumulatedMeanGrads);\n    }\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount =\n        this.centered ? weightValues.length / 3 : weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedMeanSquares =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedMoments =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n    if (this.centered) {\n      this.accumulatedMeanGrads =\n          weightValues.slice(variableCount * 2, variableCount * 3)\n              .map(v => ({\n                     originalName: v.name,\n                     variable: v.tensor.variable(trainable)\n                   }));\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'decay': this.decay,\n      'momentum': this.momentum,\n      'epsilon': this.epsilon,\n      'centered': this.centered\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['decay'], config['momentum'],\n        config['epsilon'], config['centered']);\n  }\n}\nregisterClass(RMSPropOptimizer);\n"]},"metadata":{},"sourceType":"module"}