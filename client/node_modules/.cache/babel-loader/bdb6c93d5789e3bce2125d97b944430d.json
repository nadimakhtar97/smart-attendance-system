{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { tidy, util } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport { ValueError } from './errors';\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\n\nexport function l2Normalize(x, axis) {\n  return tidy(() => {\n    if (x.dtype !== 'float32') {\n      x = tfc.cast(x, 'float32');\n    }\n\n    const squareSum = tfc.sum(K.square(x), axis, true);\n    const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\nexport function meanSquaredError(yTrue, yPred) {\n  return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsoluteError(yTrue, yPred) {\n  return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\nexport function meanAbsolutePercentageError(yTrue, yPred) {\n  return tidy(() => {\n    const diff = tfc.sub(yTrue, yPred);\n    const clippedTrue = tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\nexport function meanSquaredLogarithmicError(yTrue, yPred) {\n  return tidy(() => {\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    const firstLog = tfc.log(tfc.add(1, clippedPred));\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    const secondLog = tfc.log(tfc.add(1, clippedTrue));\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\nexport function squaredHinge(yTrue, yPred) {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\nexport function hinge(yTrue, yPred) {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\nexport function categoricalHinge(yTrue, yPred) {\n  return tidy(() => {\n    const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\n\nexport function logcosh(yTrue, yPred) {\n  return tidy(() => {\n    const log2 = Math.log(2);\n    const predictionDiff = tfc.sub(yPred, yTrue);\n    const logcoshResult = tfc.sub(tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))), log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\nexport function categoricalCrossentropy(target, output) {\n  let fromLogits = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return tidy(() => {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      const outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(tfc.mul(tfc.cast(target, 'float32'), tfc.log(output)), output.shape.length - 1));\n  });\n}\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\n\nexport function sparseCategoricalCrossentropy(target, output) {\n  let fromLogits = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return tidy(() => {\n    const flatTarget = tfc.cast(tfc.floor(K.flatten(target)), 'int32');\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    const outputShape = output.shape;\n    const oneHotTarget = tfc.reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\n\nexport function sigmoidCrossEntropyWithLogits(labels, logits) {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(`logits and labels must have the same shape, but got shapes ` + `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n  }\n\n  return tidy(() => {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    const reluLogits = tfc.relu(logits);\n    const negAbsLogits = tfc.neg(tfc.abs(logits));\n    return tfc.add(tfc.sub(reluLogits, tfc.mul(logits, labels)), tfc.log1p(tfc.exp(negAbsLogits)));\n  });\n}\nexport function binaryCrossentropy(yTrue, yPred) {\n  return tidy(() => {\n    let y;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\nexport function kullbackLeiblerDivergence(yTrue, yPred) {\n  return tidy(() => {\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\nexport function poisson(yTrue, yPred) {\n  return tidy(() => {\n    const logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\nexport function cosineProximity(yTrue, yPred) {\n  return tidy(() => {\n    const trueNormalized = l2Normalize(yTrue, -1);\n    const predNormalized = l2Normalize(yPred, -1);\n    const trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity; // TODO(michaelterry): Add deserialize() function.\n\nexport const lossesMap = {\n  meanSquaredError,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  binaryCrossentropy,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity\n}; // Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\n\nexport function get(identifierOrFn) {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n\n    let errMsg = `Unknown loss ${identifierOrFn}`;\n\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = `Unknown loss ${identifierOrFn}. ` + 'Use \"categoricalCrossentropy\" as the string name for ' + 'tf.losses.softmaxCrossEntropy';\n    }\n\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;AACA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAA0BC,IAA1B,EAAgCC,IAAhC,QAA2C,uBAA3C;AAEA,SAAQC,OAAR,QAAsB,kBAAtB;AACA,OAAO,KAAKC,CAAZ,MAAmB,wBAAnB;AACA,SAAQC,UAAR,QAAyB,UAAzB;AAGA;;;;;;AAKA,OAAM,SAAUC,WAAV,CAAsBC,CAAtB,EAAiCC,IAAjC,EAA8C;AAClD,SAAOP,IAAI,CAAC,MAAK;AACf,QAAIM,CAAC,CAACE,KAAF,KAAY,SAAhB,EAA2B;AACzBF,OAAC,GAAGP,GAAG,CAACU,IAAJ,CAASH,CAAT,EAAY,SAAZ,CAAJ;AACD;;AACD,UAAMI,SAAS,GAAGX,GAAG,CAACY,GAAJ,CAAQR,CAAC,CAACS,MAAF,CAASN,CAAT,CAAR,EAAqBC,IAArB,EAA2B,IAA3B,CAAlB;AACA,UAAMM,aAAa,GAAGd,GAAG,CAACe,IAAJ,CAASJ,SAAS,CAACK,KAAnB,EAA0Bb,OAAO,EAAjC,CAAtB;AACA,UAAMc,IAAI,GAAGjB,GAAG,CAACkB,IAAJ,CAASlB,GAAG,CAACmB,OAAJ,CAAYR,SAAZ,EAAuBG,aAAvB,CAAT,CAAb;AACA,WAAOd,GAAG,CAACoB,GAAJ,CAAQb,CAAR,EAAWU,IAAX,CAAP;AACD,GARU,CAAX;AASD;AAED,OAAM,SAAUI,gBAAV,CAA2BC,KAA3B,EAA0CC,KAA1C,EAAuD;AAC3D,SAAOtB,IAAI,CAAC,MAAMD,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAASb,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAT,CAAT,EAA0C,CAAC,CAA3C,CAAP,CAAX;AACD;AAED,OAAM,SAAUI,iBAAV,CAA4BJ,KAA5B,EAA2CC,KAA3C,EAAwD;AAC5D,SAAOtB,IAAI,CAAC,MAAMD,GAAG,CAACwB,IAAJ,CAASxB,GAAG,CAAC2B,GAAJ,CAAQ3B,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAR,CAAT,EAAyC,CAAC,CAA1C,CAAP,CAAX;AACD;AAED,OAAM,SAAUM,2BAAV,CACFN,KADE,EACaC,KADb,EAC0B;AAC9B,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAM4B,IAAI,GAAG7B,GAAG,CAACyB,GAAJ,CAAQH,KAAR,EAAeC,KAAf,CAAb;AACA,UAAMO,WAAW,GACb9B,GAAG,CAAC+B,WAAJ,CAAgB/B,GAAG,CAAC2B,GAAJ,CAAQL,KAAR,CAAhB,EAAgCnB,OAAO,EAAvC,EAA2C6B,MAAM,CAACC,SAAlD,CADJ;AAEA,UAAMC,SAAS,GAAGlC,GAAG,CAAC2B,GAAJ,CAAQ3B,GAAG,CAACoB,GAAJ,CAAQS,IAAR,EAAcC,WAAd,CAAR,CAAlB;AACA,WAAO9B,GAAG,CAACmC,GAAJ,CAAQ,GAAR,EAAanC,GAAG,CAACwB,IAAJ,CAASU,SAAT,EAAoB,CAAC,CAArB,CAAb,CAAP;AACD,GANU,CAAX;AAOD;AAED,OAAM,SAAUE,2BAAV,CACFd,KADE,EACaC,KADb,EAC0B;AAC9B,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAMoC,WAAW,GAAGrC,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC6B,MAAM,CAACC,SAAzC,CAApB;AACA,UAAMK,QAAQ,GAAGtC,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWH,WAAX,CAAR,CAAjB;AAEA,UAAMP,WAAW,GAAG9B,GAAG,CAAC+B,WAAJ,CAAgBT,KAAhB,EAAuBnB,OAAO,EAA9B,EAAkC6B,MAAM,CAACC,SAAzC,CAApB;AACA,UAAMQ,SAAS,GAAGzC,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWV,WAAX,CAAR,CAAlB;AAEA,WAAO9B,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAASb,GAAG,CAACyB,GAAJ,CAAQa,QAAR,EAAkBG,SAAlB,CAAT,CAAT,EAAiD,CAAC,CAAlD,CAAP;AACD,GARU,CAAX;AASD;AAED,OAAM,SAAUC,YAAV,CAAuBpB,KAAvB,EAAsCC,KAAtC,EAAmD;AACvD,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAM0C,SAAS,GAAG3C,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWzB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAX,CAAf,CAAlB;AACA,WAAOvB,GAAG,CAACwB,IAAJ,CAASpB,CAAC,CAACS,MAAF,CAAS8B,SAAT,CAAT,EAA8B,CAAC,CAA/B,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAUC,KAAV,CAAgBtB,KAAhB,EAA+BC,KAA/B,EAA4C;AAChD,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAM0C,SAAS,GAAG3C,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWzB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAX,CAAf,CAAlB;AACA,WAAOvB,GAAG,CAACwB,IAAJ,CAASmB,SAAT,EAAoB,CAAC,CAArB,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAUE,gBAAV,CAA2BvB,KAA3B,EAA0CC,KAA1C,EAAuD;AAC3D,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAM6C,GAAG,GAAG9C,GAAG,CAACY,GAAJ,CAAQZ,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAeC,KAAf,CAAR,EAA+B,CAAC,CAAhC,CAAZ;AACA,UAAMwB,GAAG,GAAG/C,GAAG,CAACgD,GAAJ,CAAQhD,GAAG,CAACmC,GAAJ,CAAQnC,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWH,KAAX,CAAR,EAA2BC,KAA3B,CAAR,EAA2C,CAAC,CAA5C,CAAZ;AACA,WAAOvB,GAAG,CAACmB,OAAJ,CAAY,CAAZ,EAAenB,GAAG,CAACwC,GAAJ,CAAQ,CAAR,EAAWxC,GAAG,CAACyB,GAAJ,CAAQsB,GAAR,EAAaD,GAAb,CAAX,CAAf,CAAP;AACD,GAJU,CAAX;AAKD;AAED;;;;;;;;;AAQA,OAAM,SAAUG,OAAV,CAAkB3B,KAAlB,EAAiCC,KAAjC,EAA8C;AAClD,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAMiD,IAAI,GAAGC,IAAI,CAACZ,GAAL,CAAS,CAAT,CAAb;AACA,UAAMa,cAAc,GAAGpD,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAeD,KAAf,CAAvB;AACA,UAAM+B,aAAa,GAAGrD,GAAG,CAACyB,GAAJ,CAClBzB,GAAG,CAACwC,GAAJ,CAAQY,cAAR,EAAwBpD,GAAG,CAACsD,QAAJ,CAAatD,GAAG,CAACmC,GAAJ,CAAQ,CAAC,CAAT,EAAYiB,cAAZ,CAAb,CAAxB,CADkB,EAElBF,IAFkB,CAAtB;AAGA,WAAOlD,GAAG,CAACwB,IAAJ,CAAS6B,aAAT,EAAwB,CAAC,CAAzB,CAAP;AACD,GAPU,CAAX;AAQD;AAED,OAAM,SAAUE,uBAAV,CACFC,MADE,EACcC,MADd,EACgD;AAAA,MAAlBC,UAAkB,uEAAL,KAAK;AACpD,SAAOzD,IAAI,CAAC,MAAK;AACf,QAAIyD,UAAJ,EAAgB;AACdD,YAAM,GAAGzD,GAAG,CAAC2D,OAAJ,CAAYF,MAAZ,CAAT;AACD,KAFD,MAEO;AACL;AACA,YAAMG,SAAS,GAAG5D,GAAG,CAACY,GAAJ,CAAQ6C,MAAR,EAAgBA,MAAM,CAACzC,KAAP,CAAa6C,MAAb,GAAsB,CAAtC,EAAyC,IAAzC,CAAlB;AACAJ,YAAM,GAAGzD,GAAG,CAACoB,GAAJ,CAAQqC,MAAR,EAAgBG,SAAhB,CAAT;AACD;;AACDH,UAAM,GAAGzD,GAAG,CAAC+B,WAAJ,CAAgB0B,MAAhB,EAAwBtD,OAAO,EAA/B,EAAmC,IAAIA,OAAO,EAA9C,CAAT;AACA,WAAOH,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAACY,GAAJ,CACXZ,GAAG,CAACmC,GAAJ,CAAQnC,GAAG,CAACU,IAAJ,CAAS8C,MAAT,EAAiB,SAAjB,CAAR,EAAqCxD,GAAG,CAACuC,GAAJ,CAAQkB,MAAR,CAArC,CADW,EAEXA,MAAM,CAACzC,KAAP,CAAa6C,MAAb,GAAsB,CAFX,CAAR,CAAP;AAGD,GAZU,CAAX;AAaD;AAED;;;;;;;;;;AASA,OAAM,SAAUC,6BAAV,CACFN,MADE,EACcC,MADd,EACgD;AAAA,MAAlBC,UAAkB,uEAAL,KAAK;AACpD,SAAOzD,IAAI,CAAC,MAAK;AACf,UAAM8D,UAAU,GACZ/D,GAAG,CAACU,IAAJ,CAASV,GAAG,CAACgE,KAAJ,CAAU5D,CAAC,CAAC6D,OAAF,CAAUT,MAAV,CAAV,CAAT,EAAuC,OAAvC,CADJ;AAEAC,UAAM,GAAGzD,GAAG,CAAC+B,WAAJ,CAAgB0B,MAAhB,EAAwBtD,OAAO,EAA/B,EAAmC,IAAIA,OAAO,EAA9C,CAAT;AACA,UAAM+D,WAAW,GAAGT,MAAM,CAACzC,KAA3B;AACA,UAAMmD,YAAY,GAAGnE,GAAG,CAACoE,OAAJ,CACjBpE,GAAG,CAACqE,MAAJ,CAAWN,UAAX,EAAuBG,WAAW,CAACA,WAAW,CAACL,MAAZ,GAAqB,CAAtB,CAAlC,CADiB,EAEjBK,WAFiB,CAArB;AAGA,WAAOX,uBAAuB,CAACY,YAAD,EAAeV,MAAf,EAAuBC,UAAvB,CAA9B;AACD,GATU,CAAX;AAUD;AAED;;;;;;;;;;;;;;;;;;;;;;AAqBA,OAAM,SAAUY,6BAAV,CACFC,MADE,EACcC,MADd,EAC4B;AAChC,MAAI,CAACtE,IAAI,CAACuE,WAAL,CAAiBF,MAAM,CAACvD,KAAxB,EAA+BwD,MAAM,CAACxD,KAAtC,CAAL,EAAmD;AACjD,UAAM,IAAIX,UAAJ,CACF,gEACA,GAAGqE,IAAI,CAACC,SAAL,CAAeJ,MAAM,CAACvD,KAAtB,CAA4B,QAAQ0D,IAAI,CAACC,SAAL,CAAeH,MAAM,CAACxD,KAAtB,CAA4B,EAFjE,CAAN;AAGD;;AACD,SAAOf,IAAI,CAAC,MAAK;AACf;AACA;AACA;AACA;AACA;AACA;AACA,UAAM2E,UAAU,GAAG5E,GAAG,CAAC6E,IAAJ,CAASL,MAAT,CAAnB;AACA,UAAMM,YAAY,GAAG9E,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAAC2B,GAAJ,CAAQ6C,MAAR,CAAR,CAArB;AACA,WAAOxE,GAAG,CAACwC,GAAJ,CACHxC,GAAG,CAACyB,GAAJ,CAAQmD,UAAR,EAAoB5E,GAAG,CAACmC,GAAJ,CAAQqC,MAAR,EAAgBD,MAAhB,CAApB,CADG,EAEHvE,GAAG,CAAC+E,KAAJ,CAAU/E,GAAG,CAACgF,GAAJ,CAAQF,YAAR,CAAV,CAFG,CAAP;AAGD,GAZU,CAAX;AAaD;AAED,OAAM,SAAUG,kBAAV,CAA6B3D,KAA7B,EAA4CC,KAA5C,EAAyD;AAC7D,SAAOtB,IAAI,CAAC,MAAK;AACf,QAAIiF,CAAJ;AACAA,KAAC,GAAGlF,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC,IAAIA,OAAO,EAA7C,CAAJ;AACA+E,KAAC,GAAGlF,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACoB,GAAJ,CAAQ8D,CAAR,EAAWlF,GAAG,CAACyB,GAAJ,CAAQ,CAAR,EAAWyD,CAAX,CAAX,CAAR,CAAJ;AACA,WAAOlF,GAAG,CAACwB,IAAJ,CAAS8C,6BAA6B,CAAChD,KAAD,EAAQ4D,CAAR,CAAtC,EAAkD,CAAC,CAAnD,CAAP;AACD,GALU,CAAX;AAMD;AAED,OAAM,SAAUC,yBAAV,CACF7D,KADE,EACaC,KADb,EAC0B;AAC9B,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAM6B,WAAW,GAAG9B,GAAG,CAAC+B,WAAJ,CAAgBT,KAAhB,EAAuBnB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;AACA,UAAMkC,WAAW,GAAGrC,GAAG,CAAC+B,WAAJ,CAAgBR,KAAhB,EAAuBpB,OAAO,EAA9B,EAAkC,CAAlC,CAApB;AACA,WAAOH,GAAG,CAACY,GAAJ,CACHZ,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAetB,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACoB,GAAJ,CAAQU,WAAR,EAAqBO,WAArB,CAAR,CAAf,CADG,EACyD,CAAC,CAD1D,CAAP;AAED,GALU,CAAX;AAMD;AAED,OAAM,SAAU+C,OAAV,CAAkB9D,KAAlB,EAAiCC,KAAjC,EAA8C;AAClD,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAMoF,OAAO,GAAGrF,GAAG,CAACuC,GAAJ,CAAQvC,GAAG,CAACwC,GAAJ,CAAQrC,OAAO,EAAf,EAAmBoB,KAAnB,CAAR,CAAhB;AACA,WAAOvB,GAAG,CAACwB,IAAJ,CAASxB,GAAG,CAACyB,GAAJ,CAAQF,KAAR,EAAevB,GAAG,CAACmC,GAAJ,CAAQb,KAAR,EAAe+D,OAAf,CAAf,CAAT,EAAkD,CAAC,CAAnD,CAAP;AACD,GAHU,CAAX;AAID;AAED,OAAM,SAAUC,eAAV,CAA0BhE,KAA1B,EAAyCC,KAAzC,EAAsD;AAC1D,SAAOtB,IAAI,CAAC,MAAK;AACf,UAAMsF,cAAc,GAAGjF,WAAW,CAACgB,KAAD,EAAQ,CAAC,CAAT,CAAlC;AACA,UAAMkE,cAAc,GAAGlF,WAAW,CAACiB,KAAD,EAAQ,CAAC,CAAT,CAAlC;AACA,UAAMkE,SAAS,GAAGzF,GAAG,CAACmC,GAAJ,CAAQoD,cAAR,EAAwBC,cAAxB,CAAlB;AACA,WAAOxF,GAAG,CAAC+C,GAAJ,CAAQ/C,GAAG,CAACY,GAAJ,CAAQ6E,SAAR,EAAmB,CAAC,CAApB,CAAR,CAAP;AACD,GALU,CAAX;AAMD;AAED,OAAO,MAAMC,GAAG,GAAGrE,gBAAZ;AACP,OAAO,MAAMsE,GAAG,GAAGtE,gBAAZ;AACP,OAAO,MAAMuE,GAAG,GAAGlE,iBAAZ;AACP,OAAO,MAAMmE,GAAG,GAAGnE,iBAAZ;AACP,OAAO,MAAMoE,IAAI,GAAGlE,2BAAb;AACP,OAAO,MAAMmE,IAAI,GAAGnE,2BAAb;AACP,OAAO,MAAMoE,IAAI,GAAG5D,2BAAb;AACP,OAAO,MAAM6D,IAAI,GAAG7D,2BAAb;AACP,OAAO,MAAM8D,GAAG,GAAGf,yBAAZ;AACP,OAAO,MAAMgB,GAAG,GAAGhB,yBAAZ;AACP,OAAO,MAAMiB,MAAM,GAAGd,eAAf,C,CAEP;;AAEA,OAAO,MAAMe,SAAS,GAA6C;AACjEhF,kBADiE;AAEjEK,mBAFiE;AAGjEE,6BAHiE;AAIjEQ,6BAJiE;AAKjEM,cALiE;AAMjEE,OANiE;AAOjEC,kBAPiE;AAQjEI,SARiE;AASjEM,yBATiE;AAUjEO,+BAViE;AAWjEmB,oBAXiE;AAYjEE,2BAZiE;AAajEC,SAbiE;AAcjEE;AAdiE,CAA5D,C,CAiBP;AACA;;AACA,OAAM,SAAUgB,GAAV,CAAcC,cAAd,EAAmD;AACvD,MAAI,OAAOA,cAAP,KAA0B,QAA9B,EAAwC;AACtC,QAAIA,cAAc,IAAIF,SAAtB,EAAiC;AAC/B,aAAOA,SAAS,CAACE,cAAD,CAAhB;AACD;;AACD,QAAIC,MAAM,GAAG,gBAAgBD,cAAc,EAA3C;;AACA,QAAIA,cAAc,CAACE,WAAf,GAA6BC,QAA7B,CAAsC,qBAAtC,CAAJ,EAAkE;AAChEF,YAAM,GAAG,gBAAgBD,cAAc,IAA9B,GACL,uDADK,GAEL,+BAFJ;AAGD;;AACD,UAAM,IAAIlG,UAAJ,CAAemG,MAAf,CAAN;AACD,GAXD,MAWO;AACL,WAAOD,cAAP;AACD;AACF","names":["tfc","tidy","util","epsilon","K","ValueError","l2Normalize","x","axis","dtype","cast","squareSum","sum","square","epsilonTensor","fill","shape","norm","sqrt","maximum","div","meanSquaredError","yTrue","yPred","mean","sub","meanAbsoluteError","abs","meanAbsolutePercentageError","diff","clippedTrue","clipByValue","Number","MAX_VALUE","absResult","mul","meanSquaredLogarithmicError","clippedPred","firstLog","log","add","secondLog","squaredHinge","maxResult","hinge","categoricalHinge","pos","neg","max","logcosh","log2","Math","predictionDiff","logcoshResult","softplus","categoricalCrossentropy","target","output","fromLogits","softmax","outputSum","length","sparseCategoricalCrossentropy","flatTarget","floor","flatten","outputShape","oneHotTarget","reshape","oneHot","sigmoidCrossEntropyWithLogits","labels","logits","arraysEqual","JSON","stringify","reluLogits","relu","negAbsLogits","log1p","exp","binaryCrossentropy","y","kullbackLeiblerDivergence","poisson","logPred","cosineProximity","trueNormalized","predNormalized","trueXPred","mse","MSE","mae","MAE","mape","MAPE","msle","MSLE","kld","KLD","cosine","lossesMap","get","identifierOrFn","errMsg","toLowerCase","includes"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/losses.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: losses.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {Tensor, Tensor1D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {epsilon} from './backend/common';\nimport * as K from './backend/tfjs_backend';\nimport {ValueError} from './errors';\nimport {LossOrMetricFn} from './types';\n\n/**\n * Normalizes a tensor wrt the L2 norm alongside the specified axis.\n * @param x\n * @param axis Axis along which to perform normalization.\n */\nexport function l2Normalize(x: Tensor, axis?: number): Tensor {\n  return tidy(() => {\n    if (x.dtype !== 'float32') {\n      x = tfc.cast(x, 'float32');\n    }\n    const squareSum = tfc.sum(K.square(x), axis, true);\n    const epsilonTensor = tfc.fill(squareSum.shape, epsilon());\n    const norm = tfc.sqrt(tfc.maximum(squareSum, epsilonTensor));\n    return tfc.div(x, norm);\n  });\n}\n\nexport function meanSquaredError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(K.square(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsoluteError(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => tfc.mean(tfc.abs(tfc.sub(yPred, yTrue)), -1));\n}\n\nexport function meanAbsolutePercentageError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const diff = tfc.sub(yTrue, yPred);\n    const clippedTrue =\n        tfc.clipByValue(tfc.abs(yTrue), epsilon(), Number.MAX_VALUE);\n    const absResult = tfc.abs(tfc.div(diff, clippedTrue));\n    return tfc.mul(100, tfc.mean(absResult, -1));\n  });\n}\n\nexport function meanSquaredLogarithmicError(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), Number.MAX_VALUE);\n    const firstLog = tfc.log(tfc.add(1, clippedPred));\n\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), Number.MAX_VALUE);\n    const secondLog = tfc.log(tfc.add(1, clippedTrue));\n\n    return tfc.mean(K.square(tfc.sub(firstLog, secondLog)), -1);\n  });\n}\n\nexport function squaredHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(K.square(maxResult), -1);\n  });\n}\n\nexport function hinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const maxResult = tfc.maximum(0, tfc.sub(1, tfc.mul(yTrue, yPred)));\n    return tfc.mean(maxResult, -1);\n  });\n}\n\nexport function categoricalHinge(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const pos = tfc.sum(tfc.mul(yTrue, yPred), -1);\n    const neg = tfc.max(tfc.mul(tfc.sub(1, yTrue), yPred), -1);\n    return tfc.maximum(0, tfc.add(1, tfc.sub(neg, pos)));\n  });\n}\n\n/**\n * Logarithm of the hyperbolic cosine of the prediction error.\n *\n * `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n * to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n * like the mean squared error, but will not be so strongly affected by the\n * occasional wildly incorrect prediction.\n */\nexport function logcosh(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const log2 = Math.log(2);\n    const predictionDiff = tfc.sub(yPred, yTrue);\n    const logcoshResult = tfc.sub(\n        tfc.add(predictionDiff, tfc.softplus(tfc.mul(-2, predictionDiff))),\n        log2);\n    return tfc.mean(logcoshResult, -1);\n  });\n}\n\nexport function categoricalCrossentropy(\n    target: Tensor, output: Tensor, fromLogits = false): Tensor {\n  return tidy(() => {\n    if (fromLogits) {\n      output = tfc.softmax(output);\n    } else {\n      // scale preds so that the class probabilities of each sample sum to 1.\n      const outputSum = tfc.sum(output, output.shape.length - 1, true);\n      output = tfc.div(output, outputSum);\n    }\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    return tfc.neg(tfc.sum(\n        tfc.mul(tfc.cast(target, 'float32'), tfc.log(output)),\n        output.shape.length - 1));\n  });\n}\n\n/**\n * Categorical crossentropy with integer targets.\n *\n * @param target An integer tensor.\n * @param output A tensor resulting from a softmax (unless `fromLogits` is\n *  `true`, in which case `output` is expected to be the logits).\n * @param fromLogits Boolean, whether `output` is the result of a softmax, or is\n *   a tensor of logits.\n */\nexport function sparseCategoricalCrossentropy(\n    target: Tensor, output: Tensor, fromLogits = false): Tensor {\n  return tidy(() => {\n    const flatTarget =\n        tfc.cast(tfc.floor(K.flatten(target)), 'int32') as Tensor1D;\n    output = tfc.clipByValue(output, epsilon(), 1 - epsilon());\n    const outputShape = output.shape;\n    const oneHotTarget = tfc.reshape(\n        tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]),\n        outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n  });\n}\n\n/**\n * From TensorFlow's implementation in nn_impl.py:\n *\n * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n *      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n *    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n *    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n *    = (1 - z) * x + log(1 + exp(-x))\n *    = x - x * z + log(1 + exp(-x))\n * For x < 0, to avoid overflow in exp(-x), we reformulate the above\n *      x - x * z + log(1 + exp(-x))\n *    = log(exp(x)) - x * z + log(1 + exp(-x))\n *    = - x * z + log(1 + exp(x))\n * Hence, to ensure stability and avoid overflow, the implementation uses this\n * equivalent formulation\n *    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n *\n * @param labels The labels.\n * @param logits The logits.\n */\nexport function sigmoidCrossEntropyWithLogits(\n    labels: Tensor, logits: Tensor): Tensor {\n  if (!util.arraysEqual(labels.shape, logits.shape)) {\n    throw new ValueError(\n        `logits and labels must have the same shape, but got shapes ` +\n        `${JSON.stringify(labels.shape)} and ${JSON.stringify(logits.shape)}`);\n  }\n  return tidy(() => {\n    // The logistic loss formula from above is\n    //   x - x * z + log(1 + exp(-x))\n    // For x < 0, a more numerically stable formula is\n    //   -x * z + log(1 + exp(x))\n    // Note that these two expressions can be combined into the following:\n    //   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    const reluLogits = tfc.relu(logits);\n    const negAbsLogits = tfc.neg(tfc.abs(logits));\n    return tfc.add(\n        tfc.sub(reluLogits, tfc.mul(logits, labels)),\n        tfc.log1p(tfc.exp(negAbsLogits)));\n  });\n}\n\nexport function binaryCrossentropy(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    let y: Tensor;\n    y = tfc.clipByValue(yPred, epsilon(), 1 - epsilon());\n    y = tfc.log(tfc.div(y, tfc.sub(1, y)));\n    return tfc.mean(sigmoidCrossEntropyWithLogits(yTrue, y), -1);\n  });\n}\n\nexport function kullbackLeiblerDivergence(\n    yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const clippedTrue = tfc.clipByValue(yTrue, epsilon(), 1);\n    const clippedPred = tfc.clipByValue(yPred, epsilon(), 1);\n    return tfc.sum(\n        tfc.mul(yTrue, tfc.log(tfc.div(clippedTrue, clippedPred))), -1);\n  });\n}\n\nexport function poisson(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const logPred = tfc.log(tfc.add(epsilon(), yPred));\n    return tfc.mean(tfc.sub(yPred, tfc.mul(yTrue, logPred)), -1);\n  });\n}\n\nexport function cosineProximity(yTrue: Tensor, yPred: Tensor): Tensor {\n  return tidy(() => {\n    const trueNormalized = l2Normalize(yTrue, -1);\n    const predNormalized = l2Normalize(yPred, -1);\n    const trueXPred = tfc.mul(trueNormalized, predNormalized);\n    return tfc.neg(tfc.sum(trueXPred, -1));\n  });\n}\n\nexport const mse = meanSquaredError;\nexport const MSE = meanSquaredError;\nexport const mae = meanAbsoluteError;\nexport const MAE = meanAbsoluteError;\nexport const mape = meanAbsolutePercentageError;\nexport const MAPE = meanAbsolutePercentageError;\nexport const msle = meanSquaredLogarithmicError;\nexport const MSLE = meanSquaredLogarithmicError;\nexport const kld = kullbackLeiblerDivergence;\nexport const KLD = kullbackLeiblerDivergence;\nexport const cosine = cosineProximity;\n\n// TODO(michaelterry): Add deserialize() function.\n\nexport const lossesMap: {[functionName: string]: LossOrMetricFn} = {\n  meanSquaredError,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy,\n  sparseCategoricalCrossentropy,\n  binaryCrossentropy,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity\n};\n\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\nexport function get(identifierOrFn: string|LossOrMetricFn): LossOrMetricFn {\n  if (typeof identifierOrFn === 'string') {\n    if (identifierOrFn in lossesMap) {\n      return lossesMap[identifierOrFn];\n    }\n    let errMsg = `Unknown loss ${identifierOrFn}`;\n    if (identifierOrFn.toLowerCase().includes('softmaxcrossentropy')) {\n      errMsg = `Unknown loss ${identifierOrFn}. ` +\n          'Use \"categoricalCrossentropy\" as the string name for ' +\n          'tf.losses.softmaxCrossEntropy';\n    }\n    throw new ValueError(errMsg);\n  } else {\n    return identifierOrFn;\n  }\n}\n"]},"metadata":{},"sourceType":"module"}