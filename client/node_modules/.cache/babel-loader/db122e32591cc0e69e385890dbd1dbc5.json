{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Tile } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { clone } from './clone';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Broadcast an array to a compatible shape NumPy-style.\n *\n * The tensor's shape is compared to the broadcast shape from end to beginning.\n * Ones are prepended to the tensor's shape until is has the same length as\n * the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is\n * already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then\n * the input tensor is tiled N times along that axis (using tf.tile).\n *\n * @param input The tensor that is to be broadcasted.\n * @param shape The input is to be broadcast to this shape.\n *\n * @doc {heading: 'Tensors', subheading: 'Transformations'}\n */\n\nfunction broadcastTo_(x, shape) {\n  let input = convertToTensor(x, 'broadcastTo', 'x');\n  const xShape = input.shape;\n\n  if (shape.some(d => !(d > 0) || d % 1 !== 0)) {\n    throw new Error(`broadcastTo(): Invalid broadcast shape [${shape}].`);\n  }\n\n  if (shape.length < input.rank) {\n    throw new Error(`broadcastTo(): shape.length=${shape.length} < input.rank=${input.rank}.`);\n  }\n\n  if (shape.length > input.rank) {\n    const newShape = input.shape.slice();\n\n    while (newShape.length < shape.length) {\n      newShape.unshift(1);\n    }\n\n    input = reshape(input, newShape);\n  }\n\n  const inputShape = input.shape;\n  const reps = Array.from(shape);\n\n  for (let i = shape.length - 1; i >= 0; i--) {\n    if (inputShape[i] === shape[i]) {\n      reps[i] = 1;\n    } else if (input.shape[i] !== 1) {\n      throw new Error(`broadcastTo(): [${xShape}] cannot be broadcast to [${shape}].`);\n    }\n  }\n\n  const axes = reps.map((n, i) => n > 1 ? i : -1).filter(i => i >= 0);\n\n  if (axes.length === 0) {\n    return clone(input);\n  } // TODO call broadcastTo kernel directly once backends implement broadcstTo\n\n\n  const inputs = {\n    x: input\n  };\n  const attrs = {\n    reps\n  };\n  return ENGINE.runKernel(Tile, inputs, attrs);\n}\n\nexport const broadcastTo = op({\n  broadcastTo_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,IAAR,QAA0C,iBAA1C;AAIA,SAAQC,eAAR,QAA8B,oBAA9B;AAGA,SAAQC,KAAR,QAAoB,SAApB;AACA,SAAQC,EAAR,QAAiB,aAAjB;AACA,SAAQC,OAAR,QAAsB,WAAtB;AAEA;;;;;;;;;;;;;;;AAcA,SAASC,YAAT,CACIC,CADJ,EAC0BC,KAD1B,EAC4C;AAC1C,MAAIC,KAAK,GAAGP,eAAe,CAACK,CAAD,EAAI,aAAJ,EAAmB,GAAnB,CAA3B;AACA,QAAMG,MAAM,GAAGD,KAAK,CAACD,KAArB;;AAEA,MAAIA,KAAK,CAACG,IAAN,CAAWC,CAAC,IAAI,EAAEA,CAAC,GAAG,CAAN,KAAYA,CAAC,GAAG,CAAJ,KAAU,CAAtC,CAAJ,EAA8C;AAC5C,UAAM,IAAIC,KAAJ,CAAU,2CAA2CL,KAAK,IAA1D,CAAN;AACD;;AAED,MAAIA,KAAK,CAACM,MAAN,GAAeL,KAAK,CAACM,IAAzB,EAA+B;AAC7B,UAAM,IAAIF,KAAJ,CAAU,+BAA+BL,KAAK,CAACM,MAAM,iBACvDL,KAAK,CAACM,IAAI,GADR,CAAN;AAED;;AAED,MAAIP,KAAK,CAACM,MAAN,GAAeL,KAAK,CAACM,IAAzB,EAA+B;AAC7B,UAAMC,QAAQ,GAAGP,KAAK,CAACD,KAAN,CAAYS,KAAZ,EAAjB;;AACA,WAAOD,QAAQ,CAACF,MAAT,GAAkBN,KAAK,CAACM,MAA/B,EAAuC;AACrCE,cAAQ,CAACE,OAAT,CAAiB,CAAjB;AACD;;AACDT,SAAK,GAAGJ,OAAO,CAACI,KAAD,EAAQO,QAAR,CAAf;AACD;;AAED,QAAMG,UAAU,GAAGV,KAAK,CAACD,KAAzB;AACA,QAAMY,IAAI,GAAaC,KAAK,CAACC,IAAN,CAAWd,KAAX,CAAvB;;AACA,OAAK,IAAIe,CAAC,GAAGf,KAAK,CAACM,MAAN,GAAe,CAA5B,EAA+BS,CAAC,IAAI,CAApC,EAAuCA,CAAC,EAAxC,EAA4C;AAC1C,QAAIJ,UAAU,CAACI,CAAD,CAAV,KAAkBf,KAAK,CAACe,CAAD,CAA3B,EAAgC;AAC9BH,UAAI,CAACG,CAAD,CAAJ,GAAU,CAAV;AACD,KAFD,MAEO,IAAId,KAAK,CAACD,KAAN,CAAYe,CAAZ,MAAmB,CAAvB,EAA0B;AAC/B,YAAM,IAAIV,KAAJ,CACF,mBAAmBH,MAAM,6BAA6BF,KAAK,IADzD,CAAN;AAED;AACF;;AACD,QAAMgB,IAAI,GAAGJ,IAAI,CAACK,GAAL,CAAS,CAACC,CAAD,EAAIH,CAAJ,KAAUG,CAAC,GAAG,CAAJ,GAAQH,CAAR,GAAY,CAAC,CAAhC,EAAmCI,MAAnC,CAA0CJ,CAAC,IAAIA,CAAC,IAAI,CAApD,CAAb;;AAEA,MAAIC,IAAI,CAACV,MAAL,KAAgB,CAApB,EAAuB;AACrB,WAAOX,KAAK,CAACM,KAAD,CAAZ;AACD,GAnCyC,CAqC1C;;;AACA,QAAMmB,MAAM,GAAe;AAACrB,KAAC,EAAEE;AAAJ,GAA3B;AACA,QAAMoB,KAAK,GAAc;AAACT;AAAD,GAAzB;AACA,SAAOpB,MAAM,CAAC8B,SAAP,CACH7B,IADG,EACG2B,MADH,EACmCC,KADnC,CAAP;AAED;;AAED,OAAO,MAAME,WAAW,GAAG3B,EAAE,CAAC;AAACE;AAAD,CAAD,CAAtB","names":["ENGINE","Tile","convertToTensor","clone","op","reshape","broadcastTo_","x","shape","input","xShape","some","d","Error","length","rank","newShape","slice","unshift","inputShape","reps","Array","from","i","axes","map","n","filter","inputs","attrs","runKernel","broadcastTo"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/broadcast_to.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {Tile, TileAttrs, TileInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {Rank, ShapeMap, TensorLike} from '../types';\n\nimport {clone} from './clone';\nimport {op} from './operation';\nimport {reshape} from './reshape';\n\n/**\n * Broadcast an array to a compatible shape NumPy-style.\n *\n * The tensor's shape is compared to the broadcast shape from end to beginning.\n * Ones are prepended to the tensor's shape until is has the same length as\n * the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is\n * already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then\n * the input tensor is tiled N times along that axis (using tf.tile).\n *\n * @param input The tensor that is to be broadcasted.\n * @param shape The input is to be broadcast to this shape.\n *\n * @doc {heading: 'Tensors', subheading: 'Transformations'}\n */\nfunction broadcastTo_<R extends Rank>(\n    x: Tensor|TensorLike, shape: ShapeMap[R]): Tensor<R> {\n  let input = convertToTensor(x, 'broadcastTo', 'x');\n  const xShape = input.shape;\n\n  if (shape.some(d => !(d > 0) || d % 1 !== 0)) {\n    throw new Error(`broadcastTo(): Invalid broadcast shape [${shape}].`);\n  }\n\n  if (shape.length < input.rank) {\n    throw new Error(`broadcastTo(): shape.length=${shape.length} < input.rank=${\n        input.rank}.`);\n  }\n\n  if (shape.length > input.rank) {\n    const newShape = input.shape.slice();\n    while (newShape.length < shape.length) {\n      newShape.unshift(1);\n    }\n    input = reshape(input, newShape);\n  }\n\n  const inputShape = input.shape;\n  const reps: number[] = Array.from(shape);\n  for (let i = shape.length - 1; i >= 0; i--) {\n    if (inputShape[i] === shape[i]) {\n      reps[i] = 1;\n    } else if (input.shape[i] !== 1) {\n      throw new Error(\n          `broadcastTo(): [${xShape}] cannot be broadcast to [${shape}].`);\n    }\n  }\n  const axes = reps.map((n, i) => n > 1 ? i : -1).filter(i => i >= 0);\n\n  if (axes.length === 0) {\n    return clone(input) as Tensor<R>;\n  }\n\n  // TODO call broadcastTo kernel directly once backends implement broadcstTo\n  const inputs: TileInputs = {x: input};\n  const attrs: TileAttrs = {reps};\n  return ENGINE.runKernel(\n      Tile, inputs as {} as NamedTensorMap, attrs as unknown as NamedAttrMap);\n}\n\nexport const broadcastTo = op({broadcastTo_});\n"]},"metadata":{},"sourceType":"module"}