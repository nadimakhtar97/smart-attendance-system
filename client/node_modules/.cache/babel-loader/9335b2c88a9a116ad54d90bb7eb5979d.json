{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { fill } from '../ops/fill';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class AdagradOptimizer extends Optimizer {\n  constructor(learningRate) {\n    let initialAccumulatorValue = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.1;\n    super();\n    this.learningRate = learningRate;\n    this.initialAccumulatorValue = initialAccumulatorValue;\n    this.accumulatedGrads = [];\n  }\n\n  applyGradients(variableGradients) {\n    const variableNames = Array.isArray(variableGradients) ? variableGradients.map(item => item.name) : Object.keys(variableGradients);\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n\n      if (this.accumulatedGrads[i] == null) {\n        const trainable = false;\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accumulator`,\n          variable: tidy(() => fill(value.shape, this.initialAccumulatorValue).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      tidy(() => {\n        const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n        accumulatedGrad.assign(newAccumulatedGrad);\n        const newValue = add(mul(div(gradient, sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose() {\n    if (this.accumulatedGrads != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n    }\n  }\n\n  async getWeights() {\n    // Order matters for Python compatibility.\n    return [await this.saveIterations()].concat(this.accumulatedGrads.map(v => ({\n      name: v.originalName,\n      tensor: v.variable\n    })));\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n    const trainable = false;\n    this.accumulatedGrads = weightValues.map(v => ({\n      originalName: v.name,\n      variable: v.tensor.variable(trainable)\n    }));\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate,\n      'initialAccumulatorValue': this.initialAccumulatorValue\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate'], config['initialAccumulatorValue']);\n  }\n\n}\n/** @nocollapse */\n\nAdagradOptimizer.className = 'Adagrad'; // Note: Name matters for Python compatibility.\n\nregisterClass(AdagradOptimizer);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,OAAR,EAAiBC,IAAjB,QAA4B,YAA5B;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,IAAR,QAAmB,aAAnB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,IAAR,QAAmB,aAAnB;AACA,SAAQC,MAAR,QAAqB,eAArB;AACA,SAAoBC,aAApB,QAA+E,kBAA/E;AAGA,SAAQC,SAAR,QAA2C,aAA3C;AAEA;;AACA,OAAM,MAAOC,gBAAP,SAAgCD,SAAhC,CAAyC;AAM7CE,cACcC,YADd,EACyE;AAAA,QAA7BC,uBAA6B,uEAAH,GAAG;AACvE;AADY;AAA8B;AAHpC,4BAAwC,EAAxC;AAKP;;AAEDC,gBAAc,CAACC,iBAAD,EAAkD;AAC9D,UAAMC,aAAa,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IAClBA,iBAAiB,CAACI,GAAlB,CAAsBC,IAAI,IAAIA,IAAI,CAACC,IAAnC,CADkB,GAElBC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAIAC,iBAAa,CAACQ,OAAd,CAAsB,CAACH,IAAD,EAAOI,CAAP,KAAY;AAChC,YAAMC,KAAK,GAAG3B,MAAM,CAAC4B,mBAAP,CAA2BN,IAA3B,CAAd;;AACA,UAAI,KAAKO,gBAAL,CAAsBH,CAAtB,KAA4B,IAAhC,EAAsC;AACpC,cAAMI,SAAS,GAAG,KAAlB;AACA,aAAKD,gBAAL,CAAsBH,CAAtB,IAA2B;AACzBK,sBAAY,EAAE,GAAGT,IAAI,cADI;AAEzBU,kBAAQ,EAAE9B,IAAI,CACV,MAAMG,IAAI,CAACsB,KAAK,CAACM,KAAP,EAAc,KAAKnB,uBAAnB,CAAJ,CACKkB,QADL,CACcF,SADd,CADI;AAFW,SAA3B;AAMD;;AAED,YAAMI,QAAQ,GAAGhB,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBS,MADR,GAEbnB,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIY,QAAQ,IAAI,IAAhB,EAAsB;AACpB;AACD;;AAED,YAAME,eAAe,GAAG,KAAKP,gBAAL,CAAsBH,CAAtB,EAAyBM,QAAjD;AAEA9B,UAAI,CAAC,MAAK;AACR,cAAMmC,kBAAkB,GAAGlC,GAAG,CAACiC,eAAD,EAAkB5B,MAAM,CAAC0B,QAAD,CAAxB,CAA9B;AACAE,uBAAe,CAACE,MAAhB,CAAuBD,kBAAvB;AAEA,cAAME,QAAQ,GAAGpC,GAAG,CAChBG,GAAG,CAACF,GAAG,CAAC8B,QAAD,EACC3B,IAAI,CAACJ,GAAG,CAACkC,kBAAD,EAAqBrC,MAAM,CAACwC,OAAP,CAAeC,OAAf,EAArB,CAAJ,CADL,CAAJ,EAEC,CAAC,KAAK5B,YAFP,CADa,EAIhBc,KAJgB,CAApB;AAKAA,aAAK,CAACW,MAAN,CAAaC,QAAb;AACD,OAVG,CAAJ;AAWD,KAhCD;AAiCA,SAAKG,mBAAL;AACD;;AAEDzC,SAAO;AACL,QAAI,KAAK4B,gBAAL,IAAyB,IAA7B,EAAmC;AACjC5B,aAAO,CAAC,KAAK4B,gBAAL,CAAsBT,GAAtB,CAA0BuB,CAAC,IAAIA,CAAC,CAACX,QAAjC,CAAD,CAAP;AACD;AACF;;AAEe,QAAVY,UAAU;AACd;AACA,WAAO,CAAC,MAAM,KAAKC,cAAL,EAAP,EAA8BC,MAA9B,CAAqC,KAAKjB,gBAAL,CAAsBT,GAAtB,CACxCuB,CAAC,KAAK;AAACrB,UAAI,EAAEqB,CAAC,CAACZ,YAAT;AAAuBI,YAAM,EAAEQ,CAAC,CAACX;AAAjC,KAAL,CADuC,CAArC,CAAP;AAED;;AAEe,QAAVe,UAAU,CAACC,YAAD,EAA4B;AAC1CA,gBAAY,GAAG,MAAM,KAAKC,iBAAL,CAAuBD,YAAvB,CAArB;AACA,UAAMlB,SAAS,GAAG,KAAlB;AACA,SAAKD,gBAAL,GAAwBmB,YAAY,CAAC5B,GAAb,CACpBuB,CAAC,KAAK;AAACZ,kBAAY,EAAEY,CAAC,CAACrB,IAAjB;AAAuBU,cAAQ,EAAEW,CAAC,CAACR,MAAF,CAASH,QAAT,CAAkBF,SAAlB;AAAjC,KAAL,CADmB,CAAxB;AAED;;AAEDoB,WAAS;AACP,WAAO;AACL,sBAAgB,KAAKrC,YADhB;AAEL,iCAA2B,KAAKC;AAF3B,KAAP;AAID;AAED;;;AACiB,SAAVqC,UAAU,CACbC,GADa,EACoBC,MADpB,EACsC;AACrD,WAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,EAAgCA,MAAM,CAAC,yBAAD,CAAtC,CAAP;AACD;;AAlF4C;AAC7C;;AACO1C,6BAAY,SAAZ,C,CAAwB;;AAkFjCF,aAAa,CAACE,gBAAD,CAAb","names":["ENGINE","dispose","tidy","add","div","fill","mul","sqrt","square","registerClass","Optimizer","AdagradOptimizer","constructor","learningRate","initialAccumulatorValue","applyGradients","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","accumulatedGrads","trainable","originalName","variable","shape","gradient","tensor","accumulatedGrad","newAccumulatedGrad","assign","newValue","backend","epsilon","incrementIterations","v","getWeights","saveIterations","concat","setWeights","weightValues","extractIterations","getConfig","fromConfig","cls","config"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/optimizers/adagrad_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {fill} from '../ops/fill';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdagradOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'Adagrad';  // Note: Name matters for Python compatibility.\n\n  private accumulatedGrads: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, private initialAccumulatorValue = 0.1) {\n    super();\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      if (this.accumulatedGrads[i] == null) {\n        const trainable = false;\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accumulator`,\n          variable: tidy(\n              () => fill(value.shape, this.initialAccumulatorValue)\n                        .variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad = add(accumulatedGrad, square(gradient));\n        accumulatedGrad.assign(newAccumulatedGrad);\n\n        const newValue = add(\n            mul(div(gradient,\n                    sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))),\n                -this.learningRate),\n            value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  dispose(): void {\n    if (this.accumulatedGrads != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n    }\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    return [await this.saveIterations()].concat(this.accumulatedGrads.map(\n        v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const trainable = false;\n    this.accumulatedGrads = weightValues.map(\n        v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'initialAccumulatorValue': this.initialAccumulatorValue,\n    };\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['initialAccumulatorValue']);\n  }\n}\nregisterClass(AdagradOptimizer);\n"]},"metadata":{},"sourceType":"module"}