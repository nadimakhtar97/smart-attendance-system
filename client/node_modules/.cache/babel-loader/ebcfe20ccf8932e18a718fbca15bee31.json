{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// tslint:disable-next-line:max-line-length\nimport { Constant, GlorotNormal, GlorotUniform, HeNormal, HeUniform, Identity, LeCunNormal, LeCunUniform, Ones, Orthogonal, RandomNormal, RandomUniform, TruncatedNormal, VarianceScaling, Zeros } from './initializers';\n/**\n * Initializer that generates tensors initialized to 0.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function zeros() {\n  return new Zeros();\n}\n/**\n * Initializer that generates tensors initialized to 1.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function ones() {\n  return new Ones();\n}\n/**\n * Initializer that generates values initialized to some constant.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function constant(args) {\n  return new Constant(args);\n}\n/**\n * Initializer that generates random values initialized to a uniform\n * distribution.\n *\n * Values will be distributed uniformly between the configured minval and\n * maxval.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function randomUniform(args) {\n  return new RandomUniform(args);\n}\n/**\n * Initializer that generates random values initialized to a normal\n * distribution.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function randomNormal(args) {\n  return new RandomNormal(args);\n}\n/**\n * Initializer that generates random values initialized to a truncated normal.\n * distribution.\n *\n * These values are similar to values from a `RandomNormal` except that values\n * more than two standard deviations from the mean are discarded and re-drawn.\n * This is the recommended initializer for neural network weights and filters.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function truncatedNormal(args) {\n  return new TruncatedNormal(args);\n}\n/**\n * Initializer that generates the identity matrix.\n * Only use for square 2D matrices.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function identity(args) {\n  return new Identity(args);\n}\n/**\n * Initializer capable of adapting its scale to the shape of weights.\n * With distribution=NORMAL, samples are drawn from a truncated normal\n * distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n *   - number of input units in the weight tensor, if mode = FAN_IN.\n *   - number of output units, if mode = FAN_OUT.\n *   - average of the numbers of input and output units, if mode = FAN_AVG.\n * With distribution=UNIFORM,\n * samples are drawn from a uniform distribution\n * within [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\n\nexport function varianceScaling(config) {\n  return new VarianceScaling(config);\n}\n/**\n * Glorot uniform initializer, also called Xavier uniform initializer.\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function glorotUniform(args) {\n  return new GlorotUniform(args);\n}\n/**\n * Glorot normal initializer, also called Xavier normal initializer.\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor.\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function glorotNormal(args) {\n  return new GlorotNormal(args);\n}\n/**\n * He normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function heNormal(args) {\n  return new HeNormal(args);\n}\n/**\n * He uniform initializer.\n *\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / fan_in)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\n\nexport function heUniform(args) {\n  return new HeUniform(args);\n}\n/**\n * LeCun normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(1 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * References:\n *   [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n *   [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function leCunNormal(args) {\n  return new LeCunNormal(args);\n}\n/**\n * LeCun uniform initializer.\n *\n * It draws samples from a uniform distribution in the interval\n * `[-limit, limit]` with `limit = sqrt(3 / fanIn)`,\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function leCunUniform(args) {\n  return new LeCunUniform(args);\n}\n/**\n * Initializer that generates a random orthogonal matrix.\n *\n * Reference:\n * [Saxe et al., http://arxiv.org/abs/1312.6120](http://arxiv.org/abs/1312.6120)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\n\nexport function orthogonal(args) {\n  return new Orthogonal(args);\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;AASA;AACA,SAAQA,QAAR,EAAgCC,YAAhC,EAA8CC,aAA9C,EAA6DC,QAA7D,EAAuEC,SAAvE,EAAkFC,QAAlF,EAAuHC,WAAvH,EAAoIC,YAApI,EAAkJC,IAAlJ,EAAwJC,UAAxJ,EAAoLC,YAApL,EAAoNC,aAApN,EAA+QC,eAA/Q,EAAqTC,eAArT,EAA2VC,KAA3V,QAAuW,gBAAvW;AAEA;;;;;;AAKA,OAAM,SAAUC,KAAV,GAAe;AACnB,SAAO,IAAID,KAAJ,EAAP;AACD;AAED;;;;;;AAKA,OAAM,SAAUE,IAAV,GAAc;AAClB,SAAO,IAAIR,IAAJ,EAAP;AACD;AAED;;;;;;AAKA,OAAM,SAAUS,QAAV,CAAmBC,IAAnB,EAAqC;AACzC,SAAO,IAAIlB,QAAJ,CAAakB,IAAb,CAAP;AACD;AAED;;;;;;;;;;AASA,OAAM,SAAUC,aAAV,CAAwBD,IAAxB,EAA+C;AACnD,SAAO,IAAIP,aAAJ,CAAkBO,IAAlB,CAAP;AACD;AAED;;;;;;;AAMA,OAAM,SAAUE,YAAV,CAAuBF,IAAvB,EAA6C;AACjD,SAAO,IAAIR,YAAJ,CAAiBQ,IAAjB,CAAP;AACD;AAED;;;;;;;;;;;AAUA,OAAM,SAAUG,eAAV,CAA0BH,IAA1B,EAAmD;AACvD,SAAO,IAAIN,eAAJ,CAAoBM,IAApB,CAAP;AACD;AAED;;;;;;;AAMA,OAAM,SAAUI,QAAV,CAAmBJ,IAAnB,EAAqC;AACzC,SAAO,IAAIb,QAAJ,CAAaa,IAAb,CAAP;AACD;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUK,eAAV,CAA0BC,MAA1B,EAAqD;AACzD,SAAO,IAAIX,eAAJ,CAAoBW,MAApB,CAAP;AACD;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUC,aAAV,CAAwBP,IAAxB,EAAqD;AACzD,SAAO,IAAIhB,aAAJ,CAAkBgB,IAAlB,CAAP;AACD;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUQ,YAAV,CAAuBR,IAAvB,EAAoD;AACxD,SAAO,IAAIjB,YAAJ,CAAiBiB,IAAjB,CAAP;AACD;AAED;;;;;;;;;;;;;AAYA,OAAM,SAAUS,QAAV,CAAmBT,IAAnB,EAAgD;AACpD,SAAO,IAAIf,QAAJ,CAAae,IAAb,CAAP;AACD;AAED;;;;;;;;;;;;;AAYA,OAAM,SAAUU,SAAV,CAAoBV,IAApB,EAAiD;AACrD,SAAO,IAAId,SAAJ,CAAcc,IAAd,CAAP;AACD;AAED;;;;;;;;;;;;;;AAaA,OAAM,SAAUW,WAAV,CAAsBX,IAAtB,EAAmD;AACvD,SAAO,IAAIZ,WAAJ,CAAgBY,IAAhB,CAAP;AACD;AAED;;;;;;;;;;AASA,OAAM,SAAUY,YAAV,CAAuBZ,IAAvB,EAAoD;AACxD,SAAO,IAAIX,YAAJ,CAAiBW,IAAjB,CAAP;AACD;AAED;;;;;;;;;AAQA,OAAM,SAAUa,UAAV,CAAqBb,IAArB,EAAyC;AAC7C,SAAO,IAAIT,UAAJ,CAAeS,IAAf,CAAP;AACD","names":["Constant","GlorotNormal","GlorotUniform","HeNormal","HeUniform","Identity","LeCunNormal","LeCunUniform","Ones","Orthogonal","RandomNormal","RandomUniform","TruncatedNormal","VarianceScaling","Zeros","zeros","ones","constant","args","randomUniform","randomNormal","truncatedNormal","identity","varianceScaling","config","glorotUniform","glorotNormal","heNormal","heUniform","leCunNormal","leCunUniform","orthogonal"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/exports_initializers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// tslint:disable-next-line:max-line-length\nimport {Constant, ConstantArgs, GlorotNormal, GlorotUniform, HeNormal, HeUniform, Identity, IdentityArgs, Initializer, LeCunNormal, LeCunUniform, Ones, Orthogonal, OrthogonalArgs, RandomNormal, RandomNormalArgs, RandomUniform, RandomUniformArgs, SeedOnlyInitializerArgs, TruncatedNormal, TruncatedNormalArgs, VarianceScaling, VarianceScalingArgs, Zeros} from './initializers';\n\n/**\n * Initializer that generates tensors initialized to 0.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function zeros(): Zeros {\n  return new Zeros();\n}\n\n/**\n * Initializer that generates tensors initialized to 1.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function ones(): Initializer {\n  return new Ones();\n}\n\n/**\n * Initializer that generates values initialized to some constant.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function constant(args: ConstantArgs): Initializer {\n  return new Constant(args);\n}\n\n/**\n * Initializer that generates random values initialized to a uniform\n * distribution.\n *\n * Values will be distributed uniformly between the configured minval and\n * maxval.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function randomUniform(args: RandomUniformArgs): Initializer {\n  return new RandomUniform(args);\n}\n\n/**\n * Initializer that generates random values initialized to a normal\n * distribution.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function randomNormal(args: RandomNormalArgs): Initializer {\n  return new RandomNormal(args);\n}\n\n/**\n * Initializer that generates random values initialized to a truncated normal.\n * distribution.\n *\n * These values are similar to values from a `RandomNormal` except that values\n * more than two standard deviations from the mean are discarded and re-drawn.\n * This is the recommended initializer for neural network weights and filters.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function truncatedNormal(args: TruncatedNormalArgs): Initializer {\n  return new TruncatedNormal(args);\n}\n\n/**\n * Initializer that generates the identity matrix.\n * Only use for square 2D matrices.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function identity(args: IdentityArgs): Initializer {\n  return new Identity(args);\n}\n\n/**\n * Initializer capable of adapting its scale to the shape of weights.\n * With distribution=NORMAL, samples are drawn from a truncated normal\n * distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\n *   - number of input units in the weight tensor, if mode = FAN_IN.\n *   - number of output units, if mode = FAN_OUT.\n *   - average of the numbers of input and output units, if mode = FAN_AVG.\n * With distribution=UNIFORM,\n * samples are drawn from a uniform distribution\n * within [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\nexport function varianceScaling(config: VarianceScalingArgs): Initializer {\n  return new VarianceScaling(config);\n}\n\n/**\n * Glorot uniform initializer, also called Xavier uniform initializer.\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function glorotUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new GlorotUniform(args);\n}\n\n/**\n * Glorot normal initializer, also called Xavier normal initializer.\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / (fan_in + fan_out))`\n * where `fan_in` is the number of input units in the weight tensor\n * and `fan_out` is the number of output units in the weight tensor.\n *\n * Reference:\n *   Glorot & Bengio, AISTATS 2010\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function glorotNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new GlorotNormal(args);\n}\n\n/**\n * He normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(2 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function heNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new HeNormal(args);\n}\n\n/**\n * He uniform initializer.\n *\n * It draws samples from a uniform distribution within [-limit, limit]\n * where `limit` is `sqrt(6 / fan_in)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * Reference:\n *     He et al., http://arxiv.org/abs/1502.01852\n *\n * @doc {heading: 'Initializers',namespace: 'initializers'}\n */\nexport function heUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new HeUniform(args);\n}\n\n/**\n * LeCun normal initializer.\n *\n * It draws samples from a truncated normal distribution centered on 0\n * with `stddev = sqrt(1 / fanIn)`\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * References:\n *   [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n *   [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function leCunNormal(args: SeedOnlyInitializerArgs): Initializer {\n  return new LeCunNormal(args);\n}\n\n/**\n * LeCun uniform initializer.\n *\n * It draws samples from a uniform distribution in the interval\n * `[-limit, limit]` with `limit = sqrt(3 / fanIn)`,\n * where `fanIn` is the number of input units in the weight tensor.\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function leCunUniform(args: SeedOnlyInitializerArgs): Initializer {\n  return new LeCunUniform(args);\n}\n\n/**\n * Initializer that generates a random orthogonal matrix.\n *\n * Reference:\n * [Saxe et al., http://arxiv.org/abs/1312.6120](http://arxiv.org/abs/1312.6120)\n *\n * @doc {heading: 'Initializers', namespace: 'initializers'}\n */\nexport function orthogonal(args: OrthogonalArgs): Initializer {\n  return new Orthogonal(args);\n}\n"]},"metadata":{},"sourceType":"module"}