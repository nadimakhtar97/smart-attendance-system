{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\nimport { cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n\n    return output;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      maxValue: this.maxValue\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 0.3;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer = getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` + `but got ${args.sharedAxes}`);\n    }\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape = inputShape.slice(1);\n\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n\n    this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint); // Set input spec.\n\n    const axes = {};\n\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes\n    })];\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  getConfig() {\n    const config = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_ALPHA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` + `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      alpha: this.alpha\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_THETA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return mul(x, cast(greater(x, this.theta), 'float32'));\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      theta: this.theta\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.DEFAULT_AXIS = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  call(inputs, kwargs) {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,SAAQA,IAAR,EAAcC,WAAd,EAA2BC,GAA3B,EAAgCC,OAAhC,EAAyCC,SAAzC,EAAoDC,GAApD,EAAyDC,KAAzD,EAAgEC,IAAhE,EAAsEC,aAAtE,QAAkG,uBAAlG;AAEA,SAAQC,OAAO,IAAIC,iBAAnB,QAA2C,gBAA3C;AACA,SAAoBC,aAApB,EAAmCC,mBAAnC,QAA6D,gBAA7D;AACA,SAAQC,SAAR,EAAmBC,KAAnB,QAA0C,oBAA1C;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AACA,SAAQC,cAAR,EAA4DC,oBAA5D,QAAuF,iBAAvF;AAEA,SAAQC,cAAR,EAAqCC,oBAArC,QAAgE,iBAAhE;AAEA,SAAQC,kBAAR,EAA4BC,mBAA5B,QAAsD,sBAAtD;AAUA,OAAM,MAAOC,IAAP,SAAoBT,KAApB,CAAyB;AAK7BU,cAAYC,IAAZ,EAAgC;AAC9B,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AACA,SAAKC,eAAL,GAAuB,IAAvB;;AACA,QAAID,IAAI,IAAI,IAAZ,EAAkB;AAChB,WAAKE,QAAL,GAAgBF,IAAI,CAACE,QAArB;AACD;AACF;;AAEDC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1CD,UAAM,GAAGP,mBAAmB,CAACO,MAAD,CAA5B;AACA,QAAIE,MAAM,GAAGxB,IAAI,CAACsB,MAAD,CAAjB;;AACA,QAAI,KAAKF,QAAL,IAAiB,IAArB,EAA2B;AACzBI,YAAM,GAAG9B,WAAW,CAAC8B,MAAD,EAAS,CAAT,EAAY,KAAKJ,QAAjB,CAApB;AACD;;AACD,WAAOI,MAAP;AACD;;AAEDC,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AAACR,cAAQ,EAAE,KAAKA;AAAhB,KAAzC;AACA,UAAMS,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AA/B4B;AAC7B;;AACOZ,iBAAY,MAAZ;AA+BTf,aAAa,CAAC+B,aAAd,CAA4BhB,IAA5B;AASA,OAAM,MAAOiB,SAAP,SAAyB1B,KAAzB,CAA8B;AAOlCU,cAAYC,IAAZ,EAAqC;AACnC,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AAHO,yBAAgB,GAAhB;;AAIP,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AACD,SAAKgB,KAAL,GAAahB,IAAI,CAACgB,KAAL,IAAc,IAAd,GAAqB,KAAKC,aAA1B,GAA0CjB,IAAI,CAACgB,KAA5D;AACD;;AAEDb,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,UAAMa,CAAC,GAAGrB,mBAAmB,CAACO,MAAD,CAA7B;AACA,WAAOzB,SAAS,CAACuC,CAAD,EAAI,KAAKF,KAAT,CAAhB;AACD;;AAEDT,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AAACM,WAAK,EAAE,KAAKA;AAAb,KAAzC;AACA,UAAML,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AA7BiC;AAClC;;AACOK,sBAAY,WAAZ;AA6BThC,aAAa,CAAC+B,aAAd,CAA4BC,SAA5B;AA6BA,OAAM,MAAOI,KAAP,SAAqB9B,KAArB,CAA0B;AAW9BU,cAAYC,IAAZ,EAAiC;AAC/B,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AAHO,qCAAmD,OAAnD;;AAIP,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AAED,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKmB,gBAAL,GACI5B,cAAc,CAACQ,IAAI,CAACoB,gBAAL,IAAyB,KAAKC,yBAA/B,CADlB;AAEA,SAAKC,gBAAL,GAAwB5B,cAAc,CAACM,IAAI,CAACsB,gBAAN,CAAtC;AACA,SAAKC,eAAL,GAAuBrC,aAAa,CAACc,IAAI,CAACuB,eAAN,CAApC;;AACA,QAAIvB,IAAI,CAACwB,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAKA,UAAL,GAAkB,IAAlB;AACD,KAFD,MAEO,IAAIC,KAAK,CAACC,OAAN,CAAc1B,IAAI,CAACwB,UAAnB,CAAJ,EAAoC;AACzC,WAAKA,UAAL,GAAkBxB,IAAI,CAACwB,UAAvB;AACD,KAFM,MAEA,IAAI,OAAOxB,IAAI,CAACwB,UAAZ,KAA2B,QAA/B,EAAyC;AAC9C,WAAKA,UAAL,GAAkB,CAACxB,IAAI,CAACwB,UAAN,CAAlB;AACD,KAFM,MAEA;AACL,YAAM,IAAIjC,UAAJ,CACF,gEACA,WAAWS,IAAI,CAACwB,UAAU,EAFxB,CAAN;AAGD;AACF;;AAEDG,OAAK,CAACnB,UAAD,EAA0B;AAC7BA,cAAU,GAAGZ,kBAAkB,CAACY,UAAD,CAA/B;AACA,UAAMoB,UAAU,GAAUpB,UAAU,CAACqB,KAAX,CAAiB,CAAjB,CAA1B;;AACA,QAAI,KAAKL,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAK,MAAMM,CAAX,IAAgB,KAAKN,UAArB,EAAiC;AAC/BI,kBAAU,CAACE,CAAC,GAAG,CAAL,CAAV,GAAoB,CAApB;AACD;AACF;;AACD,SAAKd,KAAL,GAAa,KAAKe,SAAL,CACT,OADS,EACAH,UADA,EACY,SADZ,EACuB,KAAKR,gBAD5B,EAET,KAAKE,gBAFI,EAEc,IAFd,EAEoB,KAAKC,eAFzB,CAAb,CAR6B,CAW7B;;AACA,UAAMS,IAAI,GAA6B,EAAvC;;AACA,QAAI,KAAKR,UAAL,IAAmB,IAAvB,EAA6B;AAC3B,WAAK,IAAIM,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGtB,UAAU,CAACyB,MAA/B,EAAuC,EAAEH,CAAzC,EAA4C;AAC1CE,YAAI,CAACF,CAAD,CAAJ,GAAUtB,UAAU,CAACsB,CAAD,CAApB;AACD;AACF;;AACD,SAAKI,SAAL,GAAiB,CAAC,IAAI9C,SAAJ,CAAc;AAC9B+C,UAAI,EAAE3B,UAAU,CAACyB,MADa;AAE9BD;AAF8B,KAAd,CAAD,CAAjB;AAIA,SAAKI,KAAL,GAAa,IAAb;AACD;;AAEDjC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1CD,UAAM,GAAGP,mBAAmB,CAACO,MAAD,CAA5B;AACA,WAAOvB,KAAK,CAACuB,MAAD,EAAS,KAAKY,KAAL,CAAWqB,IAAX,EAAT,CAAZ;AACD;;AAED5B,WAAS;AACP,UAAMC,MAAM,GAA6B;AACvCU,sBAAgB,EAAE3B,oBAAoB,CAAC,KAAK2B,gBAAN,CADC;AAEvCE,sBAAgB,EAAE3B,oBAAoB,CAAC,KAAK2B,gBAAN,CAFC;AAGvCC,qBAAe,EAAEpC,mBAAmB,CAAC,KAAKoC,eAAN,CAHG;AAIvCC,gBAAU,EAAE,KAAKA;AAJsB,KAAzC;AAMA,UAAMb,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AA3E6B;AAC9B;;AACOS,kBAAY,OAAZ;AA2ETpC,aAAa,CAAC+B,aAAd,CAA4BK,KAA5B;AASA,OAAM,MAAOmB,GAAP,SAAmBjD,KAAnB,CAAwB;AAO5BU,cAAYC,IAAZ,EAA+B;AAC7B,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AAHO,yBAAgB,GAAhB;;AAIP,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AAED,QAAIA,IAAI,CAACgB,KAAL,IAAc,IAAd,IAAsBhB,IAAI,CAACgB,KAAL,KAAe,KAAKC,aAA9C,EAA6D;AAC3D,YAAM,IAAI3B,mBAAJ,CACF,4BAA4BU,IAAI,CAACgB,KAAK,4BAAtC,GACA,gBAFE,CAAN;AAGD;;AAED,SAAKA,KAAL,GAAahB,IAAI,CAACgB,KAAL,IAAc,IAAd,GAAqB,KAAKC,aAA1B,GAA0CjB,IAAI,CAACgB,KAA5D;AACD;;AAEDb,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,UAAMa,CAAC,GAAGrB,mBAAmB,CAACO,MAAD,CAA7B;AACA,WAAO3B,GAAG,CAACyC,CAAD,CAAV;AACD;;AAEDX,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AAACM,WAAK,EAAE,KAAKA;AAAb,KAAzC;AACA,UAAML,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AApC2B;AAC5B;;AACO4B,gBAAY,KAAZ;AAoCTvD,aAAa,CAAC+B,aAAd,CAA4BwB,GAA5B;AASA,OAAM,MAAOC,eAAP,SAA+BlD,KAA/B,CAAoC;AAOxCU,cAAYC,IAAZ,EAA2C;AACzC,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AAHO,yBAAgB,GAAhB;;AAIP,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AAED,SAAKwC,KAAL,GAAaxC,IAAI,CAACwC,KAAL,IAAc,IAAd,GAAqB,KAAKC,aAA1B,GAA0CzC,IAAI,CAACwC,KAA5D;AACD;;AAEDrC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,UAAMa,CAAC,GAAGrB,mBAAmB,CAACO,MAAD,CAA7B;AACA,WAAOxB,GAAG,CAACsC,CAAD,EAAI3C,IAAI,CAACG,OAAO,CAACwC,CAAD,EAAI,KAAKsB,KAAT,CAAR,EAAyB,SAAzB,CAAR,CAAV;AACD;;AAEDjC,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AAAC8B,WAAK,EAAE,KAAKA;AAAb,KAAzC;AACA,UAAM7B,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AA9BuC;AACxC;;AACO6B,4BAAY,iBAAZ;AA8BTxD,aAAa,CAAC+B,aAAd,CAA4ByB,eAA5B;AAUA,OAAM,MAAOvD,OAAP,SAAuBK,KAAvB,CAA4B;AAOhCU,cAAYC,IAAZ,EAAmC;AACjC,UAAMA,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoBA,IAA1B;AAHO,wBAAe,GAAf;;AAIP,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AACD,SAAK0C,OAAL,GAAe,IAAIzD,iBAAJ,GAAwB0D,KAAvC;AACA,SAAKC,IAAL,GAAY5C,IAAI,CAAC4C,IAAL,IAAa,IAAb,GAAoB,KAAKC,YAAzB,GAAwC7C,IAAI,CAAC4C,IAAzD;AACD;;AAEDzC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,UAAMa,CAAC,GAAGrB,mBAAmB,CAACO,MAAD,CAA7B;AACA,WAAO,KAAKsC,OAAL,CAAaxB,CAAb,EAAgB,KAAK0B,IAArB,CAAP;AACD;;AAEDrC,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,MAAM,GAA6B;AAACkC,UAAI,EAAE,KAAKA;AAAZ,KAAzC;AACA,UAAMjC,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AA9B+B;AAChC;;AACO1B,oBAAY,SAAZ;AA8BTD,aAAa,CAAC+B,aAAd,CAA4B9B,OAA5B","names":["cast","clipByValue","elu","greater","leakyRelu","mul","prelu","relu","serialization","Softmax","softmaxActivation","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","getExactlyOneShape","getExactlyOneTensor","ReLU","constructor","args","supportsMasking","maxValue","call","inputs","kwargs","output","computeOutputShape","inputShape","getConfig","config","baseConfig","Object","assign","registerClass","LeakyReLU","alpha","DEFAULT_ALPHA","x","PReLU","alphaInitializer","DEFAULT_ALPHA_INITIALIZER","alphaRegularizer","alphaConstraint","sharedAxes","Array","isArray","build","paramShape","slice","i","addWeight","axes","length","inputSpec","ndim","built","read","ELU","ThresholdedReLU","theta","DEFAULT_THETA","softmax","apply","axis","DEFAULT_AXIS"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/layers/advanced_activations.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\n\nimport {cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization, Tensor} from '@tensorflow/tfjs-core';\n\nimport {Softmax as softmaxActivation} from '../activations';\nimport {Constraint, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nexport declare interface ReLULayerArgs extends LayerArgs {\n  /**\n   * Float, the maximum output value.\n   */\n  maxValue?: number;\n}\n\nexport class ReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ReLU';\n  maxValue: number;\n\n  constructor(args?: ReLULayerArgs) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n    return output;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {maxValue: this.maxValue};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ReLU);\n\nexport declare interface LeakyReLULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `0.3`.\n   */\n  alpha?: number;\n}\n\nexport class LeakyReLU extends Layer {\n  /** @nocollapse */\n  static className = 'LeakyReLU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 0.3;\n\n  constructor(args?: LeakyReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LeakyReLU);\n\nexport declare interface PReLULayerArgs extends LayerArgs {\n  /**\n   * Initializer for the learnable alpha.\n   */\n  alphaInitializer?: Initializer|InitializerIdentifier;\n\n  /**\n   * Regularizer for the learnable alpha.\n   */\n  alphaRegularizer?: Regularizer;\n\n  /**\n   * Constraint for the learnable alpha.\n   */\n  alphaConstraint?: Constraint;\n\n  /**\n   * The axes along which to share learnable parameters for the activation\n   * function. For example, if the incoming feature maps are from a 2D\n   * convolution with output shape `[numExamples, height, width, channels]`,\n   * and you wish to share parameters across space (height and width) so that\n   * each filter channels has only one set of parameters, set\n   * `shared_axes: [1, 2]`.\n   */\n  sharedAxes?: number|number[];\n}\n\nexport class PReLU extends Layer {\n  /** @nocollapse */\n  static className = 'PReLU';\n  private readonly alphaInitializer: Initializer;\n  private readonly alphaRegularizer: Regularizer;\n  private readonly alphaConstraint: Constraint;\n  private readonly sharedAxes: number[];\n  private alpha: LayerVariable;\n\n  readonly DEFAULT_ALPHA_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  constructor(args?: PReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer =\n        getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\n          `Expected sharedAxes to be a number or an array of numbers, ` +\n          `but got ${args.sharedAxes}`);\n    }\n  }\n\n  build(inputShape: Shape|Shape[]) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape: Shape = inputShape.slice(1);\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n    this.alpha = this.addWeight(\n        'alpha', paramShape, 'float32', this.alphaInitializer,\n        this.alphaRegularizer, true, this.alphaConstraint);\n    // Set input spec.\n    const axes: {[axis: number]: number} = {};\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes,\n    })];\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(PReLU);\n\nexport declare interface ELULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `1.0`.\n   */\n  alpha?: number;\n}\n\nexport class ELU extends Layer {\n  /** @nocollapse */\n  static className = 'ELU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 1.0;\n\n  constructor(args?: ELULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\n          `Non-default alpha value (${args.alpha}) is not supported by the ` +\n          `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ELU);\n\nexport declare interface ThresholdedReLULayerArgs extends LayerArgs {\n  /**\n   * Float >= 0. Threshold location of activation.\n   */\n  theta?: number;\n}\n\nexport class ThresholdedReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ThresholdedReLU';\n  readonly theta: number;\n\n  readonly DEFAULT_THETA = 1.0;\n\n  constructor(args?: ThresholdedReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return mul(x, cast(greater(x, this.theta), 'float32'));\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {theta: this.theta};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ThresholdedReLU);\n\nexport declare interface SoftmaxLayerArgs extends LayerArgs {\n  /**\n   * Integer, axis along which the softmax normalization is applied.\n   * Defaults to `-1` (i.e., the last axis).\n   */\n  axis?: number;\n}\n\nexport class Softmax extends Layer {\n  /** @nocollapse */\n  static className = 'Softmax';\n  readonly axis: number;\n  readonly softmax: (t: Tensor, a?: number) => Tensor;\n  readonly DEFAULT_AXIS = 1.0;\n\n  constructor(args?: SoftmaxLayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {axis: this.axis};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Softmax);\n"]},"metadata":{},"sourceType":"module"}