{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast, Identity } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport * as log from './log';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\n\nfunction isRegisteredKernelInvocation(kernelInvocation) {\n  return kernelInvocation.kernelName != null;\n}\n\nclass EngineState {\n  constructor() {\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0; // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n\n    this.gradientDepth = 0; // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\n     * Keeps track of the number of data moves during a kernel execution. We\n     * maintain a stack since kernels can call other kernels, recursively.\n     */\n\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(k => k.name)));\n      }\n\n    };\n  }\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n\n}\n\nexport class Engine {\n  constructor(ENV) {\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n\n  async ready() {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n\n    if (this.backendInstance != null) {\n      return;\n    }\n\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n\n  get backend() {\n    if (this.pendingBackendInit != null) {\n      throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` + `sure to await tf.ready() or await tf.setBackend() before calling ` + `other methods`);\n    }\n\n    if (this.backendInstance == null) {\n      const {\n        name,\n        asyncInit\n      } = this.initializeBackendsAndReturnBest();\n\n      if (asyncInit) {\n        throw new Error(`The highest priority backend '${name}' has not yet been ` + `initialized. Make sure to await tf.ready() or ` + `await tf.setBackend() before calling other methods`);\n      }\n\n      this.setBackend(name);\n    }\n\n    return this.backendInstance;\n  }\n\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName) {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {\n          asyncInit\n        } = this.initializeBackend(backendName);\n\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(backendName, factory) {\n    let priority = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n\n    if (backendName in this.registryFactory) {\n      log.warn(`${backendName} backend was already registered. ` + `Reusing existing backend factory.`);\n      return false;\n    }\n\n    this.registryFactory[backendName] = {\n      factory,\n      priority\n    };\n    return true;\n  }\n\n  async setBackend(backendName) {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n\n    this.backendName = backendName;\n\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n\n      if (!result) {\n        return false;\n      }\n    }\n\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels(); // Reset the profiler.\n\n    this.profiler = new Profiler(this.backendInstance);\n    return true;\n  }\n\n  setupRegisteredKernels() {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  disposeRegisteredKernels(backendName) {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n\n\n  initializeBackend(backendName) {\n    const registryFactoryEntry = this.registryFactory[backendName];\n\n    if (registryFactoryEntry == null) {\n      throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\n      Done in a more liberal way than\n      previous 'Promise.resolve(backend)===backend'\n      as we needed to account for custom Promise\n      implementations (e.g. Angular) */\n\n      if (backend && !(backend instanceof KernelBackend) && typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success = backend.then(backendInstance => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.registry[backendName] = backendInstance;\n          this.pendingBackendInit = null;\n          return true;\n        }).catch(err => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n\n          this.pendingBackendInit = null;\n          log.warn(`Initialization of backend ${backendName} failed`);\n          log.warn(err.stack || err.message);\n          return false;\n        });\n        this.pendingBackendInit = success;\n        return {\n          success,\n          asyncInit: true\n        };\n      } else {\n        this.registry[backendName] = backend;\n        return {\n          success: true,\n          asyncInit: false\n        };\n      }\n    } catch (err) {\n      log.warn(`Initialization of backend ${backendName} failed`);\n      log.warn(err.stack || err.message);\n      return {\n        success: false,\n        asyncInit: false\n      };\n    }\n  }\n\n  removeBackend(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName]; // Unset the backend if it is active.\n\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  getSortedBackends() {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n\n    return Object.keys(this.registryFactory).sort((a, b) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority - this.registryFactory[a].priority;\n    });\n  }\n\n  initializeBackendsAndReturnBest() {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n\n      if (asyncInit || success) {\n        return {\n          name: backendName,\n          asyncInit\n        };\n      }\n    }\n\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n\n  moveData(backend, dataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    const refCount = srcBackend.refCount(dataId); // Delete the tensor from the old backend and move it to the new\n    // backend.\n\n    srcBackend.disposeData(dataId, true);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype, refCount);\n\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy(nameOrFn, fn) {\n    let name = null;\n\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n      }\n\n      if (typeof fn !== 'function') {\n        throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n      }\n\n      name = nameOrFn; // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n\n    let result;\n    return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n      result = fn();\n\n      if (result instanceof Promise) {\n        console.error('Cannot return a Promise inside of tidy.');\n      }\n\n      return result;\n    });\n  }\n\n  scopedRun(start, end, f) {\n    start();\n\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  nextTensorId() {\n    return Engine.nextTensorId++;\n  }\n\n  nextVariableId() {\n    return Engine.nextVariableId++;\n  }\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   */\n\n\n  clone(x) {\n    const y = ENGINE.runKernel(Identity, {\n      x\n    });\n    const inputs = {\n      x\n    };\n\n    const grad = dy => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {\n          x: dy\n        };\n        const attrs = {\n          dtype\n        };\n        return ENGINE.runKernel(Cast, gradInputs, // tslint:disable-next-line: no-unnecessary-type-assertion\n        attrs);\n      }\n    });\n\n    const saved = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n\n\n  runKernel(kernelName, inputs, attrs) {\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    const hasKernel = getKernel(kernelName, this.backendName) != null;\n\n    if (!hasKernel) {\n      throw new Error(`Kernel '${kernelName}' not registered for backend '${this.backendName}'`);\n    }\n\n    return this.runKernelFunc({\n      kernelName,\n      inputs,\n      attrs\n    });\n  }\n\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n    const numDataIdsAfter = this.backend.numDataIds(); // Count the number of data ids associated with the result of the kernel.\n\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n    }); // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n\n    const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n\n    if (dataIdsLeaked > 0) {\n      throw new Error(`Backend '${this.backendName}' has an internal memory leak ` + `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n  /**\n   * Internal helper method to execute a kernel Func\n   *\n   * Use `runKernel` to execute kernels from outside of engine.\n   */\n\n\n  runKernelFunc(kernelParams) {\n    let outputs;\n    let saved = [];\n    const isTapeOn = this.isTapeOn();\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc;\n\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    let out;\n    const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ? kernelParams.kernelName : this.state.activeScope != null ? this.state.activeScope.name : ''; // Create the kernelFunc from either a registered kernel OR passed in\n    // forward/backward functions (used by custom grad). In this context a\n    // kernelFunc wraps a kernel implementation with some bookkeeping.\n\n    if (isRegisteredKernelInvocation(kernelParams)) {\n      const {\n        kernelName,\n        inputs,\n        attrs\n      } = kernelParams;\n\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n\n      const kernel = getKernel(kernelName, this.backendName);\n      util.assert(kernel != null, () => `Cannot find registered kernel '${kernelName}' for backend '${this.backendName}'`);\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({\n          inputs,\n          attrs,\n          backend: this.backend\n        });\n        const outInfos = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map(outInfo => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if (outInfo.rank != null) {\n            return outInfo;\n          }\n\n          const {\n            dataId,\n            shape,\n            dtype\n          } = outInfo;\n          return this.makeTensorFromDataId(dataId, shape, dtype);\n        }); // Save any required inputs and outputs.\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since there would be no backprop for these tensors\n        // (which would otherwise dispose them).\n\n        if (isTapeOn) {\n          const tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n\n        return outTensors;\n      };\n    } else {\n      const {\n        forwardFunc\n      } = kernelParams; // Running a customGrad op.\n\n      const saveFunc = tensors => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = Array.isArray(out) ? out : [out];\n\n        if (this.shouldCheckForMemLeaks()) {\n          // Scope name is used to print a more helpful error message if needed.\n          this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n        }\n\n        return outs;\n      };\n    } //\n    // Run the kernelFunc. Optionally profiling it.\n    //\n\n\n    const {\n      inputs,\n      attrs\n    } = kernelParams;\n    const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ? null : kernelParams.backwardsFunc;\n    let kernelProfile;\n    this.scopedRun( // Stop recording to a tape when running a kernel.\n    () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n        outputs = kernelFunc();\n      } else {\n        kernelProfile = this.profiler.profileKernel(kernelOrScopeName, inputs, () => kernelFunc());\n\n        if (this.ENV.getBool('DEBUG')) {\n          this.profiler.logKernelProfile(kernelProfile);\n        }\n\n        outputs = kernelProfile.outputs;\n      }\n    });\n\n    if (isTapeOn) {\n      this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelOrScopeName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n\n    return Array.isArray(out) ? outputs : outputs[0];\n  }\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n\n\n  saveTensorsForBackwardMode(tensors) {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n\n\n  getTensorsForGradient(kernelName, inputs, outputs) {\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      const inputsToSave = gradConfig.inputsToSave || [];\n      const outputsToSave = gradConfig.outputsToSave || []; // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n\n      let inputTensorsToSave;\n\n      if (gradConfig.saveAllInputs) {\n        util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n        inputTensorsToSave = Object.keys(inputs).map(key => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map(inputName => inputs[inputName]);\n      }\n\n      const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    } // We return an empty list rather than throw an error because the kernel we\n    // are looking up may not actually be relevant to backproping through the\n    // overall function\n    //\n    // See 'does not error if irrelevant (pruned) ops are missing grads' test\n    // in gradients_test.ts for an example.\n\n\n    return [];\n  }\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n\n\n  makeTensor(values, shape, dtype, backend) {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values;\n\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = values.map(d => util.encodeString(d));\n    }\n\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend); // Count bytes for string tensors.\n\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n\n    return t;\n  }\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   */\n\n\n  makeTensorFromDataId(dataId, shape, dtype, backend) {\n    dtype = dtype || 'float32';\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n    return t;\n  }\n\n  makeVariable(initialValue) {\n    let trainable = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n    let name = arguments.length > 2 ? arguments[2] : undefined;\n    let dtype = arguments.length > 3 ? arguments[3] : undefined;\n    name = name || this.nextVariableId().toString();\n\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  trackTensor(a, backend) {\n    this.state.numTensors++;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    } // Bytes for complex numbers are counted by their components. Bytes for\n    // string tensors are counted when writing values.\n\n\n    let bytes = 0;\n\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      bytes = a.size * util.bytesPerElement(a.dtype);\n    }\n\n    this.state.numBytes += bytes;\n\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      this.state.numDataBuffers++;\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes\n      });\n    }\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  } // Track the tensor by dataId and increase the refCount for the dataId in the\n  // backend.\n  // TODO(pyu10055): This is currently used by makeVariable method, to increase\n  // refCount on the backend for the dataId. It can potentially be replaced with\n  // Identity op indead of calling backend directly.\n\n\n  incRef(a, backend) {\n    this.trackTensor(a, backend);\n    this.backend.incRef(a.dataId);\n  }\n\n  removeDataId(dataId, backend) {\n    if (this.state.tensorInfo.has(dataId) && this.state.tensorInfo.get(dataId).backend === backend) {\n      this.state.tensorInfo.delete(dataId);\n      this.state.numDataBuffers--;\n    }\n  }\n\n  disposeTensor(a) {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n\n    const info = this.state.tensorInfo.get(a.dataId);\n    this.state.numTensors--;\n\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n      this.state.numBytes -= info.bytes;\n    } // Don't count bytes for complex numbers as they are counted by their\n    // components.\n\n\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      const bytes = a.size * util.bytesPerElement(a.dtype);\n      this.state.numBytes -= bytes;\n    } // Remove the reference to dataId if backend dispose the data successfully\n\n\n    if (info.backend.disposeData(a.dataId)) {\n      this.removeDataId(a.dataId, info.backend);\n    } // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n\n  }\n\n  disposeVariables() {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v) {\n    this.disposeTensor(v);\n\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory() {\n    const info = this.backend.memory();\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n\n      info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n    }\n\n    return info;\n  }\n\n  async profile(query) {\n    this.state.profiling = true;\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n    this.state.profiling = false;\n    this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;\n\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n\n    return this.state.activeProfile;\n  }\n\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n    const tapeNode = {\n      id: this.state.nextTapeNodeId++,\n      kernelName,\n      inputs,\n      outputs,\n      saved\n    };\n    const gradConfig = getGradient(kernelName);\n\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n\n    if (gradientsFunc != null) {\n      tapeNode.gradient = dys => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n\n          return dy;\n        }); // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep(result) {\n    result.kept = true;\n    return result;\n  }\n\n  startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n\n    this.state.gradientDepth++;\n  }\n\n  endTape() {\n    this.state.gradientDepth--;\n  }\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  startScope(name) {\n    const scopeInfo = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n\n    if (name) {\n      scopeInfo.name = name;\n    }\n\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n\n\n  endScope(result) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id)); // Dispose the arrays tracked in this scope.\n\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1]; // Track the current result in the parent scope.\n\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n\n\n  gradients(f, xs, dy) {\n    let allowNoGradients = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n    util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n    util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.'); // Filter out the nodes that don't connect x => y.\n\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap = {};\n      accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy; // Backprop gradients through the filtered nodes.\n\n      backpropagateGradients(accumulatedGradientMap, filteredTape, // Pass the tidy function to avoid circular dep with `tape.ts`.\n      f => this.tidy(f), // Pass an add function to avoide a circular dep with `tape.ts`.\n      add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n\n      return {\n        value: y,\n        grads\n      };\n    });\n  }\n\n  customGrad(f) {\n    var _this = this;\n\n    util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n    return function () {\n      for (var _len = arguments.length, inputs = new Array(_len), _key = 0; _key < _len; _key++) {\n        inputs[_key] = arguments[_key];\n      }\n\n      util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors');\n      let res;\n      const inputMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n\n      const forwardFunc = (_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor');\n        util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.');\n        return res.value;\n      };\n\n      const backwardsFunc = (dy, saved) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).');\n        util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.');\n        const gradMap = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      };\n\n      return _this.runKernelFunc({\n        forwardFunc,\n        backwardsFunc,\n        inputs: inputMap\n      });\n    };\n  }\n\n  readSync(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n\n  read(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  readToGPU(dataId, options) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readToGPU(dataId, options);\n  }\n\n  async time(query) {\n    const start = now();\n    const timingInfo = await this.backend.time(query);\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n\n\n  track(result) {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n\n\n  reset() {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\n\nfunction ones(shape) {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine() {\n  const ns = getGlobalNamespace();\n\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n\n  setEnvironmentGlobal(ns._tfengine.ENV); // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\n\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {\n    a,\n    b\n  };\n  return ENGINE.runKernel(Add, inputs);\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsCA,aAAtC,QAA0D,oBAA1D;AACA,SAAQC,WAAR,EAAqBC,oBAArB,QAAgD,eAAhD;AACA,SAAQC,kBAAR,QAAiC,eAAjC;AACA,SAAQC,GAAR,EAAaC,IAAb,EAAmBC,QAAnB,QAAkC,gBAAlC;AACA,SAAQC,WAAR,EAAqBC,SAArB,EAAgCC,oBAAhC,QAA+F,mBAA/F;AACA,OAAO,KAAKC,GAAZ,MAAqB,OAArB;AACA,SAAuBC,QAAvB,QAAsC,YAAtC;AACA,SAAQC,sBAAR,EAAgCC,oBAAhC,QAAqE,QAArE;AACA,SAA2CC,gBAA3C,EAA6DC,MAA7D,EAAoFC,QAApF,QAAmG,UAAnG;AAEA,SAAQC,qBAAR,QAAoC,eAApC;AAEA,OAAO,KAAKC,IAAZ,MAAsB,QAAtB;AACA,SAAQC,oBAAR,EAA8BC,kBAA9B,EAAkDC,GAAlD,EAAuDC,aAAvD,QAA2E,QAA3E;;AAuEA,SAASC,4BAAT,CAEIC,gBAFJ,EAGoC;AAElC,SAAQA,gBAAkD,CAACC,UAAnD,IAAiE,IAAzE;AACD;;AAED,MAAMC,WAAN,CAAiB;AAAjBC;AACE;AACA,+BAAwC,EAAxC;AAEA,0BAAiB,CAAjB;AACA,oBAAW,CAAX;AACA,sBAAa,CAAb;AACA,4BAAmB,CAAnB;AACA,0BAAiB,CAAjB,CARF,CAWE;AACA;AACA;;AACA,yBAAgB,CAAhB,CAdF,CAeE;AACA;;AACA,uBAAc,CAAd;AAIA,sBAA2B,EAA3B;AACA;;;;;AAIA,6BAA8B,EAA9B;AACA,uBAAc,CAAd;AAEA,sBAAa,IAAIC,OAAJ,EAAb;AAOA,qBAAY,KAAZ;AACA,yBAA6B;AAC3BC,cAAQ,EAAE,CADiB;AAE3BC,gBAAU,EAAE,CAFe;AAG3BC,eAAS,EAAE,CAHgB;AAI3BC,aAAO,EAAE,EAJkB;AAK3BC,YAAM,EAAE,IALmB;;AAM3B,UAAIC,WAAJ,GAAe;AAET,eAAOC,KAAK,CAACC,IAAN,CAAW,IAAIC,GAAJ,CAAQ,KAAKL,OAAL,CAAaM,GAAb,CAAiBC,CAAC,IAAIA,CAAC,CAACC,IAAxB,CAAR,CAAX,CAAP;AACD;;AATsB,KAA7B;AAiBD;;AALCC,SAAO;AACL,SAAK,MAAMC,YAAX,IAA2B,KAAKC,mBAAhC,EAAqD;AACnD,WAAKA,mBAAL,CAAyBD,YAAzB,EAAuCD,OAAvC;AACD;AACF;;AArDc;;AAwDjB,OAAM,MAAOG,MAAP,CAAa;AAgBjBjB,cAAmBkB,GAAnB,EAAmC;AAAhB;AAbnB,oBAA0C,EAA1C;AACA,2BAKI,EALJ;AAUQ,gCAAuB,CAAvB;AAGN,SAAKC,KAAL,GAAa,IAAIpB,WAAJ,EAAb;AACD;;AAEU,QAALqB,KAAK;AACT,QAAI,KAAKC,kBAAL,IAA2B,IAA/B,EAAqC;AACnC,aAAO,KAAKA,kBAAL,CAAwBC,IAAxB,CAA6B,MAAK,CAAG,CAArC,CAAP;AACD;;AACD,QAAI,KAAKC,eAAL,IAAwB,IAA5B,EAAkC;AAChC;AACD;;AACD,UAAMC,cAAc,GAAG,KAAKC,iBAAL,EAAvB;;AAEA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGF,cAAc,CAACG,MAAnC,EAA2CD,CAAC,EAA5C,EAAgD;AAC9C,YAAME,WAAW,GAAGJ,cAAc,CAACE,CAAD,CAAlC;AACA,YAAMG,OAAO,GAAG,MAAM,KAAKC,iBAAL,CAAuBF,WAAvB,EAAoCC,OAA1D;;AACA,UAAIA,OAAJ,EAAa;AACX,cAAM,KAAKE,UAAL,CAAgBH,WAAhB,CAAN;AACA;AACD;AACF;;AAED,UAAM,IAAII,KAAJ,CACF,oEACA,SAFE,CAAN;AAGD;;AAEU,MAAPC,OAAO;AACT,QAAI,KAAKZ,kBAAL,IAA2B,IAA/B,EAAqC;AACnC,YAAM,IAAIW,KAAJ,CACF,YAAY,KAAKJ,WAAW,uCAA5B,GACA,mEADA,GAEA,eAHE,CAAN;AAID;;AACD,QAAI,KAAKL,eAAL,IAAwB,IAA5B,EAAkC;AAChC,YAAM;AAACV,YAAD;AAAOqB;AAAP,UAAoB,KAAKC,+BAAL,EAA1B;;AACA,UAAID,SAAJ,EAAe;AACb,cAAM,IAAIF,KAAJ,CACF,iCAAiCnB,IAAI,qBAArC,GACA,gDADA,GAEA,oDAHE,CAAN;AAID;;AACD,WAAKkB,UAAL,CAAgBlB,IAAhB;AACD;;AACD,WAAO,KAAKU,eAAZ;AACD;;AAEDa,cAAY;AACV,WAAOC,MAAM,CAACC,IAAP,CAAY,KAAKC,eAAjB,CAAP;AACD;;AAEDC,aAAW,CAACZ,WAAD,EAAoB;AAC7B,QAAI,EAAEA,WAAW,IAAI,KAAKa,QAAtB,CAAJ,EAAqC;AACnC;AACA;AACA,UAAIb,WAAW,IAAI,KAAKW,eAAxB,EAAyC;AACvC,cAAM;AAACL;AAAD,YAAc,KAAKJ,iBAAL,CAAuBF,WAAvB,CAApB;;AACA,YAAIM,SAAJ,EAAe;AACb;AACA,iBAAO,IAAP;AACD;AACF,OAND,MAMO;AACL,eAAO,IAAP;AACD;AACF;;AACD,WAAO,KAAKO,QAAL,CAAcb,WAAd,CAAP;AACD;;AAEDc,oBAAkB,CAACd,WAAD,EAAoB;AAEpC,QAAI,EAAEA,WAAW,IAAI,KAAKW,eAAtB,CAAJ,EAA4C;AAC1C,aAAO,IAAP;AACD;;AACD,WAAO,KAAKA,eAAL,CAAqBX,WAArB,EAAkCe,OAAzC;AACD;;AAEDC,iBAAe,CACXhB,WADW,EAEXe,OAFW,EAGC;AAAA,QAAZE,QAAY,uEAAD,CAAC;;AACd,QAAIjB,WAAW,IAAI,KAAKW,eAAxB,EAAyC;AACvCxD,SAAG,CAAC+D,IAAJ,CACI,GAAGlB,WAAW,mCAAd,GACA,mCAFJ;AAGA,aAAO,KAAP;AACD;;AACD,SAAKW,eAAL,CAAqBX,WAArB,IAAoC;AAACe,aAAD;AAAUE;AAAV,KAApC;AACA,WAAO,IAAP;AACD;;AAEe,QAAVd,UAAU,CAACH,WAAD,EAAoB;AAClC,QAAI,KAAKW,eAAL,CAAqBX,WAArB,KAAqC,IAAzC,EAA+C;AAC7C,YAAM,IAAII,KAAJ,CAAU,iBAAiBJ,WAAW,yBAAtC,CAAN;AACD;;AACD,SAAKA,WAAL,GAAmBA,WAAnB;;AACA,QAAI,KAAKa,QAAL,CAAcb,WAAd,KAA8B,IAAlC,EAAwC;AACtC,WAAKL,eAAL,GAAuB,IAAvB;AACA,YAAM;AAACM,eAAD;AAAUK;AAAV,UAAuB,KAAKJ,iBAAL,CAAuBF,WAAvB,CAA7B;AACA,YAAMtB,MAAM,GAAG4B,SAAS,GAAG,MAAML,OAAT,GAAmBA,OAA3C;;AACA,UAAI,CAACvB,MAAL,EAAa;AACX,eAAO,KAAP;AACD;AACF;;AACD,SAAKiB,eAAL,GAAuB,KAAKkB,QAAL,CAAcb,WAAd,CAAvB;AACA,SAAKmB,sBAAL,GAdkC,CAelC;;AACA,SAAKC,QAAL,GAAgB,IAAIhE,QAAJ,CAAa,KAAKuC,eAAlB,CAAhB;AAEA,WAAO,IAAP;AACD;;AAEOwB,wBAAsB;AAC5B,UAAM1C,OAAO,GAAGvB,oBAAoB,CAAC,KAAK8C,WAAN,CAApC;AACAvB,WAAO,CAAC4C,OAAR,CAAgBC,MAAM,IAAG;AACvB,UAAIA,MAAM,CAACC,SAAP,IAAoB,IAAxB,EAA8B;AAC5BD,cAAM,CAACC,SAAP,CAAiB,KAAK5B,eAAtB;AACD;AACF,KAJD;AAKD;;AAEO6B,0BAAwB,CAACxB,WAAD,EAAoB;AAClD,UAAMvB,OAAO,GAAGvB,oBAAoB,CAAC8C,WAAD,CAApC;AACAvB,WAAO,CAAC4C,OAAR,CAAgBC,MAAM,IAAG;AACvB,UAAIA,MAAM,CAACG,WAAP,IAAsB,IAA1B,EAAgC;AAC9BH,cAAM,CAACG,WAAP,CAAmB,KAAKZ,QAAL,CAAcb,WAAd,CAAnB;AACD;AACF,KAJD;AAKD;AAED;;;;;;;;AAMQE,mBAAiB,CAACF,WAAD,EAAoB;AAE3C,UAAM0B,oBAAoB,GAAG,KAAKf,eAAL,CAAqBX,WAArB,CAA7B;;AACA,QAAI0B,oBAAoB,IAAI,IAA5B,EAAkC;AAChC,YAAM,IAAItB,KAAJ,CACF,6BAA6BJ,WAAW,0BADtC,CAAN;AAED;;AAED,QAAI;AACF,YAAMK,OAAO,GAAGqB,oBAAoB,CAACX,OAArB,EAAhB;AACA;;;;;;AAKA,UAAIV,OAAO,IAAI,EAAEA,OAAO,YAAY5D,aAArB,CAAX,IACA,OAAO4D,OAAO,CAACX,IAAf,KAAwB,UAD5B,EACwC;AACtC,cAAMiC,SAAS,GAAG,EAAE,KAAKC,oBAAzB;AACA,cAAM3B,OAAO,GACTI,OAAO,CACFX,IADL,CACUC,eAAe,IAAG;AACtB;AACA,cAAIgC,SAAS,GAAG,KAAKC,oBAArB,EAA2C;AACzC,mBAAO,KAAP;AACD;;AACD,eAAKf,QAAL,CAAcb,WAAd,IAA6BL,eAA7B;AACA,eAAKF,kBAAL,GAA0B,IAA1B;AACA,iBAAO,IAAP;AACD,SATL,EAUKoC,KAVL,CAUWC,GAAG,IAAG;AACX;AACA,cAAIH,SAAS,GAAG,KAAKC,oBAArB,EAA2C;AACzC,mBAAO,KAAP;AACD;;AACD,eAAKnC,kBAAL,GAA0B,IAA1B;AACAtC,aAAG,CAAC+D,IAAJ,CAAS,6BAA6BlB,WAAW,SAAjD;AACA7C,aAAG,CAAC+D,IAAJ,CAASY,GAAG,CAACC,KAAJ,IAAaD,GAAG,CAACE,OAA1B;AACA,iBAAO,KAAP;AACD,SAnBL,CADJ;AAqBA,aAAKvC,kBAAL,GAA0BQ,OAA1B;AACA,eAAO;AAACA,iBAAD;AAAUK,mBAAS,EAAE;AAArB,SAAP;AACD,OA1BD,MA0BO;AACL,aAAKO,QAAL,CAAcb,WAAd,IAA6BK,OAA7B;AACA,eAAO;AAACJ,iBAAO,EAAE,IAAV;AAAgBK,mBAAS,EAAE;AAA3B,SAAP;AACD;AACF,KArCD,CAqCE,OAAOwB,GAAP,EAAY;AACZ3E,SAAG,CAAC+D,IAAJ,CAAS,6BAA6BlB,WAAW,SAAjD;AACA7C,SAAG,CAAC+D,IAAJ,CAASY,GAAG,CAACC,KAAJ,IAAaD,GAAG,CAACE,OAA1B;AACA,aAAO;AAAC/B,eAAO,EAAE,KAAV;AAAiBK,iBAAS,EAAE;AAA5B,OAAP;AACD;AACF;;AAED2B,eAAa,CAACjC,WAAD,EAAoB;AAC/B,QAAI,EAAEA,WAAW,IAAI,KAAKW,eAAtB,CAAJ,EAA4C;AAC1C,YAAM,IAAIP,KAAJ,CAAU,GAAGJ,WAAW,gCAAxB,CAAN;AACD;;AACD,QAAI,KAAKA,WAAL,KAAqBA,WAArB,IAAoC,KAAKP,kBAAL,IAA2B,IAAnE,EAAyE;AACvE;AACA;AACA,WAAKmC,oBAAL;AACD;;AAED,QAAI5B,WAAW,IAAI,KAAKa,QAAxB,EAAkC;AAChC,WAAKW,wBAAL,CAA8BxB,WAA9B;AACA,WAAKa,QAAL,CAAcb,WAAd,EAA2Bd,OAA3B;AACA,aAAO,KAAK2B,QAAL,CAAcb,WAAd,CAAP;AACD;;AAED,WAAO,KAAKW,eAAL,CAAqBX,WAArB,CAAP,CAhB+B,CAkB/B;;AACA,QAAI,KAAKA,WAAL,KAAqBA,WAAzB,EAAsC;AACpC,WAAKP,kBAAL,GAA0B,IAA1B;AACA,WAAKO,WAAL,GAAmB,IAAnB;AACA,WAAKL,eAAL,GAAuB,IAAvB;AACD;AACF;;AAEOE,mBAAiB;AACvB,QAAIY,MAAM,CAACC,IAAP,CAAY,KAAKC,eAAjB,EAAkCZ,MAAlC,KAA6C,CAAjD,EAAoD;AAClD,YAAM,IAAIK,KAAJ,CAAU,+BAAV,CAAN;AACD;;AACD,WAAOK,MAAM,CAACC,IAAP,CAAY,KAAKC,eAAjB,EAAkCuB,IAAlC,CAAuC,CAACC,CAAD,EAAYC,CAAZ,KAAyB;AACrE;AACA,aAAO,KAAKzB,eAAL,CAAqByB,CAArB,EAAwBnB,QAAxB,GACH,KAAKN,eAAL,CAAqBwB,CAArB,EAAwBlB,QAD5B;AAED,KAJM,CAAP;AAKD;;AAEOV,iCAA+B;AAErC,UAAMX,cAAc,GAAG,KAAKC,iBAAL,EAAvB;;AAEA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGF,cAAc,CAACG,MAAnC,EAA2CD,CAAC,EAA5C,EAAgD;AAC9C,YAAME,WAAW,GAAGJ,cAAc,CAACE,CAAD,CAAlC;AACA,YAAM;AAACG,eAAD;AAAUK;AAAV,UAAuB,KAAKJ,iBAAL,CAAuBF,WAAvB,CAA7B;;AACA,UAAIM,SAAS,IAAIL,OAAjB,EAA0B;AACxB,eAAO;AAAChB,cAAI,EAAEe,WAAP;AAAoBM;AAApB,SAAP;AACD;AACF;;AACD,UAAM,IAAIF,KAAJ,CACF,oEACA,SAFE,CAAN;AAGD;;AAEDiC,UAAQ,CAAChC,OAAD,EAAyBiC,MAAzB,EAAuC;AAC7C,UAAMC,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,CAAb;AACA,UAAMI,UAAU,GAAGH,IAAI,CAAClC,OAAxB;AACA,UAAMsC,MAAM,GAAG,KAAKC,QAAL,CAAcN,MAAd,CAAf;AACA,UAAMO,QAAQ,GAAGH,UAAU,CAACG,QAAX,CAAoBP,MAApB,CAAjB,CAJ6C,CAK7C;AACA;;AACAI,cAAU,CAACI,WAAX,CAAuBR,MAAvB,EAA+B,IAA/B;AACAC,QAAI,CAAClC,OAAL,GAAeA,OAAf;AACAA,WAAO,CAAC0C,IAAR,CAAaT,MAAb,EAAqBK,MAArB,EAA6BJ,IAAI,CAACS,KAAlC,EAAyCT,IAAI,CAACU,KAA9C,EAAqDJ,QAArD;;AACA,QAAI,KAAKK,sBAAL,EAAJ,EAAmC;AACjC;AACA;AACA,WAAK3D,KAAL,CAAW4D,iBAAX,CAA6B,KAAK5D,KAAL,CAAW4D,iBAAX,CAA6BpD,MAA7B,GAAsC,CAAnE;AACD;AACF;;AAEDqD,MAAI,CAA4BC,QAA5B,EAAyDC,EAAzD,EAAwE;AAE1E,QAAIrE,IAAI,GAAW,IAAnB;;AACA,QAAIqE,EAAE,IAAI,IAAV,EAAgB;AACd;AACA,UAAI,OAAOD,QAAP,KAAoB,UAAxB,EAAoC;AAClC,cAAM,IAAIjD,KAAJ,CAAU,qCAAV,CAAN;AACD;;AACDkD,QAAE,GAAGD,QAAL;AACD,KAND,MAMO;AACL;AACA,UAAI,OAAOA,QAAP,KAAoB,QAApB,IAAgC,EAAEA,QAAQ,YAAYE,MAAtB,CAApC,EAAmE;AACjE,cAAM,IAAInD,KAAJ,CACF,yDACA,4BAFE,CAAN;AAGD;;AACD,UAAI,OAAOkD,EAAP,KAAc,UAAlB,EAA8B;AAC5B,cAAM,IAAIlD,KAAJ,CACF,uDACA,8BAFE,CAAN;AAGD;;AACDnB,UAAI,GAAGoE,QAAP,CAZK,CAaL;AACA;AACD;;AACD,QAAI3E,MAAJ;AACA,WAAO,KAAK8E,SAAL,CACH,MAAM,KAAKC,UAAL,CAAgBxE,IAAhB,CADH,EAC0B,MAAM,KAAKyE,QAAL,CAAchF,MAAd,CADhC,EACuD,MAAK;AAC7DA,YAAM,GAAG4E,EAAE,EAAX;;AACA,UAAI5E,MAAM,YAAYiF,OAAtB,EAA+B;AAC7BC,eAAO,CAACC,KAAR,CAAc,yCAAd;AACD;;AACD,aAAOnF,MAAP;AACD,KAPE,CAAP;AAQD;;AAEO8E,WAAS,CAAIM,KAAJ,EAAuBC,GAAvB,EAAwCC,CAAxC,EAAkD;AACjEF,SAAK;;AACL,QAAI;AACF,YAAMG,GAAG,GAAGD,CAAC,EAAb;AACAD,SAAG;AACH,aAAOE,GAAP;AACD,KAJD,CAIE,OAAOC,EAAP,EAAW;AACXH,SAAG;AACH,YAAMG,EAAN;AACD;AACF;;AAGOC,cAAY;AAClB,WAAO9E,MAAM,CAAC8E,YAAP,EAAP;AACD;;AAGOC,gBAAc;AACpB,WAAO/E,MAAM,CAAC+E,cAAP,EAAP;AACD;AAED;;;;;;;;AAMQC,OAAK,CAACC,CAAD,EAAU;AACrB,UAAMC,CAAC,GAAWC,MAAM,CAACC,SAAP,CAAiB1H,QAAjB,EAA2B;AAACuH;AAAD,KAA3B,CAAlB;AACA,UAAMI,MAAM,GAAG;AAACJ;AAAD,KAAf;;AACA,UAAMK,IAAI,GAAIC,EAAD,KAAiB;AAC5BN,OAAC,EAAE,MAAK;AACN,cAAMrB,KAAK,GAAG,SAAd;AACA,cAAM4B,UAAU,GAAG;AAACP,WAAC,EAAEM;AAAJ,SAAnB;AACA,cAAME,KAAK,GAAG;AAAC7B;AAAD,SAAd;AAEA,eAAOuB,MAAM,CAACC,SAAP,CACI3H,IADJ,EACU+H,UADV,EAEI;AACAC,aAHJ,CAAP;AAID;AAV2B,KAAjB,CAAb;;AAYA,UAAMC,KAAK,GAAa,EAAxB;AACA,SAAKC,WAAL,CAAiB,KAAKzF,KAAL,CAAW0F,WAAX,CAAuBhG,IAAxC,EAA8CyF,MAA9C,EAAsD,CAACH,CAAD,CAAtD,EAA2DI,IAA3D,EAAiEI,KAAjE,EAAwE,EAAxE;AACA,WAAOR,CAAP;AACD;AAED;;;;;;;;;;;;;;;AAaAE,WAAS,CACLvG,UADK,EACewG,MADf,EACuCI,KADvC,EAC2D;AAClE,QAAI,KAAK9E,WAAL,IAAoB,IAAxB,EAA8B;AAC5B;AACA;AACA;AACA;AACA;AACA,WAAKK,OAAL;AACD;;AACD,UAAM6E,SAAS,GAAGjI,SAAS,CAACiB,UAAD,EAAa,KAAK8B,WAAlB,CAAT,IAA2C,IAA7D;;AACA,QAAI,CAACkF,SAAL,EAAgB;AACd,YAAM,IAAI9E,KAAJ,CAAU,WAAWlC,UAAU,iCACjC,KAAK8B,WAAW,GADd,CAAN;AAED;;AACD,WAAO,KAAKmF,aAAL,CAAmB;AAACjH,gBAAD;AAAawG,YAAb;AAAqBI;AAArB,KAAnB,CAAP;AACD;;AAEO5B,wBAAsB;AAC5B,WAAO,KAAK5D,GAAL,CAAS8F,OAAT,CAAiB,SAAjB,CAAP;AACD;;AAEOC,uBAAqB,CACzBnH,UADyB,EACLoH,gBADK,EAEzBC,QAFyB,EAEH;AACxB,UAAMC,eAAe,GAAG,KAAKnF,OAAL,CAAaoF,UAAb,EAAxB,CADwB,CAGxB;;AACA,QAAIC,gBAAgB,GAAG,CAAvB;AACAH,YAAQ,CAAClE,OAAT,CAAiBkB,IAAI,IAAG;AACtB;AACA;AACAmD,sBAAgB,IAAKnD,IAAI,CAACU,KAAL,KAAe,WAAf,GAA6B,CAA7B,GAAiC,CAAtD;AACD,KAJD,EALwB,CAWxB;AACA;AACA;AACA;AACA;;AACA,UAAM0C,QAAQ,GACV,KAAKpG,KAAL,CAAW4D,iBAAX,CAA6B,KAAK5D,KAAL,CAAW4D,iBAAX,CAA6BpD,MAA7B,GAAsC,CAAnE,CADJ;AAEA,UAAM6F,aAAa,GACfJ,eAAe,GAAGF,gBAAlB,GAAqCI,gBAArC,GAAwDC,QAD5D;;AAEA,QAAIC,aAAa,GAAG,CAApB,EAAuB;AACrB,YAAM,IAAIxF,KAAJ,CACF,YAAY,KAAKJ,WAAW,gCAA5B,GACA,IAAI4F,aAAa,6BAA6B1H,UAAU,GAFtD,CAAN;AAGD;AACF;AAED;;;;;;;AAKQiH,eAAa,CACjBU,YADiB,EAEe;AAClC,QAAIC,OAAJ;AACA,QAAIf,KAAK,GAAa,EAAtB;AACA,UAAMgB,QAAQ,GAAG,KAAKA,QAAL,EAAjB;AAEA,UAAMC,iBAAiB,GAAG,KAAKzG,KAAL,CAAW0G,QAArC;AACA,UAAMC,kBAAkB,GAAG,KAAK3G,KAAL,CAAW4G,UAAtC;;AAEA,QAAI,KAAKjD,sBAAL,EAAJ,EAAmC;AACjC,WAAK3D,KAAL,CAAW4D,iBAAX,CAA6BiD,IAA7B,CAAkC,CAAlC;AACD;;AAED,QAAIC,UAAJ;;AACA,QAAI,KAAKrG,WAAL,IAAoB,IAAxB,EAA8B;AAC5B;AACA;AACA;AACA;AACA;AACA,WAAKK,OAAL;AACD;;AAED,QAAIiG,GAAJ;AAEA,UAAMC,iBAAiB,GAAGvI,4BAA4B,CAAC6H,YAAD,CAA5B,GACtBA,YAAY,CAAC3H,UADS,GAEtB,KAAKqB,KAAL,CAAW0F,WAAX,IAA0B,IAA1B,GAAiC,KAAK1F,KAAL,CAAW0F,WAAX,CAAuBhG,IAAxD,GAA+D,EAFnE,CAxBkC,CA4BlC;AACA;AACA;;AAEA,QAAIjB,4BAA4B,CAAC6H,YAAD,CAAhC,EAAgD;AAC9C,YAAM;AAAC3H,kBAAD;AAAawG,cAAb;AAAqBI;AAArB,UAA8Be,YAApC;;AACA,UAAI,KAAK7F,WAAL,IAAoB,IAAxB,EAA8B;AAC5B;AACA;AACA;AACA;AACA;AACA,aAAKK,OAAL;AACD;;AACD,YAAMiB,MAAM,GAAGrE,SAAS,CAACiB,UAAD,EAAa,KAAK8B,WAAlB,CAAxB;AACArC,UAAI,CAAC6I,MAAL,CACIlF,MAAM,IAAI,IADd,EAEI,MAAM,kCAAkCpD,UAAU,kBAC9C,KAAK8B,WAAW,GAHxB;;AAKAqG,gBAAU,GAAG,MAAK;AAChB,cAAMf,gBAAgB,GAAG,KAAKjF,OAAL,CAAaoF,UAAb,EAAzB;AACAa,WAAG,GAAGhF,MAAM,CAAC+E,UAAP,CAAkB;AAAC3B,gBAAD;AAASI,eAAT;AAAgBzE,iBAAO,EAAE,KAAKA;AAA9B,SAAlB,CAAN;AACA,cAAMkF,QAAQ,GAAG3G,KAAK,CAAC6H,OAAN,CAAcH,GAAd,IAAqBA,GAArB,GAA2B,CAACA,GAAD,CAA5C;;AACA,YAAI,KAAKpD,sBAAL,EAAJ,EAAmC;AACjC,eAAKmC,qBAAL,CAA2BnH,UAA3B,EAAuCoH,gBAAvC,EAAyDC,QAAzD;AACD;;AAED,cAAMmB,UAAU,GAAGnB,QAAQ,CAACxG,GAAT,CAAc4H,OAAD,IAA+B;AAC7D;AACA;AACA;AACA,cAAKA,OAAkB,CAACC,IAAnB,IAA2B,IAAhC,EAAsC;AACpC,mBAAOD,OAAP;AACD;;AACD,gBAAM;AAACrE,kBAAD;AAASU,iBAAT;AAAgBC;AAAhB,cAAyB0D,OAA/B;AACA,iBAAO,KAAKE,oBAAL,CAA0BvE,MAA1B,EAAkCU,KAAlC,EAAyCC,KAAzC,CAAP;AACD,SATkB,CAAnB,CARgB,CAmBhB;AAEA;AACA;AACA;;AACA,YAAI8C,QAAJ,EAAc;AACZ,gBAAMe,aAAa,GACf,KAAKC,qBAAL,CAA2B7I,UAA3B,EAAuCwG,MAAvC,EAA+CgC,UAA/C,CADJ;AAEA3B,eAAK,GAAG,KAAKiC,0BAAL,CAAgCF,aAAhC,CAAR;AACD;;AACD,eAAOJ,UAAP;AACD,OA9BD;AA+BD,KA/CD,MA+CO;AACL,YAAM;AAACO;AAAD,UAAgBpB,YAAtB,CADK,CAEL;;AACA,YAAMqB,QAAQ,GAAkBC,OAAD,IAAY;AACzC;AACA;AACA;AACA,YAAI,CAACpB,QAAL,EAAe;AACb;AACD;;AACDhB,aAAK,GAAGoC,OAAO,CAACpI,GAAR,CAAYqI,MAAM,IAAI,KAAKC,IAAL,CAAU,KAAKhD,KAAL,CAAW+C,MAAX,CAAV,CAAtB,CAAR;AACD,OARD;;AAUAf,gBAAU,GAAG,MAAK;AAChB,cAAMf,gBAAgB,GAAG,KAAKjF,OAAL,CAAaoF,UAAb,EAAzB;AACAa,WAAG,GAAG,KAAKlD,IAAL,CAAU,MAAM6D,WAAW,CAAC,KAAK5G,OAAN,EAAe6G,QAAf,CAA3B,CAAN;AACA,cAAMI,IAAI,GAAI1I,KAAK,CAAC6H,OAAN,CAAcH,GAAd,IAAqBA,GAArB,GAA2B,CAACA,GAAD,CAAzC;;AACA,YAAI,KAAKpD,sBAAL,EAAJ,EAAmC;AACjC;AACA,eAAKmC,qBAAL,CAA2BkB,iBAA3B,EAA8CjB,gBAA9C,EAAgEgC,IAAhE;AACD;;AACD,eAAOA,IAAP;AACD,OATD;AAUD,KAtGiC,CAwGlC;AACA;AACA;;;AACA,UAAM;AAAC5C,YAAD;AAASI;AAAT,QAAkBe,YAAxB;AACA,UAAM0B,aAAa,GAAGvJ,4BAA4B,CAAC6H,YAAD,CAA5B,GAClB,IADkB,GAElBA,YAAY,CAAC0B,aAFjB;AAIA,QAAIC,aAAJ;AACA,SAAKhE,SAAL,EACI;AACA,UAAM,KAAKjE,KAAL,CAAWkI,WAAX,EAFV,EAEoC,MAAM,KAAKlI,KAAL,CAAWkI,WAAX,EAF1C,EAEoE,MAAK;AACnE,UAAI,CAAC,KAAKnI,GAAL,CAAS8F,OAAT,CAAiB,OAAjB,CAAD,IAA8B,CAAC,KAAK7F,KAAL,CAAWmI,SAA9C,EAAyD;AACvD5B,eAAO,GAAGO,UAAU,EAApB;AACD,OAFD,MAEO;AACLmB,qBAAa,GAAG,KAAKpG,QAAL,CAAcuG,aAAd,CACZpB,iBADY,EACO7B,MADP,EACe,MAAM2B,UAAU,EAD/B,CAAhB;;AAEA,YAAI,KAAK/G,GAAL,CAAS8F,OAAT,CAAiB,OAAjB,CAAJ,EAA+B;AAC7B,eAAKhE,QAAL,CAAcwG,gBAAd,CAA+BJ,aAA/B;AACD;;AACD1B,eAAO,GAAG0B,aAAa,CAAC1B,OAAxB;AACD;AACF,KAbL;;AAeA,QAAIC,QAAJ,EAAc;AACZ,WAAKf,WAAL,CACIuB,iBADJ,EACuB7B,MADvB,EAC+BoB,OAD/B,EACwCyB,aADxC,EACuDxC,KADvD,EAC8DD,KAD9D;AAED;;AAED,QAAI,KAAKvF,KAAL,CAAWmI,SAAf,EAA0B;AACxB,WAAKnI,KAAL,CAAWsI,aAAX,CAAyBpJ,OAAzB,CAAiC2H,IAAjC,CAAsC;AACpCnH,YAAI,EAAEsH,iBAD8B;AAEpCuB,kBAAU,EAAE,KAAKvI,KAAL,CAAW0G,QAAX,GAAsBD,iBAFE;AAGpC+B,0BAAkB,EAAE,KAAKxI,KAAL,CAAW0G,QAHK;AAIpC+B,oBAAY,EAAE,KAAKzI,KAAL,CAAW4G,UAAX,GAAwBD,kBAJF;AAKpC+B,4BAAoB,EAAE,KAAK1I,KAAL,CAAW4G,UALG;AAMpC+B,mBAAW,EAAEzH,MAAM,CAACC,IAAP,CAAYgE,MAAZ,EAAoB3F,GAApB,CACToJ,GAAG,IAAIzD,MAAM,CAACyD,GAAD,CAAN,IAAe,IAAf,GAAsBzD,MAAM,CAACyD,GAAD,CAAN,CAAYnF,KAAlC,GAA0C,IADxC,CANuB;AAQpCoF,oBAAY,EAAEtC,OAAO,CAAC/G,GAAR,CAAYsJ,IAAI,IAAIA,IAAI,CAACrF,KAAzB,CARsB;AASpCsF,oBAAY,EAAEd,aAAa,CAACe,MATQ;AAUpCC,iBAAS,EAAEhB,aAAa,CAACgB;AAVW,OAAtC;AAYD;;AACD,WAAQ5J,KAAK,CAAC6H,OAAN,CAAcH,GAAd,IAAqBR,OAArB,GAA+BA,OAAO,CAAC,CAAD,CAA9C;AACD;AAED;;;;;;;AAKQkB,4BAA0B,CAACG,OAAD,EAAkB;AAClD,UAAMpC,KAAK,GAAGoC,OAAO,CAACpI,GAAR,CAAYqI,MAAM,IAAI,KAAKC,IAAL,CAAU,KAAKhD,KAAL,CAAW+C,MAAX,CAAV,CAAtB,CAAd;AACA,WAAOrC,KAAP;AACD;AAED;;;;;;;;;AAOQgC,uBAAqB,CACzB7I,UADyB,EACLwG,MADK,EAEzBoB,OAFyB,EAER;AACnB,UAAM2C,UAAU,GAAGzL,WAAW,CAACkB,UAAD,CAA9B;;AACA,QAAIuK,UAAU,IAAI,IAAlB,EAAwB;AACtB,YAAMC,YAAY,GAAaD,UAAU,CAACC,YAAX,IAA2B,EAA1D;AACA,YAAMC,aAAa,GAAcF,UAAU,CAACE,aAAX,IAA4B,EAA7D,CAFsB,CAItB;AACA;;AACA,UAAIC,kBAAJ;;AACA,UAAIH,UAAU,CAACI,aAAf,EAA8B;AAC5BlL,YAAI,CAAC6I,MAAL,CACI5H,KAAK,CAAC6H,OAAN,CAAc/B,MAAd,CADJ,EAEI,MAAM,wDAFV;AAIAkE,0BAAkB,GAAGnI,MAAM,CAACC,IAAP,CAAYgE,MAAZ,EAAoB3F,GAApB,CAAyBoJ,GAAD,IAASzD,MAAM,CAACyD,GAAD,CAAvC,CAArB;AACD,OAND,MAMO;AACLS,0BAAkB,GAAGF,YAAY,CAAC3J,GAAb,CAAkB+J,SAAD,IAAepE,MAAM,CAACoE,SAAD,CAAtC,CAArB;AACD;;AAED,YAAMC,mBAAmB,GACrBjD,OAAO,CAACkD,MAAR,CAAe,CAACC,CAAD,EAAInJ,CAAJ,KAAU6I,aAAa,CAAC7I,CAAD,CAAtC,CADJ;AAGA,aAAO8I,kBAAkB,CAACM,MAAnB,CAA0BH,mBAA1B,CAAP;AACD,KAvBkB,CAwBnB;AACA;AACA;AACA;AACA;AACA;;;AACA,WAAO,EAAP;AACD;AAED;;;;;;;AAKAI,YAAU,CACNxG,MADM,EACcK,KADd,EAC+BC,KAD/B,EAEN5C,OAFM,EAEiB;AACzB,QAAIsC,MAAM,IAAI,IAAd,EAAoB;AAClB,YAAM,IAAIvC,KAAJ,CAAU,+CAAV,CAAN;AACD;;AACD6C,SAAK,GAAGA,KAAK,IAAI,SAAjB;AACA5C,WAAO,GAAGA,OAAO,IAAI,KAAKA,OAA1B;AACA,QAAI+I,WAAW,GAAGzG,MAAlB;;AACA,QAAIM,KAAK,KAAK,QAAV,IAAsBtF,IAAI,CAAC0L,QAAL,CAAc1G,MAAM,CAAC,CAAD,CAApB,CAA1B,EAAoD;AAClDyG,iBAAW,GAAIzG,MAAmB,CAAC5D,GAApB,CAAwBuK,CAAC,IAAI3L,IAAI,CAAC4L,YAAL,CAAkBD,CAAlB,CAA7B,CAAf;AACD;;AACD,UAAMhH,MAAM,GAAGjC,OAAO,CAACmJ,KAAR,CAAcJ,WAAd,EAA2BpG,KAA3B,EAAkCC,KAAlC,CAAf;AACA,UAAMwG,CAAC,GAAG,IAAIjM,MAAJ,CAAWwF,KAAX,EAAkBC,KAAlB,EAAyBX,MAAzB,EAAiC,KAAK6B,YAAL,EAAjC,CAAV;AACA,SAAKuF,WAAL,CAAiBD,CAAjB,EAAoBpJ,OAApB,EAZyB,CAczB;;AACA,QAAI4C,KAAK,KAAK,QAAd,EAAwB;AACtB,YAAMV,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,CAAb;AACA,YAAMhE,QAAQ,GAAGV,oBAAoB,CAACwL,WAAD,CAArC;AACA,WAAK7J,KAAL,CAAW0G,QAAX,IAAuB3H,QAAQ,GAAGiE,IAAI,CAACoH,KAAvC;AACApH,UAAI,CAACoH,KAAL,GAAarL,QAAb;AACD;;AACD,WAAOmL,CAAP;AACD;AAED;;;;;;;AAKA5C,sBAAoB,CAChBvE,MADgB,EACAU,KADA,EACiBC,KADjB,EAEhB5C,OAFgB,EAEO;AACzB4C,SAAK,GAAGA,KAAK,IAAI,SAAjB;AACA,UAAMwG,CAAC,GAAG,IAAIjM,MAAJ,CAAWwF,KAAX,EAAkBC,KAAlB,EAAyBX,MAAzB,EAAiC,KAAK6B,YAAL,EAAjC,CAAV;AACA,SAAKuF,WAAL,CAAiBD,CAAjB,EAAoBpJ,OAApB;AACA,WAAOoJ,CAAP;AACD;;AAEDG,cAAY,CACRC,YADQ,EAEQ;AAAA,QADMC,SACN,uEADkB,IAClB;AAAA,QADwB7K,IACxB;AAAA,QAAhBgE,KAAgB;AAClBhE,QAAI,GAAGA,IAAI,IAAI,KAAKmF,cAAL,GAAsB2F,QAAtB,EAAf;;AACA,QAAI9G,KAAK,IAAI,IAAT,IAAiBA,KAAK,KAAK4G,YAAY,CAAC5G,KAA5C,EAAmD;AACjD4G,kBAAY,GAAGA,YAAY,CAACG,IAAb,CAAkB/G,KAAlB,CAAf;AACD;;AACD,UAAMgH,CAAC,GAAG,IAAIxM,QAAJ,CAAaoM,YAAb,EAA2BC,SAA3B,EAAsC7K,IAAtC,EAA4C,KAAKkF,YAAL,EAA5C,CAAV;;AACA,QAAI,KAAK5E,KAAL,CAAWH,mBAAX,CAA+B6K,CAAC,CAAChL,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,YAAM,IAAImB,KAAJ,CAAU,sBAAsB6J,CAAC,CAAChL,IAAI,yBAAtC,CAAN;AACD;;AACD,SAAKM,KAAL,CAAWH,mBAAX,CAA+B6K,CAAC,CAAChL,IAAjC,IAAyCgL,CAAzC;AACA,SAAKC,MAAL,CAAYD,CAAZ,EAAe,KAAK5J,OAApB;AACA,WAAO4J,CAAP;AACD;;AAEDP,aAAW,CAACvH,CAAD,EAAY9B,OAAZ,EAAkC;AAC3C,SAAKd,KAAL,CAAW4G,UAAX;;AACA,QAAIhE,CAAC,CAACc,KAAF,KAAY,QAAhB,EAA0B;AACxB,WAAK1D,KAAL,CAAW4K,gBAAX;AACD,KAJ0C,CAK3C;AACA;;;AACA,QAAIR,KAAK,GAAG,CAAZ;;AACA,QAAIxH,CAAC,CAACc,KAAF,KAAY,WAAZ,IAA2Bd,CAAC,CAACc,KAAF,KAAY,QAA3C,EAAqD;AACnD0G,WAAK,GAAGxH,CAAC,CAACiI,IAAF,GAASzM,IAAI,CAAC0M,eAAL,CAAqBlI,CAAC,CAACc,KAAvB,CAAjB;AACD;;AACD,SAAK1D,KAAL,CAAW0G,QAAX,IAAuB0D,KAAvB;;AAEA,QAAI,CAAC,KAAKpK,KAAL,CAAWiD,UAAX,CAAsB8H,GAAtB,CAA0BnI,CAAC,CAACG,MAA5B,CAAL,EAA0C;AACxC,WAAK/C,KAAL,CAAWgL,cAAX;AACA,WAAKhL,KAAL,CAAWiD,UAAX,CAAsBgI,GAAtB,CAA0BrI,CAAC,CAACG,MAA5B,EAAoC;AAClCjC,eAAO,EAAEA,OAAO,IAAI,KAAKA,OADS;AAElC4C,aAAK,EAAEd,CAAC,CAACc,KAFyB;AAGlCD,aAAK,EAAEb,CAAC,CAACa,KAHyB;AAIlC2G;AAJkC,OAApC;AAMD;;AAED,QAAI,EAAExH,CAAC,YAAY1E,QAAf,CAAJ,EAA8B;AAC5B,WAAKgN,KAAL,CAAWtI,CAAX;AACD;AACF,GA7sBgB,CA+sBjB;AACA;AACA;AACA;AACA;;;AACA+H,QAAM,CAAC/H,CAAD,EAAY9B,OAAZ,EAAkC;AACtC,SAAKqJ,WAAL,CAAiBvH,CAAjB,EAAoB9B,OAApB;AACA,SAAKA,OAAL,CAAa6J,MAAb,CAAoB/H,CAAC,CAACG,MAAtB;AACD;;AAEDoI,cAAY,CAACpI,MAAD,EAAiBjC,OAAjB,EAAuC;AACjD,QAAI,KAAKd,KAAL,CAAWiD,UAAX,CAAsB8H,GAAtB,CAA0BhI,MAA1B,KACA,KAAK/C,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,EAAkCjC,OAAlC,KAA8CA,OADlD,EAC2D;AACzD,WAAKd,KAAL,CAAWiD,UAAX,CAAsBmI,MAAtB,CAA6BrI,MAA7B;AACA,WAAK/C,KAAL,CAAWgL,cAAX;AACD;AACF;;AACDK,eAAa,CAACzI,CAAD,EAAU;AACrB,QAAI,CAAC,KAAK5C,KAAL,CAAWiD,UAAX,CAAsB8H,GAAtB,CAA0BnI,CAAC,CAACG,MAA5B,CAAL,EAA0C;AACxC;AACD;;AACD,UAAMC,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BN,CAAC,CAACG,MAA5B,CAAb;AAEA,SAAK/C,KAAL,CAAW4G,UAAX;;AACA,QAAIhE,CAAC,CAACc,KAAF,KAAY,QAAhB,EAA0B;AACxB,WAAK1D,KAAL,CAAW4K,gBAAX;AACA,WAAK5K,KAAL,CAAW0G,QAAX,IAAuB1D,IAAI,CAACoH,KAA5B;AACD,KAVoB,CAWrB;AACA;;;AACA,QAAIxH,CAAC,CAACc,KAAF,KAAY,WAAZ,IAA2Bd,CAAC,CAACc,KAAF,KAAY,QAA3C,EAAqD;AACnD,YAAM0G,KAAK,GAAGxH,CAAC,CAACiI,IAAF,GAASzM,IAAI,CAAC0M,eAAL,CAAqBlI,CAAC,CAACc,KAAvB,CAAvB;AACA,WAAK1D,KAAL,CAAW0G,QAAX,IAAuB0D,KAAvB;AACD,KAhBoB,CAkBrB;;;AACA,QAAIpH,IAAI,CAAClC,OAAL,CAAayC,WAAb,CAAyBX,CAAC,CAACG,MAA3B,CAAJ,EAAwC;AACtC,WAAKoI,YAAL,CAAkBvI,CAAC,CAACG,MAApB,EAA4BC,IAAI,CAAClC,OAAjC;AACD,KArBoB,CAuBrB;AACA;AACA;;AACD;;AAEDwK,kBAAgB;AACd,SAAK,MAAMC,OAAX,IAAsB,KAAKvL,KAAL,CAAWH,mBAAjC,EAAsD;AACpD,YAAM6K,CAAC,GAAG,KAAK1K,KAAL,CAAWH,mBAAX,CAA+B0L,OAA/B,CAAV;AACA,WAAKC,eAAL,CAAqBd,CAArB;AACD;AACF;;AAEDc,iBAAe,CAACd,CAAD,EAAY;AACzB,SAAKW,aAAL,CAAmBX,CAAnB;;AACA,QAAI,KAAK1K,KAAL,CAAWH,mBAAX,CAA+B6K,CAAC,CAAChL,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,aAAO,KAAKM,KAAL,CAAWH,mBAAX,CAA+B6K,CAAC,CAAChL,IAAjC,CAAP;AACD;AACF;;AAED+L,QAAM;AACJ,UAAMzI,IAAI,GAAG,KAAKlC,OAAL,CAAa2K,MAAb,EAAb;AACAzI,QAAI,CAAC4D,UAAL,GAAkB,KAAK5G,KAAL,CAAW4G,UAA7B;AACA5D,QAAI,CAACgI,cAAL,GAAsB,KAAKhL,KAAL,CAAWgL,cAAjC;AACAhI,QAAI,CAAC0D,QAAL,GAAgB,KAAK1G,KAAL,CAAW0G,QAA3B;;AACA,QAAI,KAAK1G,KAAL,CAAW4K,gBAAX,GAA8B,CAAlC,EAAqC;AACnC5H,UAAI,CAAC0I,UAAL,GAAkB,IAAlB;;AACA,UAAI1I,IAAI,CAAC2I,OAAL,IAAgB,IAApB,EAA0B;AACxB3I,YAAI,CAAC2I,OAAL,GAAe,EAAf;AACD;;AACD3I,UAAI,CAAC2I,OAAL,CAAa9E,IAAb,CACI,mDACA,yBAFJ;AAGD;;AACD,WAAO7D,IAAP;AACD;;AAEY,QAAP4I,OAAO,CAACC,KAAD,EAA0D;AAErE,SAAK7L,KAAL,CAAWmI,SAAX,GAAuB,IAAvB;AAEA,UAAM2D,UAAU,GAAG,KAAK9L,KAAL,CAAW0G,QAA9B;AACA,UAAMqF,eAAe,GAAG,KAAK/L,KAAL,CAAW4G,UAAnC;AAEA,SAAK5G,KAAL,CAAWsI,aAAX,CAAyBpJ,OAAzB,GAAmC,EAAnC;AACA,SAAKc,KAAL,CAAWsI,aAAX,CAAyBnJ,MAAzB,GAAkC,MAAM0M,KAAK,EAA7C;AAEA,SAAK7L,KAAL,CAAWmI,SAAX,GAAuB,KAAvB;AAEA,SAAKnI,KAAL,CAAWsI,aAAX,CAAyBrJ,SAAzB,GAAqC+M,IAAI,CAACC,GAAL,CACjC,GAAG,KAAKjM,KAAL,CAAWsI,aAAX,CAAyBpJ,OAAzB,CAAiCM,GAAjC,CAAqCuK,CAAC,IAAIA,CAAC,CAACvB,kBAA5C,CAD8B,CAArC;AAEA,SAAKxI,KAAL,CAAWsI,aAAX,CAAyBvJ,QAAzB,GAAoC,KAAKiB,KAAL,CAAW0G,QAAX,GAAsBoF,UAA1D;AACA,SAAK9L,KAAL,CAAWsI,aAAX,CAAyBtJ,UAAzB,GACI,KAAKgB,KAAL,CAAW4G,UAAX,GAAwBmF,eAD5B;;AAEA,SAAK,MAAMhK,MAAX,IAAqB,KAAK/B,KAAL,CAAWsI,aAAX,CAAyBpJ,OAA9C,EAAuD;AACrD6C,YAAM,CAACgH,YAAP,GAAsB,MAAMhH,MAAM,CAACgH,YAAnC;AACAhH,YAAM,CAACkH,SAAP,GAAmB,MAAMlH,MAAM,CAACkH,SAAhC;AACD;;AACD,WAAO,KAAKjJ,KAAL,CAAWsI,aAAlB;AACD;;AAED9B,UAAQ;AACN,WAAO,KAAKxG,KAAL,CAAWkM,aAAX,GAA2B,CAA3B,IAAgC,KAAKlM,KAAL,CAAWkI,WAAX,KAA2B,CAAlE;AACD;;AAEOzC,aAAW,CACf9G,UADe,EACKwG,MADL,EAC6BoB,OAD7B,EAEf4F,aAFe,EAEU3G,KAFV,EAE2BD,KAF3B,EAE8C;AAC/D,UAAM6G,QAAQ,GACV;AAACC,QAAE,EAAE,KAAKrM,KAAL,CAAWsM,cAAX,EAAL;AAAkC3N,gBAAlC;AAA8CwG,YAA9C;AAAsDoB,aAAtD;AAA+Df;AAA/D,KADJ;AAGA,UAAM0D,UAAU,GAAGzL,WAAW,CAACkB,UAAD,CAA9B;;AACA,QAAIuK,UAAU,IAAI,IAAlB,EAAwB;AACtBiD,mBAAa,GAAGjD,UAAU,CAACqD,QAA3B;AACD;;AACD,QAAIJ,aAAa,IAAI,IAArB,EAA2B;AACzBC,cAAQ,CAACI,QAAT,GAAqBC,GAAD,IAAkB;AACpC;AACA;AACAA,WAAG,GAAGA,GAAG,CAACjN,GAAJ,CAAQ,CAAC6F,EAAD,EAAK9E,CAAL,KAAU;AACtB,cAAI8E,EAAE,IAAI,IAAV,EAAgB;AACd,kBAAMqH,MAAM,GAAGnG,OAAO,CAAChG,CAAD,CAAtB;AACA,kBAAMoM,IAAI,GAAGvO,IAAI,CAACwO,mBAAL,CAAyBF,MAAM,CAAC7B,IAAhC,EAAsC6B,MAAM,CAAChJ,KAA7C,CAAb;AACA,mBAAO,KAAKkG,UAAL,CAAgB+C,IAAhB,EAAsBD,MAAM,CAACjJ,KAA7B,EAAoCiJ,MAAM,CAAChJ,KAA3C,CAAP;AACD;;AACD,iBAAO2B,EAAP;AACD,SAPK,CAAN,CAHoC,CAWpC;AACA;;AACA,eAAO8G,aAAa,CAACM,GAAG,CAACjM,MAAJ,GAAa,CAAb,GAAiBiM,GAAjB,GAAuBA,GAAG,CAAC,CAAD,CAA3B,EAAgCjH,KAAhC,EAAuCD,KAAvC,CAApB;AACD,OAdD;AAeD;;AACD,SAAKvF,KAAL,CAAW6M,UAAX,CAAsBhG,IAAtB,CAA2BuF,QAA3B;AACD;;AAEDtE,MAAI,CAAmB3I,MAAnB,EAA4B;AAC9BA,UAAM,CAAC2N,IAAP,GAAc,IAAd;AACA,WAAO3N,MAAP;AACD;;AAEO4N,WAAS;AACf,QAAI,KAAK/M,KAAL,CAAWkM,aAAX,KAA6B,CAAjC,EAAoC;AAClC,WAAKlM,KAAL,CAAW6M,UAAX,GAAwB,EAAxB;AACD;;AACD,SAAK7M,KAAL,CAAWkM,aAAX;AACD;;AAEOc,SAAO;AACb,SAAKhN,KAAL,CAAWkM,aAAX;AACD;AAED;;;;;;AAIAhI,YAAU,CAACxE,IAAD,EAAc;AACtB,UAAMuN,SAAS,GAAe;AAC5B/B,WAAK,EAAE,EADqB;AAE5BxL,UAAI,EAAE,eAFsB;AAG5B2M,QAAE,EAAE,KAAKrM,KAAL,CAAWkN,WAAX;AAHwB,KAA9B;;AAKA,QAAIxN,IAAJ,EAAU;AACRuN,eAAS,CAACvN,IAAV,GAAiBA,IAAjB;AACD;;AACD,SAAKM,KAAL,CAAWmN,UAAX,CAAsBtG,IAAtB,CAA2BoG,SAA3B;AACA,SAAKjN,KAAL,CAAW0F,WAAX,GAAyBuH,SAAzB;AACD;AAED;;;;;;AAIA9I,UAAQ,CAAChF,MAAD,EAAyB;AAC/B,UAAMiO,sBAAsB,GAAGjP,qBAAqB,CAACgB,MAAD,CAApD;AACA,UAAMkO,yBAAyB,GAC3B,IAAI9N,GAAJ,CAAQ6N,sBAAsB,CAAC5N,GAAvB,CAA2B0K,CAAC,IAAIA,CAAC,CAACmC,EAAlC,CAAR,CADJ,CAF+B,CAK/B;;AACA,SAAK,IAAI9L,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAKP,KAAL,CAAW0F,WAAX,CAAuBwF,KAAvB,CAA6B1K,MAAjD,EAAyDD,CAAC,EAA1D,EAA8D;AAC5D,YAAMsH,MAAM,GAAG,KAAK7H,KAAL,CAAW0F,WAAX,CAAuBwF,KAAvB,CAA6B3K,CAA7B,CAAf;;AACA,UAAI,CAACsH,MAAM,CAACiF,IAAR,IAAgB,CAACO,yBAAyB,CAACtC,GAA1B,CAA8BlD,MAAM,CAACwE,EAArC,CAArB,EAA+D;AAC7DxE,cAAM,CAAClI,OAAP;AACD;AACF;;AAED,UAAM2N,QAAQ,GAAG,KAAKtN,KAAL,CAAWmN,UAAX,CAAsBI,GAAtB,EAAjB;AACA,SAAKvN,KAAL,CAAW0F,WAAX,GAAyB,KAAK1F,KAAL,CAAWmN,UAAX,CAAsB3M,MAAtB,KAAiC,CAAjC,GACrB,IADqB,GAErB,KAAKR,KAAL,CAAWmN,UAAX,CAAsB,KAAKnN,KAAL,CAAWmN,UAAX,CAAsB3M,MAAtB,GAA+B,CAArD,CAFJ,CAd+B,CAkB/B;;AACA4M,0BAAsB,CAACtL,OAAvB,CAA+B+F,MAAM,IAAG;AACtC;AACA;AACA,UAAI,CAACA,MAAM,CAACiF,IAAR,IAAgBjF,MAAM,CAAC2F,OAAP,KAAmBF,QAAQ,CAACjB,EAAhD,EAAoD;AAClD,aAAKnB,KAAL,CAAWrD,MAAX;AACD;AACF,KAND;AAOD;AAED;;;;;;;;AAMA4F,WAAS,CACLhJ,CADK,EACOiJ,EADP,EACqBrI,EADrB,EAEmB;AAAA,QAAxBsI,gBAAwB,uEAAL,KAAK;AAC1BvP,QAAI,CAAC6I,MAAL,CACIyG,EAAE,CAAClN,MAAH,GAAY,CADhB,EACmB,MAAM,2CADzB;;AAEA,QAAI6E,EAAE,IAAI,IAAN,IAAcA,EAAE,CAAC3B,KAAH,KAAa,SAA/B,EAA0C;AACxC,YAAM,IAAI7C,KAAJ,CAAU,0CAA0CwE,EAAE,CAAC3B,KAAK,GAA5D,CAAN;AACD;;AAED,UAAMsB,CAAC,GAAG,KAAKf,SAAL,CACN,MAAM,KAAK8I,SAAL,EADA,EACkB,MAAM,KAAKC,OAAL,EADxB,EAEN,MAAM,KAAKnJ,IAAL,CAAU,SAAV,EAAqBY,CAArB,CAFA,CAAV;AAIArG,QAAI,CAAC6I,MAAL,CACIjC,CAAC,YAAY/G,MADjB,EAEI,MAAM,gDAFV,EAX0B,CAc1B;;AACA,UAAM2P,YAAY,GAAG7P,oBAAoB,CAAC,KAAKiC,KAAL,CAAW6M,UAAZ,EAAwBa,EAAxB,EAA4B1I,CAA5B,CAAzC;;AACA,QAAI,CAAC2I,gBAAD,IAAqBC,YAAY,CAACpN,MAAb,KAAwB,CAA7C,IAAkDkN,EAAE,CAAClN,MAAH,GAAY,CAAlE,EAAqE;AACnE,YAAM,IAAIK,KAAJ,CACF,oEACA,iEADA,GAEA,OAHE,CAAN;AAID;;AAED,WAAO,KAAKgD,IAAL,CAAU,UAAV,EAAsB,MAAK;AAChC,YAAMgK,sBAAsB,GAAiC,EAA7D;AACAA,4BAAsB,CAAC7I,CAAC,CAACqH,EAAH,CAAtB,GAAgChH,EAAE,IAAI,IAAP,GAAeyI,IAAI,CAAC9I,CAAC,CAACvB,KAAH,CAAnB,GAA+B4B,EAA9D,CAFgC,CAIhC;;AACAvH,4BAAsB,CAClB+P,sBADkB,EACMD,YADN,EAElB;AACAnJ,OAAC,IAAI,KAAKZ,IAAL,CAAUY,CAAV,CAHa,EAIlB;AACAsJ,SALkB,CAAtB;AAMA,YAAMC,KAAK,GAAGN,EAAE,CAAClO,GAAH,CAAOuF,CAAC,IAAI8I,sBAAsB,CAAC9I,CAAC,CAACsH,EAAH,CAAlC,CAAd;;AAEA,UAAI,KAAKrM,KAAL,CAAWkM,aAAX,KAA6B,CAAjC,EAAoC;AAClC;AACA;AACA,aAAKlM,KAAL,CAAW6M,UAAX,CAAsB/K,OAAtB,CAA8BmM,IAAI,IAAG;AACnC,eAAK,MAAMpG,MAAX,IAAqBoG,IAAI,CAACzI,KAA1B,EAAiC;AAC/BqC,kBAAM,CAAClI,OAAP;AACD;AACF,SAJD;AAKA,aAAKK,KAAL,CAAW6M,UAAX,GAAwB,IAAxB;AACD;;AACD,aAAO;AAACqB,aAAK,EAAElJ,CAAR;AAAWgJ;AAAX,OAAP;AACD,KAxBM,CAAP;AAyBD;;AAEDG,YAAU,CAAmB1J,CAAnB,EAA2C;AAAA;;AAEnDrG,QAAI,CAAC6I,MAAL,CACI7I,IAAI,CAACgQ,UAAL,CAAgB3J,CAAhB,CADJ,EAEI,MAAM,mDAFV;AAGA,WAAO,YAA2B;AAAA,wCAAvBU,MAAuB;AAAvBA,cAAuB;AAAA;;AAChC/G,UAAI,CAAC6I,MAAL,CACI9B,MAAM,CAACkJ,KAAP,CAAanE,CAAC,IAAIA,CAAC,YAAYjM,MAA/B,CADJ,EAEI,MAAM,8DACF,SAHR;AAKA,UAAIyG,GAAJ;AAIA,YAAM4J,QAAQ,GAAmB,EAAjC;AACAnJ,YAAM,CAACrD,OAAP,CAAe,CAACyM,KAAD,EAAQhO,CAAR,KAAa;AAC1B+N,gBAAQ,CAAC/N,CAAD,CAAR,GAAcgO,KAAd;AACD,OAFD;;AAIA,YAAM7G,WAAW,GAAmB,CAACgC,CAAD,EAAI8E,IAAJ,KAAY;AAC9C9J,WAAG,GAAGD,CAAC,CAAC,GAAG,CAAC,GAAGU,MAAJ,EAAYqJ,IAAZ,CAAJ,CAAP;AACApQ,YAAI,CAAC6I,MAAL,CACIvC,GAAG,CAACwJ,KAAJ,YAAqBjQ,MADzB,EAEI,MAAM,2DACF,sCAHR;AAIAG,YAAI,CAAC6I,MAAL,CACI7I,IAAI,CAACgQ,UAAL,CAAgB1J,GAAG,CAAC6H,QAApB,CADJ,EAEI,MAAM,2DACF,4CAHR;AAIA,eAAO7H,GAAG,CAACwJ,KAAX;AACD,OAXD;;AAaA,YAAMlG,aAAa,GAAG,CAAC3C,EAAD,EAAQG,KAAR,KAA2B;AAC/C,cAAMiJ,OAAO,GAAG/J,GAAG,CAAC6H,QAAJ,CAAalH,EAAb,EAAiBG,KAAjB,CAAhB;AACA,cAAMwI,KAAK,GAAa3O,KAAK,CAAC6H,OAAN,CAAcuH,OAAd,IAAyBA,OAAzB,GAAmC,CAACA,OAAD,CAA3D;AACArQ,YAAI,CAAC6I,MAAL,CACI+G,KAAK,CAACxN,MAAN,KAAiB2E,MAAM,CAAC3E,MAD5B,EAEI,MAAM,2DACF,yDADE,GAEF,wDAJR;AAKApC,YAAI,CAAC6I,MAAL,CACI+G,KAAK,CAACK,KAAN,CAAYnE,CAAC,IAAIA,CAAC,YAAYjM,MAA9B,CADJ,EAEI,MAAM,2DACF,yDADE,GAEF,yBAJR;AAKA,cAAMyQ,OAAO,GAAkC,EAA/C;AACAV,aAAK,CAAClM,OAAN,CAAc,CAACsD,IAAD,EAAO7E,CAAP,KAAY;AACxBmO,iBAAO,CAACnO,CAAD,CAAP,GAAa,MAAM6E,IAAnB;AACD,SAFD;AAGA,eAAOsJ,OAAP;AACD,OAlBD;;AAoBA,aAAO,KAAI,CAAC9I,aAAL,CAAmB;AACxB8B,mBADwB;AAExBM,qBAFwB;AAGxB7C,cAAM,EAAEmJ;AAHgB,OAAnB,CAAP;AAKD,KArDD;AAsDD;;AAEDjL,UAAQ,CAACN,MAAD,EAAe;AACrB;AACA,UAAMC,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,CAAb;AACA,WAAOC,IAAI,CAAClC,OAAL,CAAauC,QAAb,CAAsBN,MAAtB,CAAP;AACD;;AACD4L,MAAI,CAAC5L,MAAD,EAAe;AACjB;AACA,UAAMC,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,CAAb;AACA,WAAOC,IAAI,CAAClC,OAAL,CAAa6N,IAAb,CAAkB5L,MAAlB,CAAP;AACD;;AAED6L,WAAS,CAAC7L,MAAD,EAAiB8L,OAAjB,EAA2C;AAClD;AACA,UAAM7L,IAAI,GAAG,KAAKhD,KAAL,CAAWiD,UAAX,CAAsBC,GAAtB,CAA0BH,MAA1B,CAAb;AACA,WAAOC,IAAI,CAAClC,OAAL,CAAa8N,SAAb,CAAuB7L,MAAvB,EAA+B8L,OAA/B,CAAP;AACD;;AAES,QAAJC,IAAI,CAACjD,KAAD,EAAkB;AAC1B,UAAMtH,KAAK,GAAGhG,GAAG,EAAjB;AACA,UAAMwQ,UAAU,GAAG,MAAM,KAAKjO,OAAL,CAAagO,IAAb,CAAkBjD,KAAlB,CAAzB;AACAkD,cAAU,CAACC,MAAX,GAAoBzQ,GAAG,KAAKgG,KAA5B;AACA,WAAOwK,UAAP;AACD;AAED;;;;;;;;AAMQ7D,OAAK,CAAmB/L,MAAnB,EAA4B;AACvC,QAAI,KAAKa,KAAL,CAAW0F,WAAX,IAA0B,IAA9B,EAAoC;AAClCvG,YAAM,CAACqO,OAAP,GAAiB,KAAKxN,KAAL,CAAW0F,WAAX,CAAuB2G,EAAxC;AACA,WAAKrM,KAAL,CAAW0F,WAAX,CAAuBwF,KAAvB,CAA6BrE,IAA7B,CAAkC1H,MAAlC;AACD;;AAED,WAAOA,MAAP;AACD;;AAEsB,MAAnBU,mBAAmB;AACrB,WAAO,KAAKG,KAAL,CAAWH,mBAAlB;AACD;AAED;;;;;;AAIAoP,OAAK;AACH;AACA,SAAK5M,oBAAL;AAEA,SAAKrC,KAAL,CAAWL,OAAX;AACA,SAAKI,GAAL,CAASkP,KAAT;AACA,SAAKjP,KAAL,GAAa,IAAIpB,WAAJ,EAAb;;AAEA,SAAK,MAAM6B,WAAX,IAA0B,KAAKa,QAA/B,EAAyC;AACvC,WAAKW,wBAAL,CAA8BxB,WAA9B;AACA,WAAKa,QAAL,CAAcb,WAAd,EAA2Bd,OAA3B;AACA,aAAO,KAAK2B,QAAL,CAAcb,WAAd,CAAP;AACD;;AACD,SAAKA,WAAL,GAAmB,IAAnB;AACA,SAAKL,eAAL,GAAuB,IAAvB;AACA,SAAKF,kBAAL,GAA0B,IAA1B;AACD;;AA5kCgB;AAiUFJ,sBAAe,CAAf;AAKAA,wBAAiB,CAAjB;;AAywBjB,SAASgO,IAAT,CAAcrK,KAAd,EAA6B;AAC3B,QAAML,MAAM,GAAG9E,kBAAkB,CAACE,aAAa,CAACiF,KAAD,CAAd,EAAuB,SAAvB,CAAjC;AACA,SAAOwB,MAAM,CAAC2E,UAAP,CAAkBxG,MAAlB,EAA0BK,KAA1B,EAAiC,SAAjC,CAAP;AACD;;AAED,OAAM,SAAUyL,eAAV,GAAyB;AAC7B,QAAMC,EAAE,GAAG9R,kBAAkB,EAA7B;;AACA,MAAI8R,EAAE,CAACC,SAAH,IAAgB,IAApB,EAA0B;AACxB,UAAMC,WAAW,GAAG,IAAIlS,WAAJ,CAAgBgS,EAAhB,CAApB;AACAA,MAAE,CAACC,SAAH,GAAe,IAAItP,MAAJ,CAAWuP,WAAX,CAAf;AACD;;AACDjS,sBAAoB,CAAC+R,EAAE,CAACC,SAAH,CAAarP,GAAd,CAApB,CAN6B,CAQ7B;AACA;;AACA/B,kBAAgB,CAAC,MAAMmR,EAAE,CAACC,SAAV,CAAhB;AACA,SAAOD,EAAE,CAACC,SAAV;AACD;AAED,OAAO,MAAMnK,MAAM,GAAGiK,eAAe,EAA9B;AAEP;;;;;;;AAMA,OAAM,SAAUnB,GAAV,CAAcnL,CAAd,EAAyBC,CAAzB,EAAkC;AACtC;AACA,QAAMsC,MAAM,GAAG;AAACvC,KAAD;AAAIC;AAAJ,GAAf;AACA,SAAOoC,MAAM,CAACC,SAAP,CAAiB5H,GAAjB,EAAsB6H,MAAtB,CAAP;AACD","names":["KernelBackend","Environment","setEnvironmentGlobal","getGlobalNamespace","Add","Cast","Identity","getGradient","getKernel","getKernelsForBackend","log","Profiler","backpropagateGradients","getFilteredNodesXToY","setTensorTracker","Tensor","Variable","getTensorsInContainer","util","bytesFromStringArray","makeOnesTypedArray","now","sizeFromShape","isRegisteredKernelInvocation","kernelInvocation","kernelName","EngineState","constructor","WeakMap","newBytes","newTensors","peakBytes","kernels","result","kernelNames","Array","from","Set","map","k","name","dispose","variableName","registeredVariables","Engine","ENV","state","ready","pendingBackendInit","then","backendInstance","sortedBackends","getSortedBackends","i","length","backendName","success","initializeBackend","setBackend","Error","backend","asyncInit","initializeBackendsAndReturnBest","backendNames","Object","keys","registryFactory","findBackend","registry","findBackendFactory","factory","registerBackend","priority","warn","setupRegisteredKernels","profiler","forEach","kernel","setupFunc","disposeRegisteredKernels","disposeFunc","registryFactoryEntry","promiseId","pendingBackendInitId","catch","err","stack","message","removeBackend","sort","a","b","moveData","dataId","info","tensorInfo","get","srcBackend","values","readSync","refCount","disposeData","move","shape","dtype","shouldCheckForMemLeaks","numDataMovesStack","tidy","nameOrFn","fn","String","scopedRun","startScope","endScope","Promise","console","error","start","end","f","res","ex","nextTensorId","nextVariableId","clone","x","y","ENGINE","runKernel","inputs","grad","dy","gradInputs","attrs","saved","addTapeNode","activeScope","hasKernel","runKernelFunc","getBool","checkKernelForMemLeak","numDataIdsBefore","outInfos","numDataIdsAfter","numDataIds","numOutputDataIds","numMoves","dataIdsLeaked","kernelParams","outputs","isTapeOn","startingBytecount","numBytes","startingNumTensors","numTensors","push","kernelFunc","out","kernelOrScopeName","assert","isArray","outTensors","outInfo","rank","makeTensorFromDataId","tensorsToSave","getTensorsForGradient","saveTensorsForBackwardMode","forwardFunc","saveFunc","tensors","tensor","keep","outs","backwardsFunc","kernelProfile","kernelDepth","profiling","profileKernel","logKernelProfile","activeProfile","bytesAdded","totalBytesSnapshot","tensorsAdded","totalTensorsSnapshot","inputShapes","key","outputShapes","item","kernelTimeMs","timeMs","extraInfo","gradConfig","inputsToSave","outputsToSave","inputTensorsToSave","saveAllInputs","inputName","outputTensorsToSave","filter","_","concat","makeTensor","backendVals","isString","d","encodeString","write","t","trackTensor","bytes","makeVariable","initialValue","trainable","toString","cast","v","incRef","numStringTensors","size","bytesPerElement","has","numDataBuffers","set","track","removeDataId","delete","disposeTensor","disposeVariables","varName","disposeVariable","memory","unreliable","reasons","profile","query","startBytes","startNumTensors","Math","max","gradientDepth","gradientsFunc","tapeNode","id","nextTapeNodeId","gradFunc","gradient","dys","output","vals","makeZerosTypedArray","activeTape","kept","startTape","endTape","scopeInfo","nextScopeId","scopeStack","tensorsToTrackInParent","tensorsToTrackInParentSet","oldScope","pop","scopeId","gradients","xs","allowNoGradients","filteredTape","accumulatedGradientMap","ones","add","grads","node","value","customGrad","isFunction","every","inputMap","input","save","gradRes","gradMap","read","readToGPU","options","time","timingInfo","wallMs","reset","getOrMakeEngine","ns","_tfengine","environment"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/engine.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BackendTimingInfo, DataMover, KernelBackend} from './backends/backend';\nimport {Environment, setEnvironmentGlobal} from './environment';\nimport {getGlobalNamespace} from './global_util';\nimport {Add, Cast, Identity} from './kernel_names';\nimport {getGradient, getKernel, getKernelsForBackend, GradFunc, NamedAttrMap, TensorInfo} from './kernel_registry';\nimport * as log from './log';\nimport {KernelProfile, Profiler} from './profiler';\nimport {backpropagateGradients, getFilteredNodesXToY, TapeNode} from './tape';\nimport {DataId, DataToGPUOptions, GPUData, setTensorTracker, Tensor, TensorTracker, Variable} from './tensor';\nimport {GradSaveFunc, NamedTensorMap, NamedVariableMap, TensorContainer} from './tensor_types';\nimport {getTensorsInContainer} from './tensor_util';\nimport {BackendValues, DataType, DataValues} from './types';\nimport * as util from './util';\nimport {bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape} from './util';\n\n/**\n * A function that computes an output. The save function is for saving tensors\n * computed in the forward pass, that we need in the backward pass.\n */\nexport type ForwardFunc<T> = (backend: KernelBackend, save?: GradSaveFunc) => T;\n\n/**\n * @docalias (a: Tensor, b: Tensor,..., save?: Function) => {\n *   value: Tensor,\n *   gradFunc: (dy: Tensor, saved?: NamedTensorMap) => Tensor | Tensor[]\n * }\n */\nexport type CustomGradientFunc<T extends Tensor> =\n    (...inputs: Array<Tensor|GradSaveFunc>) => {\n      value: T;\n      gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[];\n    };\n\nexport type MemoryInfo = {\n  numTensors: number; numDataBuffers: number; numBytes: number;\n  unreliable?: boolean; reasons: string[];\n};\n\ntype KernelInfo = {\n  name: string; bytesAdded: number; totalBytesSnapshot: number;\n  tensorsAdded: number;\n  totalTensorsSnapshot: number;\n  inputShapes: number[][];\n  outputShapes: number[][];\n  kernelTimeMs: number | {error: string} | Promise<number|{error: string}>;\n  extraInfo: string | Promise<string>;\n};\n\nexport type ProfileInfo = {\n  newBytes: number; newTensors: number; peakBytes: number;\n  kernels: KernelInfo[];\n  result: TensorContainer;\n  kernelNames: string[];\n};\n\nexport interface TimingInfo extends BackendTimingInfo {\n  wallMs: number;\n}\n\n/** @docalias Function */\nexport type ScopeFn<T extends TensorContainer> = () => T;\n\ninterface ScopeState {\n  track: Tensor[];\n  name: string;\n  id: number;\n}\n\ninterface RegisteredKernelInvocation<I extends NamedTensorMap> {\n  kernelName: string;\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\ninterface CustomGradKernelInvocation<T extends Tensor|Tensor[],\n                                               I extends NamedTensorMap> {\n  forwardFunc: ForwardFunc<T>;\n  backwardsFunc: (dy: T, saved: Tensor[]) => {\n    [P in keyof I]: () => I[P]\n  };\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\nfunction isRegisteredKernelInvocation<T extends Tensor|Tensor[],\n                                                I extends NamedTensorMap>(\n    kernelInvocation: RegisteredKernelInvocation<I>|\n    CustomGradKernelInvocation<T, I>):\n    kernelInvocation is RegisteredKernelInvocation<I> {\n  return (kernelInvocation as RegisteredKernelInvocation<I>).kernelName != null;\n}\n\nclass EngineState {\n  // Public since optimizers will use it.\n  registeredVariables: NamedVariableMap = {};\n\n  nextTapeNodeId = 0;\n  numBytes = 0;\n  numTensors = 0;\n  numStringTensors = 0;\n  numDataBuffers = 0;\n\n  activeTape: TapeNode[];\n  // Number of nested tf.grad() statements when computing higher-order\n  // gradients. E.g. `1` for first-order gradients and `2` for second-order\n  // gradients. Used to track if the tape should be removed after a backprop.\n  gradientDepth = 0;\n  // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n  // off the tape.\n  kernelDepth = 0;\n\n  // Keep Tensors that parallel the tapes.\n  activeScope: ScopeState;\n  scopeStack: ScopeState[] = [];\n  /**\n   * Keeps track of the number of data moves during a kernel execution. We\n   * maintain a stack since kernels can call other kernels, recursively.\n   */\n  numDataMovesStack: number[] = [];\n  nextScopeId = 0;\n\n  tensorInfo = new WeakMap<DataId, {\n    backend: KernelBackend,\n    bytes: number,\n    dtype: DataType,\n    shape: number[]\n  }>();\n\n  profiling = false;\n  activeProfile: ProfileInfo = {\n    newBytes: 0,\n    newTensors: 0,\n    peakBytes: 0,\n    kernels: [],\n    result: null,\n    get kernelNames():\n        string[] {\n          return Array.from(new Set(this.kernels.map(k => k.name)));\n        }\n  };\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n}\n\nexport class Engine implements TensorTracker, DataMover {\n  state: EngineState;\n  backendName: string;\n  registry: {[id: string]: KernelBackend} = {};\n  registryFactory: {\n    [id: string]: {\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority: number\n    }\n  } = {};\n\n  private profiler: Profiler;\n  private backendInstance: KernelBackend;\n  private pendingBackendInit: Promise<boolean>;\n  private pendingBackendInitId = 0;\n\n  constructor(public ENV: Environment) {\n    this.state = new EngineState();\n  }\n\n  async ready(): Promise<void> {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n    if (this.backendInstance != null) {\n      return;\n    }\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  get backend(): KernelBackend {\n    if (this.pendingBackendInit != null) {\n      throw new Error(\n          `Backend '${this.backendName}' has not yet been initialized. Make ` +\n          `sure to await tf.ready() or await tf.setBackend() before calling ` +\n          `other methods`);\n    }\n    if (this.backendInstance == null) {\n      const {name, asyncInit} = this.initializeBackendsAndReturnBest();\n      if (asyncInit) {\n        throw new Error(\n            `The highest priority backend '${name}' has not yet been ` +\n            `initialized. Make sure to await tf.ready() or ` +\n            `await tf.setBackend() before calling other methods`);\n      }\n      this.setBackend(name);\n    }\n    return this.backendInstance;\n  }\n\n  backendNames(): string[] {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName: string): KernelBackend {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {asyncInit} = this.initializeBackend(backendName);\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName: string):\n      () => KernelBackend | Promise<KernelBackend> {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(\n      backendName: string,\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority = 1): boolean {\n    if (backendName in this.registryFactory) {\n      log.warn(\n          `${backendName} backend was already registered. ` +\n          `Reusing existing backend factory.`);\n      return false;\n    }\n    this.registryFactory[backendName] = {factory, priority};\n    return true;\n  }\n\n  async setBackend(backendName: string): Promise<boolean> {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n    this.backendName = backendName;\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n      if (!result) {\n        return false;\n      }\n    }\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels();\n    // Reset the profiler.\n    this.profiler = new Profiler(this.backendInstance);\n\n    return true;\n  }\n\n  private setupRegisteredKernels(): void {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  private disposeRegisteredKernels(backendName: string): void {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n  private initializeBackend(backendName: string):\n      {success: boolean|Promise<boolean>, asyncInit: boolean} {\n    const registryFactoryEntry = this.registryFactory[backendName];\n    if (registryFactoryEntry == null) {\n      throw new Error(\n          `Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\n      Done in a more liberal way than\n      previous 'Promise.resolve(backend)===backend'\n      as we needed to account for custom Promise\n      implementations (e.g. Angular) */\n      if (backend && !(backend instanceof KernelBackend) &&\n          typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success =\n            backend\n                .then(backendInstance => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.registry[backendName] = backendInstance;\n                  this.pendingBackendInit = null;\n                  return true;\n                })\n                .catch(err => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.pendingBackendInit = null;\n                  log.warn(`Initialization of backend ${backendName} failed`);\n                  log.warn(err.stack || err.message);\n                  return false;\n                });\n        this.pendingBackendInit = success;\n        return {success, asyncInit: true};\n      } else {\n        this.registry[backendName] = backend as KernelBackend;\n        return {success: true, asyncInit: false};\n      }\n    } catch (err) {\n      log.warn(`Initialization of backend ${backendName} failed`);\n      log.warn(err.stack || err.message);\n      return {success: false, asyncInit: false};\n    }\n  }\n\n  removeBackend(backendName: string): void {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName];\n\n    // Unset the backend if it is active.\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  private getSortedBackends(): string[] {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n    return Object.keys(this.registryFactory).sort((a: string, b: string) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority -\n          this.registryFactory[a].priority;\n    });\n  }\n\n  private initializeBackendsAndReturnBest():\n      {name: string, asyncInit: boolean} {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      if (asyncInit || success) {\n        return {name: backendName, asyncInit};\n      }\n    }\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  moveData(backend: KernelBackend, dataId: DataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    const refCount = srcBackend.refCount(dataId);\n    // Delete the tensor from the old backend and move it to the new\n    // backend.\n    srcBackend.disposeData(dataId, true);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype, refCount);\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy<T extends TensorContainer>(nameOrFn: string|ScopeFn<T>, fn?: ScopeFn<T>):\n      T {\n    let name: string = null;\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error(\n            'When calling with two arguments, the first argument ' +\n            'to tidy() must be a string');\n      }\n      if (typeof fn !== 'function') {\n        throw new Error(\n            'When calling with two arguments, the 2nd argument ' +\n            'to tidy() must be a function');\n      }\n      name = nameOrFn as string;\n      // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n    let result: T;\n    return this.scopedRun(\n        () => this.startScope(name), () => this.endScope(result), () => {\n          result = fn();\n          if (result instanceof Promise) {\n            console.error('Cannot return a Promise inside of tidy.');\n          }\n          return result;\n        });\n  }\n\n  private scopedRun<T>(start: () => void, end: () => void, f: () => T): T {\n    start();\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  private static nextTensorId = 0;\n  private nextTensorId(): number {\n    return Engine.nextTensorId++;\n  }\n\n  private static nextVariableId = 0;\n  private nextVariableId(): number {\n    return Engine.nextVariableId++;\n  }\n\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   */\n  private clone(x: Tensor): Tensor {\n    const y: Tensor = ENGINE.runKernel(Identity, {x} as {} as NamedTensorMap);\n    const inputs = {x};\n    const grad = (dy: Tensor) => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {x: dy};\n        const attrs = {dtype};\n\n        return ENGINE.runKernel(\n                   Cast, gradInputs as {} as NamedTensorMap,\n                   // tslint:disable-next-line: no-unnecessary-type-assertion\n                   attrs as {} as NamedAttrMap) as Tensor;\n      }\n    });\n    const saved: Tensor[] = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n  runKernel<T extends Tensor|Tensor[]>(\n      kernelName: string, inputs: NamedTensorMap, attrs?: NamedAttrMap): T {\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n    const hasKernel = getKernel(kernelName, this.backendName) != null;\n    if (!hasKernel) {\n      throw new Error(`Kernel '${kernelName}' not registered for backend '${\n          this.backendName}'`);\n    }\n    return this.runKernelFunc({kernelName, inputs, attrs});\n  }\n\n  private shouldCheckForMemLeaks(): boolean {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  private checkKernelForMemLeak(\n      kernelName: string, numDataIdsBefore: number,\n      outInfos: TensorInfo[]): void {\n    const numDataIdsAfter = this.backend.numDataIds();\n\n    // Count the number of data ids associated with the result of the kernel.\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\n    });\n\n    // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n    const numMoves =\n        this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked =\n        numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n    if (dataIdsLeaked > 0) {\n      throw new Error(\n          `Backend '${this.backendName}' has an internal memory leak ` +\n          `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n\n  /**\n   * Internal helper method to execute a kernel Func\n   *\n   * Use `runKernel` to execute kernels from outside of engine.\n   */\n  private runKernelFunc<T extends Tensor|Tensor[], I extends NamedTensorMap>(\n      kernelParams: RegisteredKernelInvocation<I>|\n      CustomGradKernelInvocation<T, I>): T {\n    let outputs: Tensor[];\n    let saved: Tensor[] = [];\n    const isTapeOn = this.isTapeOn();\n\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc: () => Tensor[];\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    let out: TensorInfo|TensorInfo[];\n\n    const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ?\n        kernelParams.kernelName :\n        this.state.activeScope != null ? this.state.activeScope.name : '';\n\n    // Create the kernelFunc from either a registered kernel OR passed in\n    // forward/backward functions (used by custom grad). In this context a\n    // kernelFunc wraps a kernel implementation with some bookkeeping.\n\n    if (isRegisteredKernelInvocation(kernelParams)) {\n      const {kernelName, inputs, attrs} = kernelParams;\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n      const kernel = getKernel(kernelName, this.backendName);\n      util.assert(\n          kernel != null,\n          () => `Cannot find registered kernel '${kernelName}' for backend '${\n              this.backendName}'`);\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({inputs, attrs, backend: this.backend});\n        const outInfos = Array.isArray(out) ? out : [out];\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map((outInfo: TensorInfo|Tensor) => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if ((outInfo as Tensor).rank != null) {\n            return outInfo as Tensor;\n          }\n          const {dataId, shape, dtype} = outInfo as TensorInfo;\n          return this.makeTensorFromDataId(dataId, shape, dtype);\n        });\n\n        // Save any required inputs and outputs.\n\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since there would be no backprop for these tensors\n        // (which would otherwise dispose them).\n        if (isTapeOn) {\n          const tensorsToSave =\n              this.getTensorsForGradient(kernelName, inputs, outTensors);\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n        return outTensors;\n      };\n    } else {\n      const {forwardFunc} = kernelParams;\n      // Running a customGrad op.\n      const saveFunc: GradSaveFunc = (tensors) => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = (Array.isArray(out) ? out : [out]) as Tensor[];\n        if (this.shouldCheckForMemLeaks()) {\n          // Scope name is used to print a more helpful error message if needed.\n          this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n        }\n        return outs;\n      };\n    }\n\n    //\n    // Run the kernelFunc. Optionally profiling it.\n    //\n    const {inputs, attrs} = kernelParams;\n    const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ?\n        null :\n        kernelParams.backwardsFunc;\n\n    let kernelProfile: KernelProfile;\n    this.scopedRun(\n        // Stop recording to a tape when running a kernel.\n        () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n          if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n            outputs = kernelFunc();\n          } else {\n            kernelProfile = this.profiler.profileKernel(\n                kernelOrScopeName, inputs, () => kernelFunc());\n            if (this.ENV.getBool('DEBUG')) {\n              this.profiler.logKernelProfile(kernelProfile);\n            }\n            outputs = kernelProfile.outputs;\n          }\n        });\n\n    if (isTapeOn) {\n      this.addTapeNode(\n          kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelOrScopeName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(\n            key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n    return (Array.isArray(out) ? outputs : outputs[0]) as T;\n  }\n\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n  private saveTensorsForBackwardMode(tensors: Tensor[]): Tensor[] {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n  private getTensorsForGradient(\n      kernelName: string, inputs: NamedTensorMap,\n      outputs: Tensor[]): Tensor[]|null {\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      const inputsToSave: string[] = gradConfig.inputsToSave || [];\n      const outputsToSave: boolean[] = gradConfig.outputsToSave || [];\n\n      // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n      let inputTensorsToSave: Tensor[];\n      if (gradConfig.saveAllInputs) {\n        util.assert(\n            Array.isArray(inputs),\n            () => 'saveAllInputs is true, expected inputs to be an array.');\n\n        inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\n      }\n\n      const outputTensorsToSave: Tensor[] =\n          outputs.filter((_, i) => outputsToSave[i]);\n\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    }\n    // We return an empty list rather than throw an error because the kernel we\n    // are looking up may not actually be relevant to backproping through the\n    // overall function\n    //\n    // See 'does not error if irrelevant (pruned) ops are missing grads' test\n    // in gradients_test.ts for an example.\n    return [];\n  }\n\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n  makeTensor(\n      values: DataValues, shape: number[], dtype: DataType,\n      backend?: KernelBackend): Tensor {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values as BackendValues;\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = (values as string[]).map(d => util.encodeString(d));\n    }\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n\n    // Count bytes for string tensors.\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals as Uint8Array[]);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n    return t;\n  }\n\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   */\n  makeTensorFromDataId(\n      dataId: DataId, shape: number[], dtype: DataType,\n      backend?: KernelBackend): Tensor {\n    dtype = dtype || 'float32';\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n    return t;\n  }\n\n  makeVariable(\n      initialValue: Tensor, trainable = true, name?: string,\n      dtype?: DataType): Variable {\n    name = name || this.nextVariableId().toString();\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  trackTensor(a: Tensor, backend: KernelBackend): void {\n    this.state.numTensors++;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    }\n    // Bytes for complex numbers are counted by their components. Bytes for\n    // string tensors are counted when writing values.\n    let bytes = 0;\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      bytes = a.size * util.bytesPerElement(a.dtype);\n    }\n    this.state.numBytes += bytes;\n\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      this.state.numDataBuffers++;\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes\n      });\n    }\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n\n  // Track the tensor by dataId and increase the refCount for the dataId in the\n  // backend.\n  // TODO(pyu10055): This is currently used by makeVariable method, to increase\n  // refCount on the backend for the dataId. It can potentially be replaced with\n  // Identity op indead of calling backend directly.\n  incRef(a: Tensor, backend: KernelBackend): void {\n    this.trackTensor(a, backend);\n    this.backend.incRef(a.dataId);\n  }\n\n  removeDataId(dataId: DataId, backend: KernelBackend) {\n    if (this.state.tensorInfo.has(dataId) &&\n        this.state.tensorInfo.get(dataId).backend === backend) {\n      this.state.tensorInfo.delete(dataId);\n      this.state.numDataBuffers--;\n    }\n  }\n  disposeTensor(a: Tensor): void {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n    const info = this.state.tensorInfo.get(a.dataId);\n\n    this.state.numTensors--;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n      this.state.numBytes -= info.bytes;\n    }\n    // Don't count bytes for complex numbers as they are counted by their\n    // components.\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      const bytes = a.size * util.bytesPerElement(a.dtype);\n      this.state.numBytes -= bytes;\n    }\n\n    // Remove the reference to dataId if backend dispose the data successfully\n    if (info.backend.disposeData(a.dataId)) {\n      this.removeDataId(a.dataId, info.backend);\n    }\n\n    // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n  }\n\n  disposeVariables(): void {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v: Variable): void {\n    this.disposeTensor(v);\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory(): MemoryInfo {\n    const info = this.backend.memory() as MemoryInfo;\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n      info.reasons.push(\n          'Memory usage by string tensors is approximate ' +\n          '(2 bytes per character)');\n    }\n    return info;\n  }\n\n  async profile(query: () => (TensorContainer | Promise<TensorContainer>)):\n      Promise<ProfileInfo> {\n    this.state.profiling = true;\n\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n\n    this.state.profiling = false;\n\n    this.state.activeProfile.peakBytes = Math.max(\n        ...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors =\n        this.state.numTensors - startNumTensors;\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n    return this.state.activeProfile;\n  }\n\n  isTapeOn(): boolean {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  private addTapeNode(\n      kernelName: string, inputs: NamedTensorMap, outputs: Tensor[],\n      gradientsFunc: GradFunc, saved: Tensor[], attrs: NamedAttrMap): void {\n    const tapeNode: TapeNode =\n        {id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved};\n\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n    if (gradientsFunc != null) {\n      tapeNode.gradient = (dys: Tensor[]) => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n          return dy;\n        });\n        // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep<T extends Tensor>(result: T): T {\n    result.kept = true;\n    return result;\n  }\n\n  private startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n    this.state.gradientDepth++;\n  }\n\n  private endTape() {\n    this.state.gradientDepth--;\n  }\n\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  startScope(name?: string) {\n    const scopeInfo: ScopeState = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n    if (name) {\n      scopeInfo.name = name;\n    }\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  endScope(result?: TensorContainer) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet =\n        new Set(tensorsToTrackInParent.map(t => t.id));\n\n    // Dispose the arrays tracked in this scope.\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ?\n        null :\n        this.state.scopeStack[this.state.scopeStack.length - 1];\n\n    // Track the current result in the parent scope.\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n  gradients<T extends Tensor>(\n      f: () => T, xs: Tensor[], dy?: T,\n      allowNoGradients = false): {value: T, grads: Tensor[]} {\n    util.assert(\n        xs.length > 0, () => 'gradients() received an empty list of xs.');\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(\n        () => this.startTape(), () => this.endTape(),\n        () => this.tidy('forward', f));\n\n    util.assert(\n        y instanceof Tensor,\n        () => 'The result y returned by f() must be a tensor.');\n    // Filter out the nodes that don't connect x => y.\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error(\n          'Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n          'that the f you passed encloses all operations that lead from x ' +\n          'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap: {[tensorId: number]: Tensor} = {};\n      accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\n\n      // Backprop gradients through the filtered nodes.\n      backpropagateGradients(\n          accumulatedGradientMap, filteredTape,\n          // Pass the tidy function to avoid circular dep with `tape.ts`.\n          f => this.tidy(f as ScopeFn<Tensor>),\n          // Pass an add function to avoide a circular dep with `tape.ts`.\n          add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n      return {value: y, grads};\n    });\n  }\n\n  customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n      (...args: Array<Tensor|GradSaveFunc>) => T {\n    util.assert(\n        util.isFunction(f),\n        () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs: Tensor[]): T => {\n      util.assert(\n          inputs.every(t => t instanceof Tensor),\n          () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\n              'tensors');\n\n      let res: {\n        value: T,\n        gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[],\n      };\n      const inputMap: NamedTensorMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n\n      const forwardFunc: ForwardFunc<T> = (_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(\n            res.value instanceof Tensor,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.value` is a tensor');\n        util.assert(\n            util.isFunction(res.gradFunc),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function.');\n        return res.value;\n      };\n\n      const backwardsFunc = (dy: T, saved: Tensor[]) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads: Tensor[] = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(\n            grads.length === inputs.length,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'the same number of tensors as inputs passed to f(...).');\n        util.assert(\n            grads.every(t => t instanceof Tensor),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'a list of only tensors.');\n        const gradMap: {[key: string]: () => Tensor} = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      };\n\n      return this.runKernelFunc({\n        forwardFunc,\n        backwardsFunc,\n        inputs: inputMap,\n      });\n    };\n  }\n\n  readSync(dataId: DataId): BackendValues {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n  read(dataId: DataId): Promise<BackendValues> {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  readToGPU(dataId: DataId, options?: DataToGPUOptions): GPUData {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readToGPU(dataId, options);\n  }\n\n  async time(query: () => void): Promise<TimingInfo> {\n    const start = now();\n    const timingInfo = await this.backend.time(query) as TimingInfo;\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n  private track<T extends Tensor>(result: T): T {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables(): NamedVariableMap {\n    return this.state.registeredVariables;\n  }\n\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n  reset(): void {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n}\n\nfunction ones(shape: number[]): Tensor {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine(): Engine {\n  const ns = getGlobalNamespace() as {} as {_tfengine: Engine};\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n  setEnvironmentGlobal(ns._tfengine.ENV);\n\n  // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\n\nexport const ENGINE = getOrMakeEngine();\n\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a: Tensor, b: Tensor): Tensor {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {a, b};\n  return ENGINE.runKernel(Add, inputs as {} as NamedTensorMap);\n}\n"]},"metadata":{},"sourceType":"module"}