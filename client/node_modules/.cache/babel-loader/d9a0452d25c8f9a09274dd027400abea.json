{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FusedBatchNorm } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { xAs4D } from './batchnorm_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be an `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\n\nfunction batchNorm_(x, mean, variance, offset, scale, varianceEpsilon) {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale;\n\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n\n  let $offset;\n\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n\n  util.assert($mean.rank === $variance.rank, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert($offset == null || $mean.rank === $offset.rank, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert($scale == null || $mean.rank === $scale.rank, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  const x4D = xAs4D($x);\n  const inputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n  const attrs = {\n    varianceEpsilon\n  }; // tslint:disable-next-line: no-unnecessary-type-assertion\n\n  const res = ENGINE.runKernel(FusedBatchNorm, inputs, attrs);\n  return reshape(res, $x.shape);\n}\n\nexport const batchNorm = op({\n  batchNorm_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,cAAR,QAAwE,iBAAxE;AAIA,SAAQC,eAAR,QAA8B,oBAA9B;AAEA,OAAO,KAAKC,IAAZ,MAAsB,SAAtB;AAEA,SAAQC,KAAR,QAAoB,kBAApB;AACA,SAAQC,EAAR,QAAiB,aAAjB;AACA,SAAQC,OAAR,QAAsB,WAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA,SAASC,UAAT,CACIC,CADJ,EAC6BC,IAD7B,EAEIC,QAFJ,EAGIC,MAHJ,EAIIC,KAJJ,EAKIC,eALJ,EAK4B;AAC1B,MAAIA,eAAe,IAAI,IAAvB,EAA6B;AAC3BA,mBAAe,GAAG,KAAlB;AACD;;AACD,QAAMC,EAAE,GAAGZ,eAAe,CAACM,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,QAAMO,KAAK,GAAGb,eAAe,CAACO,IAAD,EAAO,MAAP,EAAe,WAAf,CAA7B;AACA,QAAMO,SAAS,GAAGd,eAAe,CAACQ,QAAD,EAAW,UAAX,EAAuB,WAAvB,CAAjC;AACA,MAAIO,MAAJ;;AACA,MAAIL,KAAK,IAAI,IAAb,EAAmB;AACjBK,UAAM,GAAGf,eAAe,CAACU,KAAD,EAAQ,OAAR,EAAiB,WAAjB,CAAxB;AACD;;AACD,MAAIM,OAAJ;;AACA,MAAIP,MAAM,IAAI,IAAd,EAAoB;AAClBO,WAAO,GAAGhB,eAAe,CAACS,MAAD,EAAS,QAAT,EAAmB,WAAnB,CAAzB;AACD;;AAEDR,MAAI,CAACgB,MAAL,CACIJ,KAAK,CAACK,IAAN,KAAeJ,SAAS,CAACI,IAD7B,EAEI,MAAM,qEACF,cAHR;AAIAjB,MAAI,CAACgB,MAAL,CACID,OAAO,IAAI,IAAX,IAAmBH,KAAK,CAACK,IAAN,KAAeF,OAAO,CAACE,IAD9C,EAEI,MAAM,mEACF,cAHR;AAIAjB,MAAI,CAACgB,MAAL,CACIF,MAAM,IAAI,IAAV,IAAkBF,KAAK,CAACK,IAAN,KAAeH,MAAM,CAACG,IAD5C,EAEI,MAAM,kEACF,cAHR;AAKA,QAAMC,GAAG,GAAajB,KAAK,CAACU,EAAD,CAA3B;AAEA,QAAMQ,MAAM,GAAyB;AACnCd,KAAC,EAAEa,GADgC;AAEnCT,SAAK,EAAEK,MAF4B;AAGnCN,UAAM,EAAEO,OAH2B;AAInCT,QAAI,EAAEM,KAJ6B;AAKnCL,YAAQ,EAAEM;AALyB,GAArC;AAQA,QAAMO,KAAK,GAAwB;AAACV;AAAD,GAAnC,CAvC0B,CAyC1B;;AACA,QAAMW,GAAG,GAAGxB,MAAM,CAACyB,SAAP,CACIxB,cADJ,EACoBqB,MADpB,EAEIC,KAFJ,CAAZ;AAIA,SAAOjB,OAAO,CAACkB,GAAD,EAAMV,EAAE,CAACY,KAAT,CAAd;AACD;;AAED,OAAO,MAAMC,SAAS,GAAGtB,EAAE,CAAC;AAACE;AAAD,CAAD,CAApB","names":["ENGINE","FusedBatchNorm","convertToTensor","util","xAs4D","op","reshape","batchNorm_","x","mean","variance","offset","scale","varianceEpsilon","$x","$mean","$variance","$scale","$offset","assert","rank","x4D","inputs","attrs","res","runKernel","shape","batchNorm"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/batchnorm.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor, Tensor1D, Tensor4D} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {Rank, TensorLike} from '../types';\nimport * as util from '../util';\n\nimport {xAs4D} from './batchnorm_util';\nimport {op} from './operation';\nimport {reshape} from './reshape';\n\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be an `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction batchNorm_<R extends Rank>(\n    x: Tensor<R>|TensorLike, mean: Tensor<R>|Tensor1D|TensorLike,\n    variance: Tensor<R>|Tensor1D|TensorLike,\n    offset?: Tensor<R>|Tensor1D|TensorLike,\n    scale?: Tensor<R>|Tensor1D|TensorLike,\n    varianceEpsilon?: number): Tensor<R> {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale: Tensor<R>|Tensor1D;\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n  let $offset: Tensor<R>|Tensor1D;\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n\n  util.assert(\n      $mean.rank === $variance.rank,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      $offset == null || $mean.rank === $offset.rank,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      $scale == null || $mean.rank === $scale.rank,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  const x4D: Tensor4D = xAs4D($x);\n\n  const inputs: FusedBatchNormInputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n\n  const attrs: FusedBatchNormAttrs = {varianceEpsilon};\n\n  // tslint:disable-next-line: no-unnecessary-type-assertion\n  const res = ENGINE.runKernel(\n                  FusedBatchNorm, inputs as {} as NamedTensorMap,\n                  attrs as {} as NamedAttrMap) as Tensor<R>;\n\n  return reshape(res, $x.shape);\n}\n\nexport const batchNorm = op({batchNorm_});\n"]},"metadata":{},"sourceType":"module"}