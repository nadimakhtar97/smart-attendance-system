{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport class GraphExecutor {\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  constructor(graph, parent) {\n    this.graph = graph;\n    this.parent = parent;\n    this.compiledMap = new Map();\n    this._weightMap = {};\n    this.SEPERATOR = ',';\n    this._functions = {};\n    this._functionExecutorMap = {};\n    this.intermediateTensors = {};\n    this.keepTensorForDebug = false;\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions; // create sub-graph executors\n\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(name => {\n        this._functionExecutorMap[name] = new GraphExecutor(graph.functions[name], this);\n      });\n    }\n  }\n\n  get weightIds() {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap() {\n    return this.parent ? this.parent.functionExecutorMap : this._functionExecutorMap;\n  }\n\n  get weightMap() {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(weightMap) {\n    const weightIds = Object.keys(weightMap).map(key => weightMap[key].map(tensor => tensor.id));\n    this._weightIds = [].concat(...weightIds);\n    this._weightMap = weightMap;\n  }\n  /**\n   * Set `ResourceManager` shared by executors of a model.\n   * @param resourceManager: `ResourceManager` of the `GraphModel`.\n   */\n\n\n  set resourceManager(resourceManager) {\n    this._resourceManager = resourceManager;\n  }\n\n  get inputs() {\n    return this._inputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n        dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n      };\n    });\n  }\n\n  get outputs() {\n    return this._outputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n        dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n      };\n    });\n  }\n\n  get inputNodes() {\n    return this._inputs.map(node => node.signatureKey || node.name);\n  }\n\n  get outputNodes() {\n    return this._outputs.map(node => {\n      const name = node.signatureKey || node.name;\n      return node.defaultOutput ? `${name}:${node.defaultOutput}` : name;\n    });\n  }\n\n  get functions() {\n    return Object.keys(this._functions).reduce((map, key) => {\n      map[key] = this._functions[key].signature;\n      return map;\n    }, {});\n  }\n\n  getCompilationKey(inputs, outputs) {\n    const sortedInputs = inputs.map(node => node.name).sort();\n    const sortedOutputs = outputs.map(node => node.name).sort();\n    return sortedInputs.join(this.SEPERATOR) + '--' + sortedOutputs.join(this.SEPERATOR);\n  }\n  /**\n   * Compiles the inference graph and returns the minimal set of nodes that are\n   * required for execution, in the correct execution order.\n   */\n\n\n  compile(inputs, outputs) {\n    const executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n    const {\n      missingInputs,\n      dynamicNode,\n      syncInputs\n    } = executionInfo;\n\n    if (dynamicNode != null) {\n      throw new Error(`This execution contains the node '${dynamicNode.name}', which has ` + `the dynamic op '${dynamicNode.op}'. Please use ` + `model.executeAsync() instead. Alternatively, to avoid the ` + `dynamic ops, specify the inputs [${syncInputs}]`);\n    }\n\n    if (missingInputs.length > 0) {\n      const outNames = outputs.map(n => n.name);\n      const inNames = Object.keys(inputs);\n      throw new Error(`Cannot compute the outputs [${outNames}] from the provided inputs ` + `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n    }\n\n    return getNodesInTopologicalOrder(this.graph, this.weightMap, executionInfo);\n  }\n  /**\n   * Executes the inference for given input tensors.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model, if\n   * no outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   */\n\n\n  execute(inputs, outputs) {\n    inputs = this.mapInputs(inputs);\n    const names = Object.keys(inputs).sort();\n    this.checkInputs(inputs);\n    this.checkInputShapeAndType(inputs);\n    outputs = this.mapOutputs(outputs);\n    this.checkOutputs(outputs);\n    const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n    this.resetIntermediateTensors(); // If no outputs are specified, then use the default outputs of the model.\n\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const compilationKey = this.getCompilationKey(inputNodes, outputNodes); // Do nothing if the compiled graph cache contains the input.\n\n    let orderedNodes = this.compiledMap.get(compilationKey);\n\n    if (orderedNodes == null) {\n      orderedNodes = this.compile(inputs, outputNodes);\n      this.compiledMap.set(compilationKey, orderedNodes);\n    }\n\n    const tensorArrayMap = {};\n    const tensorListMap = {};\n    return tidy(() => {\n      const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n      const tensorsMap = Object.assign({}, this.weightMap);\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name);\n        const tensors = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n      });\n      const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n      const intermediateTensorConsumerCount = {};\n\n      for (let i = 0; i < orderedNodes.length; i++) {\n        const node = orderedNodes[i];\n\n        if (!tensorsMap[node.name]) {\n          const tensors = executeOp(node, tensorsMap, context, this._resourceManager);\n\n          if (util.isPromise(tensors)) {\n            throw new Error(`The execution of the op '${node.op}' returned a promise. ` + `Please use model.executeAsync() instead.`);\n          }\n\n          tensorsMap[node.name] = tensors;\n          this.checkTensorForDisposal(node.name, node, tensorsMap, context, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount);\n        }\n      } // dispose the context for the root executor\n\n\n      if (this.parent == null) {\n        context.dispose(tensorsToKeep);\n      }\n\n      return outputs.map(name => getTensor(name, tensorsMap, context));\n    });\n  }\n\n  getFrozenTensorIds(tensorMap) {\n    const ids = [].concat.apply([], Object.keys(tensorMap).map(key => tensorMap[key]).map(tensors => tensors.map(tensor => tensor.id)));\n    return new Set(ids);\n  }\n\n  checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount) {\n    // Skip output nodes and any control flow nodes, since its dependency is\n    // tricky to track correctly.\n    if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n      return;\n    }\n\n    tensorMap[nodeName].forEach(tensor => {\n      if (tensor != null) {\n        intermediateTensorConsumerCount[tensor.id] = (intermediateTensorConsumerCount[tensor.id] || 0) + node.children.length;\n      }\n    });\n    node.inputs.forEach(input => {\n      // Skip any control flow nodes, since its dependency is tricky to track\n      // correctly.\n      if (input.category !== 'control') {\n        const tensors = getTensorsForCurrentContenxt(input.name, tensorMap, context);\n\n        if (tensors != null) {\n          tensors.forEach(tensor => {\n            if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n              const count = intermediateTensorConsumerCount[tensor.id];\n\n              if (count === 1) {\n                if (!this.keepTensorForDebug) {\n                  tensor.dispose();\n                } else {\n                  const [nodeName, index] = getNodeNameAndIndex(node.name, context);\n\n                  if (this.intermediateTensors[nodeName]) {\n                    this.intermediateTensors[nodeName][index] = tensor;\n                  } else {\n                    this.intermediateTensors[nodeName] = [];\n                    this.intermediateTensors[nodeName][index] = tensor;\n                  }\n                }\n\n                delete intermediateTensorConsumerCount[tensor.id];\n              } else if (count != null) {\n                // only intermediate nodes has count set, inputs and weights are\n                // not.\n                intermediateTensorConsumerCount[tensor.id]--;\n              }\n            }\n          });\n        }\n      }\n    });\n  }\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   */\n\n\n  async executeAsync(inputs, outputs) {\n    return this._executeAsync(inputs, outputs);\n  }\n\n  disposeIntermediateTensors() {\n    if (!this.intermediateTensors) {\n      return;\n    }\n\n    Object.keys(this.intermediateTensors).forEach(key => this.intermediateTensors[key].forEach(tensor => tensor.dispose()));\n    this.disposeTensorsMap();\n  }\n\n  disposeTensorsMap() {\n    if (!this.tensorsMap) {\n      return;\n    }\n\n    Object.keys(this.tensorsMap).forEach(key => {\n      const tensorArray = this.tensorsMap[key];\n      tensorArray.forEach(tensor => {\n        if (tensor && !tensor.kept && !tensor.isDisposed && !this.keepIds.has(tensor.id)) {\n          tensor.dispose();\n        }\n      });\n    });\n  }\n\n  getIntermediateTensors() {\n    return this.tensorsMap;\n  }\n\n  resetIntermediateTensors() {\n    for (const key in this.intermediateTensors) {\n      this.intermediateTensors[key].forEach(tensor => tensor.dispose());\n      delete this.intermediateTensors[key];\n    }\n  }\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Optional. Flag for executing a function.\n   * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n   * function execution.\n   * @param tensorArrayMap Optinal global TensorList map by id. Used for\n   * function execution.\n   */\n\n\n  async _executeAsync(inputs, outputs) {\n    let isFunctionExecution = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n    let tensorArrayMap = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};\n    let tensorListMap = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : {};\n\n    if (!isFunctionExecution) {\n      inputs = this.mapInputs(inputs);\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n    } // For model debug.\n\n\n    try {\n      this.keepTensorForDebug = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      console.warn(e.message);\n    }\n\n    this.resetIntermediateTensors();\n    const context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap); // Graph with control flow op requires runtime evaluation of the execution\n    // order, while without control flow the execution order is pre-determined\n    // in the compile method.\n\n    this.tensorsMap = await this.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n    const results = outputs.map(name => getTensor(name, this.tensorsMap, context)); // dispose all the intermediate tensors\n\n    const outputIds = results.map(t => t.id);\n    const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n    this.keepIds = new Set([...outputIds, ...inputIds, ...this.weightIds]);\n\n    if (!this.keepTensorForDebug) {\n      this.disposeTensorsMap();\n    } // dispose the context for the root executor\n\n\n    if (this.parent == null) {\n      context.dispose(this.keepIds);\n    }\n\n    return results;\n  }\n\n  async executeFunctionAsync(inputs, tensorArrayMap, tensorListMap) {\n    const mappedInputs = inputs.reduce((map, tensor, index) => {\n      map[this.inputs[index].name] = tensor;\n      return map;\n    }, {});\n    return this._executeAsync(mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n  }\n  /**\n   * When there are control flow nodes in the graph, the graph execution use\n   * ExecutionContext to keep track of the frames and loop iterators.\n   * @param inputs placeholder tensors for the graph.\n   * @param context the execution context object for current execution.\n   * @param outputNames Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Flag for executing a function.\n   */\n\n\n  async executeWithControlFlow(inputs, context, outputNames, isFunctionExecution) {\n    const names = Object.keys(inputs);\n    const inputNodes = names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]); // If no outputs are specified, then use the default outputs of the model.\n\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const {\n      usedNodes,\n      missingInputs,\n      dynamicNode,\n      syncInputs\n    } = getExecutionSubgraph(inputs, outputNodes, this.weightMap, this._initNodes); // First nodes to execute include inputNodes, weights, and initNodes.\n\n    const stack = [...inputNodes, ...this.graph.weights, ...(this._initNodes || [])].map(node => {\n      return {\n        node,\n        contexts: context.currentContext\n      };\n    });\n    const tensorsMap = Object.assign({}, this.weightMap);\n    Object.keys(inputs).forEach(name => {\n      const [nodeName, index] = parseNodeName(name);\n      const tensors = [];\n      tensors[index] = inputs[name];\n      tensorsMap[nodeName] = tensors;\n    });\n    const intermediateTensorConsumerCount = {};\n    const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n    const added = {};\n\n    while (stack.length > 0) {\n      const promises = this.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n      await Promise.all(promises);\n    }\n\n    if (dynamicNode == null && !isFunctionExecution) {\n      console.warn(`This model execution did not contain any nodes with control flow ` + `or dynamic output shapes. You can use model.execute() instead.`);\n    }\n\n    const missingOutputs = outputNodes.filter(node => !isControlFlow(node) && !getTensor(node.name, tensorsMap, context)).map(node => node.name);\n\n    if (missingOutputs.length > 0) {\n      let alternativeMsg = '';\n\n      if (dynamicNode != null) {\n        alternativeMsg = `Alternatively, to avoid the dynamic ops, use model.execute() ` + `and specify the inputs [${syncInputs}]`;\n      }\n\n      throw new Error(`Cannot compute the outputs [${missingOutputs}] from the provided ` + `inputs [${names}]. Consider providing the following inputs: ` + `[${missingInputs}]. ${alternativeMsg}`);\n    }\n\n    return tensorsMap;\n  }\n\n  processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNames, intermediateTensorConsumerCount, usedNodes) {\n    const promises = [];\n\n    while (stack.length > 0) {\n      const item = stack.pop();\n      context.currentContext = item.contexts;\n      let nodeName = ''; // The tensor of the Enter op with isConstant set should be set\n      // in the parent scope, so it will be available as constant for the\n      // whole loop.\n\n      if (item.node.op === 'Enter' && getParamValue('isConstant', item.node, tensorMap, context)) {\n        [nodeName] = getNodeNameAndIndex(item.node.name, context);\n      } // only process nodes that are not in the tensorMap yet, this include\n      // inputNodes and internal initNodes.\n\n\n      if (tensorMap[item.node.name] == null) {\n        const tensors = executeOp(item.node, tensorMap, context, this._resourceManager);\n\n        if (!nodeName) {\n          [nodeName] = getNodeNameAndIndex(item.node.name, context);\n        }\n\n        const currentContext = context.currentContext;\n\n        if (util.isPromise(tensors)) {\n          promises.push(tensors.then(t => {\n            tensorMap[nodeName] = t;\n            context.currentContext = currentContext;\n            this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n            this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n            return t;\n          }));\n        } else {\n          tensorMap[nodeName] = tensors;\n          this.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n          this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      } else {\n        this.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n      }\n    }\n\n    return promises;\n  }\n\n  processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n    node.children.forEach(childNode => {\n      const [nodeName] = getNodeNameAndIndex(childNode.name, context);\n\n      if (added[nodeName] || !usedNodes.has(childNode.name)) {\n        return;\n      } // Merge op can be pushed if any of its inputs has value.\n\n\n      if (childNode.op === 'Merge') {\n        if (childNode.inputNames.some(name => {\n          return !!getTensor(name, tensorMap, context);\n        })) {\n          added[nodeName] = true;\n          stack.push({\n            contexts: context.currentContext,\n            node: childNode\n          });\n        }\n      } else // Otherwise all inputs must to have value.\n        if (childNode.inputNames.every(name => {\n          return !!getTensor(name, tensorMap, context);\n        })) {\n          added[nodeName] = true;\n          stack.push({\n            contexts: context.currentContext,\n            node: childNode\n          });\n        }\n    });\n  }\n  /**\n   * Releases the memory used by the weight tensors.\n   */\n\n\n  dispose() {\n    Object.keys(this.weightMap).forEach(key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n  }\n\n  checkInputShapeAndType(inputs) {\n    Object.keys(inputs).forEach(name => {\n      const input = inputs[name];\n      const [nodeName] = parseNodeName(name);\n      const node = this.graph.nodes[nodeName];\n\n      if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n        const shape = node.attrParams['shape'].value;\n        const match = shape.length === input.shape.length && input.shape.every((dim, index) => shape[index] === -1 || shape[index] === dim);\n        util.assert(match, () => `The shape of dict['${node.name}'] provided in ` + `model.execute(dict) must be [${shape}], but was ` + `[${input.shape}]`);\n      }\n\n      if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n        util.assert(input.dtype === node.attrParams['dtype'].value, () => `The dtype of dict['${node.name}'] provided in ` + `model.execute(dict) must be ` + `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n      }\n    });\n  }\n\n  mapInputs(inputs) {\n    const result = {};\n\n    for (const inputName in inputs) {\n      if (this._signature != null && this._signature.inputs != null && this._signature.inputs[inputName] != null) {\n        const tensor = this._signature.inputs[inputName];\n        result[tensor.name] = inputs[inputName];\n      } else {\n        result[inputName] = inputs[inputName];\n      }\n    }\n\n    return result;\n  }\n\n  checkInputs(inputs) {\n    const notInGraph = Object.keys(inputs).filter(name => {\n      const [nodeName] = parseNodeName(name);\n      return this.graph.nodes[nodeName] == null;\n    });\n\n    if (notInGraph.length > 0) {\n      throw new Error(`The dict provided in model.execute(dict) has ` + `keys: [${notInGraph}] that are not part of graph`);\n    }\n  }\n\n  mapOutputs(outputs) {\n    return outputs.map(name => {\n      if (this._signature != null && this._signature.outputs != null && this._signature.outputs[name] != null) {\n        const tensor = this._signature.outputs[name];\n        return tensor.name;\n      }\n\n      return name;\n    }, {});\n  }\n\n  checkOutputs(outputs) {\n    outputs.forEach(name => {\n      const [normalizedName] = parseNodeName(name);\n\n      if (!this.graph.nodes[normalizedName]) {\n        throw new Error(`The output '${name}' is not found in the graph`);\n      }\n    });\n  }\n\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAkBA,GAAlB,EAA+CC,IAA/C,EAAqDC,IAArD,QAAgE,uBAAhE;AAIA,SAAQC,mBAAR,EAA6BC,aAA7B,EAA4CC,SAA5C,EAAuDC,4BAAvD,EAAqFC,aAArF,QAAyG,+BAAzG;AACA,SAAQC,SAAR,QAAwB,kCAAxB;AAGA,SAAQC,gBAAR,QAAqD,qBAArD;AACA,SAAQC,oBAAR,EAA8BC,0BAA9B,EAA0DC,aAA1D,QAA8E,kBAA9E;AASA,OAAM,MAAOC,aAAP,CAAoB;AA2FxB;;;;;;;;AAQAC,cAAoBC,KAApB,EAA0CC,MAA1C,EAAgE;AAA5C;AAAsB;AAlGlC,uBAAmC,IAAIC,GAAJ,EAAnC;AACA,sBAA8B,EAA9B;AAMA,qBAAY,GAAZ;AACA,sBAAqC,EAArC;AACA,gCAA0D,EAA1D;AAEA,+BAAuC,EAAvC;AAGA,8BAAqB,KAArB;AAqFN,SAAKC,QAAL,GAAgBH,KAAK,CAACI,OAAtB;AACA,SAAKC,OAAL,GAAeL,KAAK,CAACM,MAArB;AACA,SAAKC,UAAL,GAAkBP,KAAK,CAACQ,SAAxB;AACA,SAAKC,UAAL,GAAkBT,KAAK,CAACU,SAAxB;AACA,SAAKC,UAAL,GAAkBX,KAAK,CAACY,SAAxB,CAL8D,CAM9D;;AACA,QAAIZ,KAAK,CAACY,SAAN,IAAmB,IAAvB,EAA6B;AAC3BC,YAAM,CAACC,IAAP,CAAYd,KAAK,CAACY,SAAlB,EAA6BG,OAA7B,CAAqCC,IAAI,IAAG;AAC1C,aAAKC,oBAAL,CAA0BD,IAA1B,IACI,IAAIlB,aAAJ,CAAkBE,KAAK,CAACY,SAAN,CAAgBI,IAAhB,CAAlB,EAAyC,IAAzC,CADJ;AAED,OAHD;AAID;AACF;;AA/FY,MAATE,SAAS;AACX,WAAO,KAAKjB,MAAL,GAAc,KAAKA,MAAL,CAAYiB,SAA1B,GAAsC,KAAKC,UAAlD;AACD;;AAEsB,MAAnBC,mBAAmB;AACrB,WAAO,KAAKnB,MAAL,GAAc,KAAKA,MAAL,CAAYmB,mBAA1B,GACc,KAAKH,oBAD1B;AAED;;AAEY,MAATI,SAAS;AACX,WAAO,KAAKpB,MAAL,GAAc,KAAKA,MAAL,CAAYoB,SAA1B,GAAsC,KAAKC,UAAlD;AACD;;AAEY,MAATD,SAAS,CAACA,SAAD,EAA2B;AACtC,UAAMH,SAAS,GAAGL,MAAM,CAACC,IAAP,CAAYO,SAAZ,EAAuBE,GAAvB,CACdC,GAAG,IAAIH,SAAS,CAACG,GAAD,CAAT,CAAeD,GAAf,CAAmBE,MAAM,IAAIA,MAAM,CAACC,EAApC,CADO,CAAlB;AAEA,SAAKP,UAAL,GAAkB,GAAGQ,MAAH,CAAU,GAAGT,SAAb,CAAlB;AACA,SAAKI,UAAL,GAAkBD,SAAlB;AACD;AAED;;;;;;AAImB,MAAfO,eAAe,CAACA,eAAD,EAAiC;AAClD,SAAKC,gBAAL,GAAwBD,eAAxB;AACD;;AAES,MAANtB,MAAM;AACR,WAAO,KAAKD,OAAL,CAAakB,GAAb,CAAiBO,IAAI,IAAG;AAC7B,aAAO;AACLd,YAAI,EAAEc,IAAI,CAACd,IADN;AAELe,aAAK,EAAED,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC,SAJC;AAKLC,aAAK,EAAEL,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC;AAPC,OAAP;AASD,KAVM,CAAP;AAWD;;AAEU,MAAP9B,OAAO;AACT,WAAO,KAAKD,QAAL,CAAcoB,GAAd,CAAkBO,IAAI,IAAG;AAC9B,aAAO;AACLd,YAAI,EAAEc,IAAI,CAACd,IADN;AAELe,aAAK,EAAED,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC,SAJC;AAKLC,aAAK,EAAEL,IAAI,CAACE,UAAL,CAAgB,OAAhB,IACHF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KADtB,GAEHC;AAPC,OAAP;AASD,KAVM,CAAP;AAWD;;AAEa,MAAVE,UAAU;AACZ,WAAO,KAAK/B,OAAL,CAAakB,GAAb,CAAiBO,IAAI,IAAIA,IAAI,CAACO,YAAL,IAAqBP,IAAI,CAACd,IAAnD,CAAP;AACD;;AAEc,MAAXsB,WAAW;AACb,WAAO,KAAKnC,QAAL,CAAcoB,GAAd,CAAmBO,IAAD,IAAS;AAChC,YAAMd,IAAI,GAAGc,IAAI,CAACO,YAAL,IAAqBP,IAAI,CAACd,IAAvC;AACA,aAAOc,IAAI,CAACS,aAAL,GAAsB,GAAGvB,IAAI,IAAIc,IAAI,CAACS,aAAa,EAAnD,GAAyDvB,IAAhE;AACD,KAHM,CAAP;AAID;;AAEY,MAATJ,SAAS;AACX,WAAOC,MAAM,CAACC,IAAP,CAAY,KAAKH,UAAjB,EAA6B6B,MAA7B,CAAoC,CAACjB,GAAD,EAAMC,GAAN,KAAa;AACtDD,SAAG,CAACC,GAAD,CAAH,GAAW,KAAKb,UAAL,CAAgBa,GAAhB,EAAqBd,SAAhC;AACA,aAAOa,GAAP;AACD,KAHM,EAGJ,EAHI,CAAP;AAID;;AAyBOkB,mBAAiB,CAACnC,MAAD,EAAiBF,OAAjB,EAAgC;AACvD,UAAMsC,YAAY,GAAGpC,MAAM,CAACiB,GAAP,CAAWO,IAAI,IAAIA,IAAI,CAACd,IAAxB,EAA8B2B,IAA9B,EAArB;AACA,UAAMC,aAAa,GAAGxC,OAAO,CAACmB,GAAR,CAAYO,IAAI,IAAIA,IAAI,CAACd,IAAzB,EAA+B2B,IAA/B,EAAtB;AACA,WAAOD,YAAY,CAACG,IAAb,CAAkB,KAAKC,SAAvB,IAAoC,IAApC,GACHF,aAAa,CAACC,IAAd,CAAmB,KAAKC,SAAxB,CADJ;AAED;AAED;;;;;;AAIQC,SAAO,CAACzC,MAAD,EAAyBF,OAAzB,EAAwC;AACrD,UAAM4C,aAAa,GACfrD,oBAAoB,CAACW,MAAD,EAASF,OAAT,EAAkB,KAAKiB,SAAvB,EAAkC,KAAKd,UAAvC,CADxB;AAEA,UAAM;AAAC0C,mBAAD;AAAgBC,iBAAhB;AAA6BC;AAA7B,QAA2CH,aAAjD;;AACA,QAAIE,WAAW,IAAI,IAAnB,EAAyB;AACvB,YAAM,IAAIE,KAAJ,CACF,qCAAqCF,WAAW,CAAClC,IAAI,eAArD,GACA,mBAAmBkC,WAAW,CAACG,EAAE,gBADjC,GAEA,4DAFA,GAGA,oCAAoCF,UAAU,GAJ5C,CAAN;AAKD;;AAED,QAAIF,aAAa,CAACK,MAAd,GAAuB,CAA3B,EAA8B;AAC5B,YAAMC,QAAQ,GAAGnD,OAAO,CAACmB,GAAR,CAAYiC,CAAC,IAAIA,CAAC,CAACxC,IAAnB,CAAjB;AACA,YAAMyC,OAAO,GAAG5C,MAAM,CAACC,IAAP,CAAYR,MAAZ,CAAhB;AACA,YAAM,IAAI8C,KAAJ,CACF,+BAA+BG,QAAQ,6BAAvC,GACA,IAAIE,OAAO,qCAAqCR,aAAa,GAF3D,CAAN;AAGD;;AAED,WAAOrD,0BAA0B,CAC7B,KAAKI,KADwB,EACjB,KAAKqB,SADY,EACD2B,aADC,CAAjC;AAED;AAED;;;;;;;;;;;AASAU,SAAO,CAACpD,MAAD,EAAyBF,OAAzB,EAA2C;AAChDE,UAAM,GAAG,KAAKqD,SAAL,CAAerD,MAAf,CAAT;AACA,UAAMsD,KAAK,GAAG/C,MAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoBqC,IAApB,EAAd;AACA,SAAKkB,WAAL,CAAiBvD,MAAjB;AACA,SAAKwD,sBAAL,CAA4BxD,MAA5B;AACAF,WAAO,GAAG,KAAK2D,UAAL,CAAgB3D,OAAhB,CAAV;AACA,SAAK4D,YAAL,CAAkB5D,OAAlB;AACA,UAAMgC,UAAU,GACZwB,KAAK,CAACrC,GAAN,CAAUP,IAAI,IAAI,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBzE,aAAa,CAACwB,IAAD,CAAb,CAAoB,CAApB,CAAjB,CAAlB,CADJ;AAEA,UAAMkD,eAAe,GAAG9D,OAAO,CAACmB,GAAR,CAAYP,IAAI,IAAIxB,aAAa,CAACwB,IAAD,CAAb,CAAoB,CAApB,CAApB,CAAxB;AACA,QAAIsB,WAAW,GAAG4B,eAAe,CAAC3C,GAAhB,CAAoBP,IAAI,IAAI,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBjD,IAAjB,CAA5B,CAAlB;AACA,SAAKmD,wBAAL,GAXgD,CAYhD;;AACA,QAAI7B,WAAW,CAACgB,MAAZ,KAAuB,CAA3B,EAA8B;AAC5BhB,iBAAW,GAAG,KAAKnC,QAAnB;AACD;;AAED,UAAMiE,cAAc,GAAG,KAAK3B,iBAAL,CAAuBL,UAAvB,EAAmCE,WAAnC,CAAvB,CAjBgD,CAmBhD;;AACA,QAAI+B,YAAY,GAAG,KAAKC,WAAL,CAAiBC,GAAjB,CAAqBH,cAArB,CAAnB;;AACA,QAAIC,YAAY,IAAI,IAApB,EAA0B;AACxBA,kBAAY,GAAG,KAAKtB,OAAL,CAAazC,MAAb,EAAqBgC,WAArB,CAAf;AACA,WAAKgC,WAAL,CAAiBE,GAAjB,CAAqBJ,cAArB,EAAqCC,YAArC;AACD;;AAED,UAAMI,cAAc,GAAmB,EAAvC;AACA,UAAMC,aAAa,GAAkB,EAArC;AAEA,WAAOxF,IAAI,CAAC,MAAK;AACf,YAAMyF,OAAO,GAAG,IAAIjF,gBAAJ,CACZ,KAAK2B,SADO,EACIoD,cADJ,EACoBC,aADpB,EAEZ,KAAKtD,mBAFO,CAAhB;AAGA,YAAMwD,UAAU,qBAAwB,KAAKvD,SAA7B,CAAhB;AAEAR,YAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoBS,OAApB,CAA4BC,IAAI,IAAG;AACjC,cAAM,CAAC6D,QAAD,EAAWC,KAAX,IAAoBtF,aAAa,CAACwB,IAAD,CAAvC;AACA,cAAM+D,OAAO,GAAa,EAA1B;AACAA,eAAO,CAACD,KAAD,CAAP,GAAiBxE,MAAM,CAACU,IAAD,CAAvB;AACA4D,kBAAU,CAACC,QAAD,CAAV,GAAuBE,OAAvB;AACD,OALD;AAOA,YAAMC,aAAa,GAAG,KAAKC,kBAAL,CAAwBL,UAAxB,CAAtB;AACA,YAAMM,+BAA+B,GAA4B,EAAjE;;AACA,WAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGd,YAAY,CAACf,MAAjC,EAAyC6B,CAAC,EAA1C,EAA8C;AAC5C,cAAMrD,IAAI,GAAGuC,YAAY,CAACc,CAAD,CAAzB;;AACA,YAAI,CAACP,UAAU,CAAC9C,IAAI,CAACd,IAAN,CAAf,EAA4B;AAC1B,gBAAM+D,OAAO,GACTtF,SAAS,CAACqC,IAAD,EAAO8C,UAAP,EAAmBD,OAAnB,EAA4B,KAAK9C,gBAAjC,CADb;;AAGA,cAAI1C,IAAI,CAACiG,SAAL,CAAeL,OAAf,CAAJ,EAA6B;AAC3B,kBAAM,IAAI3B,KAAJ,CACF,4BAA4BtB,IAAI,CAACuB,EAAE,wBAAnC,GACA,0CAFE,CAAN;AAGD;;AACDuB,oBAAU,CAAC9C,IAAI,CAACd,IAAN,CAAV,GAAwB+D,OAAxB;AACA,eAAKM,sBAAL,CACIvD,IAAI,CAACd,IADT,EACec,IADf,EACqB8C,UADrB,EACiCD,OADjC,EAC0CK,aAD1C,EAEId,eAFJ,EAEqBgB,+BAFrB;AAGD;AACF,OA/Bc,CAgCf;;;AACA,UAAI,KAAKjF,MAAL,IAAe,IAAnB,EAAyB;AACvB0E,eAAO,CAACW,OAAR,CAAgBN,aAAhB;AACD;;AACD,aAAO5E,OAAO,CAACmB,GAAR,CAAYP,IAAI,IAAI1B,SAAS,CAAC0B,IAAD,EAAO4D,UAAP,EAAmBD,OAAnB,CAA7B,CAAP;AACD,KArCU,CAAX;AAsCD;;AAEOM,oBAAkB,CAACM,SAAD,EAA2B;AACnD,UAAMC,GAAG,GAAG,GAAG7D,MAAH,CAAU8D,KAAV,CACR,EADQ,EAER5E,MAAM,CAACC,IAAP,CAAYyE,SAAZ,EACKhE,GADL,CACSC,GAAG,IAAI+D,SAAS,CAAC/D,GAAD,CADzB,EAEKD,GAFL,CAESwD,OAAO,IAAIA,OAAO,CAACxD,GAAR,CAAYE,MAAM,IAAIA,MAAM,CAACC,EAA7B,CAFpB,CAFQ,CAAZ;AAKA,WAAO,IAAIgE,GAAJ,CAAQF,GAAR,CAAP;AACD;;AACOH,wBAAsB,CAC1BR,QAD0B,EACR/C,IADQ,EACIyD,SADJ,EAE1BZ,OAF0B,EAECK,aAFD,EAG1BW,WAH0B,EAI1BT,+BAJ0B,EAI8B;AAC1D;AACA;AACA,QAAIpD,IAAI,CAAC8D,QAAL,KAAkB,SAAlB,IAA+BD,WAAW,CAACE,OAAZ,CAAoBhB,QAApB,MAAkC,CAAC,CAAtE,EAAyE;AACvE;AACD;;AAEDU,aAAS,CAACV,QAAD,CAAT,CAAoB9D,OAApB,CAA4BU,MAAM,IAAG;AACnC,UAAIA,MAAM,IAAI,IAAd,EAAoB;AAClByD,uCAA+B,CAACzD,MAAM,CAACC,EAAR,CAA/B,GACI,CAACwD,+BAA+B,CAACzD,MAAM,CAACC,EAAR,CAA/B,IAA8C,CAA/C,IACAI,IAAI,CAACgE,QAAL,CAAcxC,MAFlB;AAGD;AACF,KAND;AAOAxB,QAAI,CAACxB,MAAL,CAAYS,OAAZ,CAAoBgF,KAAK,IAAG;AAC1B;AACA;AACA,UAAIA,KAAK,CAACH,QAAN,KAAmB,SAAvB,EAAkC;AAChC,cAAMb,OAAO,GACTxF,4BAA4B,CAACwG,KAAK,CAAC/E,IAAP,EAAauE,SAAb,EAAwBZ,OAAxB,CADhC;;AAEA,YAAII,OAAO,IAAI,IAAf,EAAqB;AACnBA,iBAAO,CAAChE,OAAR,CAAgBU,MAAM,IAAG;AACvB,gBAAIA,MAAM,IAAI,CAACA,MAAM,CAACuE,IAAlB,IAA0B,CAAChB,aAAa,CAACiB,GAAd,CAAkBxE,MAAM,CAACC,EAAzB,CAA/B,EAA6D;AAC3D,oBAAMwE,KAAK,GAAGhB,+BAA+B,CAACzD,MAAM,CAACC,EAAR,CAA7C;;AACA,kBAAIwE,KAAK,KAAK,CAAd,EAAiB;AACf,oBAAI,CAAC,KAAKC,kBAAV,EAA8B;AAC5B1E,wBAAM,CAAC6D,OAAP;AACD,iBAFD,MAEO;AACL,wBAAM,CAACT,QAAD,EAAWC,KAAX,IACF1F,mBAAmB,CAAC0C,IAAI,CAACd,IAAN,EAAY2D,OAAZ,CADvB;;AAEA,sBAAI,KAAKyB,mBAAL,CAAyBvB,QAAzB,CAAJ,EAAwC;AACtC,yBAAKuB,mBAAL,CAAyBvB,QAAzB,EAAmCC,KAAnC,IAA4CrD,MAA5C;AACD,mBAFD,MAEO;AACL,yBAAK2E,mBAAL,CAAyBvB,QAAzB,IAAqC,EAArC;AACA,yBAAKuB,mBAAL,CAAyBvB,QAAzB,EAAmCC,KAAnC,IAA4CrD,MAA5C;AACD;AACF;;AACD,uBAAOyD,+BAA+B,CAACzD,MAAM,CAACC,EAAR,CAAtC;AACD,eAdD,MAcO,IAAIwE,KAAK,IAAI,IAAb,EAAmB;AACxB;AACA;AACAhB,+CAA+B,CAACzD,MAAM,CAACC,EAAR,CAA/B;AACD;AACF;AACF,WAvBD;AAwBD;AACF;AACF,KAjCD;AAkCD;AAED;;;;;;;;;;;AASkB,QAAZ2E,YAAY,CAAC/F,MAAD,EAAyBF,OAAzB,EAA2C;AAE3D,WAAO,KAAKkG,aAAL,CAAmBhG,MAAnB,EAA2BF,OAA3B,CAAP;AACD;;AAEDmG,4BAA0B;AACxB,QAAI,CAAC,KAAKH,mBAAV,EAA+B;AAC7B;AACD;;AACDvF,UAAM,CAACC,IAAP,CAAY,KAAKsF,mBAAjB,EACKrF,OADL,CAEQS,GAAG,IAAI,KAAK4E,mBAAL,CAAyB5E,GAAzB,EAA8BT,OAA9B,CACHU,MAAM,IAAIA,MAAM,CAAC6D,OAAP,EADP,CAFf;AAIA,SAAKkB,iBAAL;AACD;;AAEOA,mBAAiB;AACvB,QAAI,CAAC,KAAK5B,UAAV,EAAsB;AACpB;AACD;;AACD/D,UAAM,CAACC,IAAP,CAAY,KAAK8D,UAAjB,EAA6B7D,OAA7B,CAAqCS,GAAG,IAAG;AACzC,YAAMiF,WAAW,GAAG,KAAK7B,UAAL,CAAgBpD,GAAhB,CAApB;AACAiF,iBAAW,CAAC1F,OAAZ,CAAoBU,MAAM,IAAG;AAC3B,YAAIA,MAAM,IAAI,CAACA,MAAM,CAACuE,IAAlB,IAA0B,CAACvE,MAAM,CAACiF,UAAlC,IACA,CAAC,KAAKC,OAAL,CAAaV,GAAb,CAAiBxE,MAAM,CAACC,EAAxB,CADL,EACkC;AAChCD,gBAAM,CAAC6D,OAAP;AACD;AACF,OALD;AAMD,KARD;AASD;;AAEDsB,wBAAsB;AACpB,WAAO,KAAKhC,UAAZ;AACD;;AAEOT,0BAAwB;AAC9B,SAAK,MAAM3C,GAAX,IAAkB,KAAK4E,mBAAvB,EAA4C;AAC1C,WAAKA,mBAAL,CAAyB5E,GAAzB,EAA8BT,OAA9B,CAAsCU,MAAM,IAAIA,MAAM,CAAC6D,OAAP,EAAhD;AACA,aAAO,KAAKc,mBAAL,CAAyB5E,GAAzB,CAAP;AACD;AACF;AAED;;;;;;;;;;;;;;;;AAc2B,QAAb8E,aAAa,CACvBhG,MADuB,EACCF,OADD,EAGU;AAAA,QAFWyG,mBAEX,uEAFiC,KAEjC;AAAA,QADjCpC,cACiC,uEADA,EACA;AAAA,QAAjCC,aAAiC,uEAAF,EAAE;;AACnC,QAAI,CAACmC,mBAAL,EAA0B;AACxBvG,YAAM,GAAG,KAAKqD,SAAL,CAAerD,MAAf,CAAT;AACA,WAAKuD,WAAL,CAAiBvD,MAAjB;AACA,WAAKwD,sBAAL,CAA4BxD,MAA5B;AACAF,aAAO,GAAG,KAAK2D,UAAL,CAAgB3D,OAAhB,CAAV;AACA,WAAK4D,YAAL,CAAkB5D,OAAlB;AACD,KAPkC,CASnC;;;AACA,QAAI;AACF,WAAK+F,kBAAL,GAA0BlH,GAAG,GAAG6H,OAAN,CAAc,2BAAd,CAA1B;AACD,KAFD,CAEE,OAAOC,CAAP,EAAU;AACVC,aAAO,CAACC,IAAR,CAAaF,CAAC,CAACG,OAAf;AACD;;AACD,SAAK/C,wBAAL;AAEA,UAAMQ,OAAO,GAAG,IAAIjF,gBAAJ,CACZ,KAAK2B,SADO,EACIoD,cADJ,EACoBC,aADpB,EAEZ,KAAKtD,mBAFO,CAAhB,CAjBmC,CAqBnC;AACA;AACA;;AACA,SAAKwD,UAAL,GAAkB,MAAM,KAAKuC,sBAAL,CACpB7G,MADoB,EACZqE,OADY,EACHvE,OADG,EACMyG,mBADN,CAAxB;AAEA,UAAMO,OAAO,GACThH,OAAO,CAACmB,GAAR,CAAYP,IAAI,IAAI1B,SAAS,CAAC0B,IAAD,EAAO,KAAK4D,UAAZ,EAAwBD,OAAxB,CAA7B,CADJ,CA1BmC,CA6BnC;;AACA,UAAM0C,SAAS,GAAGD,OAAO,CAAC7F,GAAR,CAAY+F,CAAC,IAAIA,CAAC,CAAC5F,EAAnB,CAAlB;AACA,UAAM6F,QAAQ,GAAG1G,MAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoBiB,GAApB,CAAwBP,IAAI,IAAIV,MAAM,CAACU,IAAD,CAAN,CAAaU,EAA7C,CAAjB;AACA,SAAKiF,OAAL,GACI,IAAIjB,GAAJ,CAAgB,CAAC,GAAG2B,SAAJ,EAAe,GAAGE,QAAlB,EAA4B,GAAG,KAAKrG,SAApC,CAAhB,CADJ;;AAEA,QAAI,CAAC,KAAKiF,kBAAV,EAA8B;AAC5B,WAAKK,iBAAL;AACD,KApCkC,CAsCnC;;;AACA,QAAI,KAAKvG,MAAL,IAAe,IAAnB,EAAyB;AACvB0E,aAAO,CAACW,OAAR,CAAgB,KAAKqB,OAArB;AACD;;AAED,WAAOS,OAAP;AACD;;AAEyB,QAApBI,oBAAoB,CACtBlH,MADsB,EACJmE,cADI,EAEtBC,aAFsB,EAEM;AAC9B,UAAM+C,YAAY,GAAGnH,MAAM,CAACkC,MAAP,CAAc,CAACjB,GAAD,EAAME,MAAN,EAAcqD,KAAd,KAAuB;AACxDvD,SAAG,CAAC,KAAKjB,MAAL,CAAYwE,KAAZ,EAAmB9D,IAApB,CAAH,GAA+BS,MAA/B;AACA,aAAOF,GAAP;AACD,KAHoB,EAGlB,EAHkB,CAArB;AAKA,WAAO,KAAK+E,aAAL,CACHmB,YADG,EACW,KAAKnF,WADhB,EAC6B,IAD7B,EACmCmC,cADnC,EACmDC,aADnD,CAAP;AAED;AAED;;;;;;;;;;;;;AAWoC,QAAtByC,sBAAsB,CAChC7G,MADgC,EACRqE,OADQ,EACmBgB,WADnB,EAEhCkB,mBAFgC,EAEH;AAC/B,UAAMjD,KAAK,GAAG/C,MAAM,CAACC,IAAP,CAAYR,MAAZ,CAAd;AACA,UAAM8B,UAAU,GACZwB,KAAK,CAACrC,GAAN,CAAUP,IAAI,IAAI,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBzE,aAAa,CAACwB,IAAD,CAAb,CAAoB,CAApB,CAAjB,CAAlB,CADJ;AAEA,UAAMkD,eAAe,GAAGyB,WAAW,CAACpE,GAAZ,CAAgBP,IAAI,IAAIxB,aAAa,CAACwB,IAAD,CAAb,CAAoB,CAApB,CAAxB,CAAxB;AACA,QAAIsB,WAAW,GAAG4B,eAAe,CAAC3C,GAAhB,CAAoBP,IAAI,IAAI,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBjD,IAAjB,CAA5B,CAAlB,CAL+B,CAO/B;;AACA,QAAIsB,WAAW,CAACgB,MAAZ,KAAuB,CAA3B,EAA8B;AAC5BhB,iBAAW,GAAG,KAAKnC,QAAnB;AACD;;AAED,UAAM;AAACuH,eAAD;AAAYzE,mBAAZ;AAA2BC,iBAA3B;AAAwCC;AAAxC,QACFxD,oBAAoB,CAChBW,MADgB,EACRgC,WADQ,EACK,KAAKjB,SADV,EACqB,KAAKd,UAD1B,CADxB,CAZ+B,CAgB/B;;AACA,UAAMoH,KAAK,GAAuB,CAChC,GAAGvF,UAD6B,EACjB,GAAG,KAAKpC,KAAL,CAAW4H,OADG,EACM,IAAI,KAAKrH,UAAL,IAAmB,EAAvB,CADN,EAEhCgB,GAFgC,CAE5BO,IAAI,IAAG;AACX,aAAO;AAACA,YAAD;AAAO+F,gBAAQ,EAAElD,OAAO,CAACmD;AAAzB,OAAP;AACD,KAJiC,CAAlC;AAKA,UAAMlD,UAAU,qBAAwB,KAAKvD,SAA7B,CAAhB;AACAR,UAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoBS,OAApB,CAA4BC,IAAI,IAAG;AACjC,YAAM,CAAC6D,QAAD,EAAWC,KAAX,IAAoBtF,aAAa,CAACwB,IAAD,CAAvC;AACA,YAAM+D,OAAO,GAAa,EAA1B;AACAA,aAAO,CAACD,KAAD,CAAP,GAAiBxE,MAAM,CAACU,IAAD,CAAvB;AACA4D,gBAAU,CAACC,QAAD,CAAV,GAAuBE,OAAvB;AACD,KALD;AAMA,UAAMG,+BAA+B,GAA4B,EAAjE;AACA,UAAMF,aAAa,GAAG,KAAKC,kBAAL,CAAwBL,UAAxB,CAAtB;AACA,UAAMmD,KAAK,GAA6B,EAAxC;;AACA,WAAOJ,KAAK,CAACrE,MAAN,GAAe,CAAtB,EAAyB;AACvB,YAAM0E,QAAQ,GAAG,KAAKC,YAAL,CACb7F,UADa,EACDuF,KADC,EACMhD,OADN,EACeC,UADf,EAC2BmD,KAD3B,EACkC/C,aADlC,EAEbd,eAFa,EAEIgB,+BAFJ,EAEqCwC,SAFrC,CAAjB;AAGA,YAAMQ,OAAO,CAACC,GAAR,CAAYH,QAAZ,CAAN;AACD;;AACD,QAAI9E,WAAW,IAAI,IAAf,IAAuB,CAAC2D,mBAA5B,EAAiD;AAC/CG,aAAO,CAACC,IAAR,CACI,sEACA,gEAFJ;AAGD;;AACD,UAAMmB,cAAc,GAChB9F,WAAW,CACN+F,MADL,CAEQvG,IAAI,IAAI,CAACjC,aAAa,CAACiC,IAAD,CAAd,IACJ,CAACxC,SAAS,CAACwC,IAAI,CAACd,IAAN,EAAY4D,UAAZ,EAAwBD,OAAxB,CAHtB,EAIKpD,GAJL,CAISO,IAAI,IAAIA,IAAI,CAACd,IAJtB,CADJ;;AAMA,QAAIoH,cAAc,CAAC9E,MAAf,GAAwB,CAA5B,EAA+B;AAC7B,UAAIgF,cAAc,GAAG,EAArB;;AACA,UAAIpF,WAAW,IAAI,IAAnB,EAAyB;AACvBoF,sBAAc,GACV,kEACA,2BAA2BnF,UAAU,GAFzC;AAGD;;AACD,YAAM,IAAIC,KAAJ,CACF,+BAA+BgF,cAAc,sBAA7C,GACA,WAAWxE,KAAK,8CADhB,GAEA,IAAIX,aAAa,MAAMqF,cAAc,EAHnC,CAAN;AAID;;AACD,WAAO1D,UAAP;AACD;;AAEOqD,cAAY,CAChB7F,UADgB,EACIuF,KADJ,EAC+BhD,OAD/B,EAEhBY,SAFgB,EAEYwC,KAFZ,EAGhB/C,aAHgB,EAGYW,WAHZ,EAIhBT,+BAJgB,EAKhBwC,SALgB,EAKM;AACxB,UAAMM,QAAQ,GAA6B,EAA3C;;AACA,WAAOL,KAAK,CAACrE,MAAN,GAAe,CAAtB,EAAyB;AACvB,YAAMiF,IAAI,GAAGZ,KAAK,CAACa,GAAN,EAAb;AACA7D,aAAO,CAACmD,cAAR,GAAyBS,IAAI,CAACV,QAA9B;AACA,UAAIhD,QAAQ,GAAG,EAAf,CAHuB,CAIvB;AACA;AACA;;AACA,UAAI0D,IAAI,CAACzG,IAAL,CAAUuB,EAAV,KAAiB,OAAjB,IACAhE,aAAa,CAAC,YAAD,EAAekJ,IAAI,CAACzG,IAApB,EAA0ByD,SAA1B,EAAqCZ,OAArC,CADjB,EACgE;AAC9D,SAACE,QAAD,IAAazF,mBAAmB,CAACmJ,IAAI,CAACzG,IAAL,CAAUd,IAAX,EAAiB2D,OAAjB,CAAhC;AACD,OAVsB,CAYvB;AACA;;;AACA,UAAIY,SAAS,CAACgD,IAAI,CAACzG,IAAL,CAAUd,IAAX,CAAT,IAA6B,IAAjC,EAAuC;AACrC,cAAM+D,OAAO,GACTtF,SAAS,CAAC8I,IAAI,CAACzG,IAAN,EAAYyD,SAAZ,EAAuBZ,OAAvB,EAAgC,KAAK9C,gBAArC,CADb;;AAEA,YAAI,CAACgD,QAAL,EAAe;AACb,WAACA,QAAD,IAAazF,mBAAmB,CAACmJ,IAAI,CAACzG,IAAL,CAAUd,IAAX,EAAiB2D,OAAjB,CAAhC;AACD;;AACD,cAAMmD,cAAc,GAAGnD,OAAO,CAACmD,cAA/B;;AACA,YAAI3I,IAAI,CAACiG,SAAL,CAAeL,OAAf,CAAJ,EAA6B;AAC3BiD,kBAAQ,CAACS,IAAT,CAAc1D,OAAO,CAAC2D,IAAR,CAAapB,CAAC,IAAG;AAC7B/B,qBAAS,CAACV,QAAD,CAAT,GAAsByC,CAAtB;AACA3C,mBAAO,CAACmD,cAAR,GAAyBA,cAAzB;AACA,iBAAKzC,sBAAL,CACIR,QADJ,EACc0D,IAAI,CAACzG,IADnB,EACyByD,SADzB,EACoCZ,OADpC,EAC6CK,aAD7C,EAEIW,WAFJ,EAEiBT,+BAFjB;AAGA,iBAAKyD,iBAAL,CACIJ,IAAI,CAACzG,IADT,EACe6F,KADf,EACsBhD,OADtB,EAC+BY,SAD/B,EAC0CwC,KAD1C,EACiDL,SADjD;AAEA,mBAAOJ,CAAP;AACD,WATa,CAAd;AAUD,SAXD,MAWO;AACL/B,mBAAS,CAACV,QAAD,CAAT,GAAsBE,OAAtB;AACA,eAAKM,sBAAL,CACIR,QADJ,EACc0D,IAAI,CAACzG,IADnB,EACyByD,SADzB,EACoCZ,OADpC,EAC6CK,aAD7C,EAEIW,WAFJ,EAEiBT,+BAFjB;AAGA,eAAKyD,iBAAL,CACIJ,IAAI,CAACzG,IADT,EACe6F,KADf,EACsBhD,OADtB,EAC+BY,SAD/B,EAC0CwC,KAD1C,EACiDL,SADjD;AAED;AACF,OA1BD,MA0BO;AACL,aAAKiB,iBAAL,CACIJ,IAAI,CAACzG,IADT,EACe6F,KADf,EACsBhD,OADtB,EAC+BY,SAD/B,EAC0CwC,KAD1C,EACiDL,SADjD;AAED;AACF;;AACD,WAAOM,QAAP;AACD;;AAEOW,mBAAiB,CACrB7G,IADqB,EACT6F,KADS,EACkBhD,OADlB,EAErBY,SAFqB,EAEOwC,KAFP,EAGrBL,SAHqB,EAGC;AACxB5F,QAAI,CAACgE,QAAL,CAAc/E,OAAd,CAAuB6H,SAAD,IAAc;AAClC,YAAM,CAAC/D,QAAD,IAAezF,mBAAmB,CAACwJ,SAAS,CAAC5H,IAAX,EAAiB2D,OAAjB,CAAxC;;AACA,UAAIoD,KAAK,CAAClD,QAAD,CAAL,IAAmB,CAAC6C,SAAS,CAACzB,GAAV,CAAc2C,SAAS,CAAC5H,IAAxB,CAAxB,EAAuD;AACrD;AACD,OAJiC,CAKlC;;;AACA,UAAI4H,SAAS,CAACvF,EAAV,KAAiB,OAArB,EAA8B;AAC5B,YAAIuF,SAAS,CAACC,UAAV,CAAqBC,IAArB,CAA0B9H,IAAI,IAAG;AAC/B,iBAAO,CAAC,CAAC1B,SAAS,CAAC0B,IAAD,EAAOuE,SAAP,EAAkBZ,OAAlB,CAAlB;AACD,SAFD,CAAJ,EAEQ;AACNoD,eAAK,CAAClD,QAAD,CAAL,GAAkB,IAAlB;AACA8C,eAAK,CAACc,IAAN,CAAW;AAACZ,oBAAQ,EAAElD,OAAO,CAACmD,cAAnB;AAAmChG,gBAAI,EAAE8G;AAAzC,WAAX;AACD;AACF,OAPD,MAOQ;AACJ,YAAIA,SAAS,CAACC,UAAV,CAAqBE,KAArB,CAA2B/H,IAAI,IAAG;AAChC,iBAAO,CAAC,CAAC1B,SAAS,CAAC0B,IAAD,EAAOuE,SAAP,EAAkBZ,OAAlB,CAAlB;AACD,SAFD,CAAJ,EAEQ;AACVoD,eAAK,CAAClD,QAAD,CAAL,GAAkB,IAAlB;AACA8C,eAAK,CAACc,IAAN,CAAW;AAACZ,oBAAQ,EAAElD,OAAO,CAACmD,cAAnB;AAAmChG,gBAAI,EAAE8G;AAAzC,WAAX;AACD;AACF,KApBD;AAqBD;AAED;;;;;AAGAtD,SAAO;AACLzE,UAAM,CAACC,IAAP,CAAY,KAAKO,SAAjB,EACKN,OADL,CAEQS,GAAG,IAAI,KAAKH,SAAL,CAAeG,GAAf,EAAoBT,OAApB,CAA4BU,MAAM,IAAIA,MAAM,CAAC6D,OAAP,EAAtC,CAFf;AAGD;;AAEOxB,wBAAsB,CAACxD,MAAD,EAAuB;AACnDO,UAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoBS,OAApB,CAA4BC,IAAI,IAAG;AACjC,YAAM+E,KAAK,GAAGzF,MAAM,CAACU,IAAD,CAApB;AACA,YAAM,CAAC6D,QAAD,IAAerF,aAAa,CAACwB,IAAD,CAAlC;AACA,YAAMc,IAAI,GAAG,KAAK9B,KAAL,CAAWiE,KAAX,CAAiBY,QAAjB,CAAb;;AACA,UAAI/C,IAAI,CAACE,UAAL,CAAgB,OAAhB,KAA4BF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAzD,EAAgE;AAC9D,cAAMF,KAAK,GAAGD,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAvC;AACA,cAAM+G,KAAK,GAAGjH,KAAK,CAACuB,MAAN,KAAiByC,KAAK,CAAChE,KAAN,CAAYuB,MAA7B,IACVyC,KAAK,CAAChE,KAAN,CAAYgH,KAAZ,CACI,CAACE,GAAD,EAAMnE,KAAN,KAAgB/C,KAAK,CAAC+C,KAAD,CAAL,KAAiB,CAAC,CAAlB,IAAuB/C,KAAK,CAAC+C,KAAD,CAAL,KAAiBmE,GAD5D,CADJ;AAGA9J,YAAI,CAAC+J,MAAL,CACIF,KADJ,EAEI,MAAM,sBAAsBlH,IAAI,CAACd,IAAI,iBAA/B,GACF,gCAAgCe,KAAK,aADnC,GAEF,IAAIgE,KAAK,CAAChE,KAAK,GAJvB;AAKD;;AACD,UAAID,IAAI,CAACE,UAAL,CAAgB,OAAhB,KAA4BF,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAzD,EAAgE;AAC9D9C,YAAI,CAAC+J,MAAL,CACInD,KAAK,CAAC5D,KAAN,KAAgBL,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAD7C,EAEI,MAAM,sBAAsBH,IAAI,CAACd,IAAI,iBAA/B,GACF,8BADE,GAEF,GAAGc,IAAI,CAACE,UAAL,CAAgB,OAAhB,EAAyBC,KAAK,aAAa8D,KAAK,CAAC5D,KAAK,EAJjE;AAKD;AACF,KAtBD;AAuBD;;AAEOwB,WAAS,CAACrD,MAAD,EAAuB;AACtC,UAAM6I,MAAM,GAAmB,EAA/B;;AACA,SAAK,MAAMC,SAAX,IAAwB9I,MAAxB,EAAgC;AAC9B,UAAI,KAAKG,UAAL,IAAmB,IAAnB,IAA2B,KAAKA,UAAL,CAAgBH,MAAhB,IAA0B,IAArD,IACA,KAAKG,UAAL,CAAgBH,MAAhB,CAAuB8I,SAAvB,KAAqC,IADzC,EAC+C;AAC7C,cAAM3H,MAAM,GAAG,KAAKhB,UAAL,CAAgBH,MAAhB,CAAuB8I,SAAvB,CAAf;AACAD,cAAM,CAAC1H,MAAM,CAACT,IAAR,CAAN,GAAsBV,MAAM,CAAC8I,SAAD,CAA5B;AACD,OAJD,MAIO;AACLD,cAAM,CAACC,SAAD,CAAN,GAAoB9I,MAAM,CAAC8I,SAAD,CAA1B;AACD;AACF;;AACD,WAAOD,MAAP;AACD;;AAEOtF,aAAW,CAACvD,MAAD,EAAuB;AACxC,UAAM+I,UAAU,GAAGxI,MAAM,CAACC,IAAP,CAAYR,MAAZ,EAAoB+H,MAApB,CAA2BrH,IAAI,IAAG;AACnD,YAAM,CAAC6D,QAAD,IAAarF,aAAa,CAACwB,IAAD,CAAhC;AACA,aAAO,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBY,QAAjB,KAA8B,IAArC;AACD,KAHkB,CAAnB;;AAIA,QAAIwE,UAAU,CAAC/F,MAAX,GAAoB,CAAxB,EAA2B;AACzB,YAAM,IAAIF,KAAJ,CACF,kDACA,UAAUiG,UAAU,8BAFlB,CAAN;AAGD;AACF;;AAEOtF,YAAU,CAAC3D,OAAD,EAAkB;AAClC,WAAOA,OAAO,CAACmB,GAAR,CAAYP,IAAI,IAAG;AACxB,UAAI,KAAKP,UAAL,IAAmB,IAAnB,IAA2B,KAAKA,UAAL,CAAgBL,OAAhB,IAA2B,IAAtD,IACA,KAAKK,UAAL,CAAgBL,OAAhB,CAAwBY,IAAxB,KAAiC,IADrC,EAC2C;AACzC,cAAMS,MAAM,GAAG,KAAKhB,UAAL,CAAgBL,OAAhB,CAAwBY,IAAxB,CAAf;AACA,eAAOS,MAAM,CAACT,IAAd;AACD;;AACD,aAAOA,IAAP;AACD,KAPM,EAOJ,EAPI,CAAP;AAQD;;AAEOgD,cAAY,CAAC5D,OAAD,EAAkB;AACpCA,WAAO,CAACW,OAAR,CAAgBC,IAAI,IAAG;AACrB,YAAM,CAACsI,cAAD,IAAmB9J,aAAa,CAACwB,IAAD,CAAtC;;AACA,UAAI,CAAC,KAAKhB,KAAL,CAAWiE,KAAX,CAAiBqF,cAAjB,CAAL,EAAuC;AACrC,cAAM,IAAIlG,KAAJ,CAAU,eAAepC,IAAI,6BAA7B,CAAN;AACD;AACF,KALD;AAMD;;AA7oBuB","names":["env","tidy","util","getNodeNameAndIndex","getParamValue","getTensor","getTensorsForCurrentContenxt","parseNodeName","executeOp","ExecutionContext","getExecutionSubgraph","getNodesInTopologicalOrder","isControlFlow","GraphExecutor","constructor","graph","parent","Map","_outputs","outputs","_inputs","inputs","_initNodes","initNodes","_signature","signature","_functions","functions","Object","keys","forEach","name","_functionExecutorMap","weightIds","_weightIds","functionExecutorMap","weightMap","_weightMap","map","key","tensor","id","concat","resourceManager","_resourceManager","node","shape","attrParams","value","undefined","dtype","inputNodes","signatureKey","outputNodes","defaultOutput","reduce","getCompilationKey","sortedInputs","sort","sortedOutputs","join","SEPERATOR","compile","executionInfo","missingInputs","dynamicNode","syncInputs","Error","op","length","outNames","n","inNames","execute","mapInputs","names","checkInputs","checkInputShapeAndType","mapOutputs","checkOutputs","nodes","outputNodeNames","resetIntermediateTensors","compilationKey","orderedNodes","compiledMap","get","set","tensorArrayMap","tensorListMap","context","tensorsMap","nodeName","index","tensors","tensorsToKeep","getFrozenTensorIds","intermediateTensorConsumerCount","i","isPromise","checkTensorForDisposal","dispose","tensorMap","ids","apply","Set","outputNames","category","indexOf","children","input","kept","has","count","keepTensorForDebug","intermediateTensors","executeAsync","_executeAsync","disposeIntermediateTensors","disposeTensorsMap","tensorArray","isDisposed","keepIds","getIntermediateTensors","isFunctionExecution","getBool","e","console","warn","message","executeWithControlFlow","results","outputIds","t","inputIds","executeFunctionAsync","mappedInputs","usedNodes","stack","weights","contexts","currentContext","added","promises","processStack","Promise","all","missingOutputs","filter","alternativeMsg","item","pop","push","then","processChildNodes","childNode","inputNames","some","every","match","dim","assert","result","inputName","notInGraph","normalizedName"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-converter/src/executor/graph_executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, env, NamedTensorMap, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {ISignatureDef} from '../data/compiled_api';\nimport {NamedTensorsMap, TensorArrayMap, TensorInfo, TensorListMap} from '../data/types';\nimport {getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName} from '../operations/executors/utils';\nimport {executeOp} from '../operations/operation_executor';\nimport {Graph, Node} from '../operations/types';\n\nimport {ExecutionContext, ExecutionContextInfo} from './execution_context';\nimport {getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow} from './model_analysis';\nimport {ResourceManager} from './resource_manager';\nimport {FunctionExecutor} from './types';\n\ninterface NodeWithContexts {\n  contexts: ExecutionContextInfo[];\n  node: Node;\n}\n\nexport class GraphExecutor implements FunctionExecutor {\n  private compiledMap: Map<string, Node[]> = new Map();\n  private _weightMap: NamedTensorsMap = {};\n  private _weightIds: number[];\n  private _signature: ISignatureDef;\n  private _inputs: Node[];\n  private _outputs: Node[];\n  private _initNodes: Node[];  // Internal init nodes to start initialization.\n  private SEPERATOR = ',';\n  private _functions: {[key: string]: Graph} = {};\n  private _functionExecutorMap: {[key: string]: FunctionExecutor} = {};\n  private _resourceManager: ResourceManager;\n  private intermediateTensors: NamedTensorsMap = {};\n  private keepIds: Set<number>;\n  private tensorsMap: NamedTensorsMap;\n  private keepTensorForDebug = false;\n\n  get weightIds(): number[] {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap(): {[key: string]: FunctionExecutor} {\n    return this.parent ? this.parent.functionExecutorMap :\n                         this._functionExecutorMap;\n  }\n\n  get weightMap(): NamedTensorsMap {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(weightMap: NamedTensorsMap) {\n    const weightIds = Object.keys(weightMap).map(\n        key => weightMap[key].map(tensor => tensor.id));\n    this._weightIds = [].concat(...weightIds);\n    this._weightMap = weightMap;\n  }\n\n  /**\n   * Set `ResourceManager` shared by executors of a model.\n   * @param resourceManager: `ResourceManager` of the `GraphModel`.\n   */\n  set resourceManager(resourceManager: ResourceManager) {\n    this._resourceManager = resourceManager;\n  }\n\n  get inputs(): TensorInfo[] {\n    return this._inputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get outputs(): TensorInfo[] {\n    return this._outputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get inputNodes(): string[] {\n    return this._inputs.map(node => node.signatureKey || node.name);\n  }\n\n  get outputNodes(): string[] {\n    return this._outputs.map((node) => {\n      const name = node.signatureKey || node.name;\n      return node.defaultOutput ? (`${name}:${node.defaultOutput}`) : name;\n    });\n  }\n\n  get functions(): {[key: string]: ISignatureDef} {\n    return Object.keys(this._functions).reduce((map, key) => {\n      map[key] = this._functions[key].signature;\n      return map;\n    }, {} as {[key: string]: ISignatureDef});\n  }\n\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  constructor(private graph: Graph, private parent?: GraphExecutor) {\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions;\n    // create sub-graph executors\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(name => {\n        this._functionExecutorMap[name] =\n            new GraphExecutor(graph.functions[name], this);\n      });\n    }\n  }\n\n  private getCompilationKey(inputs: Node[], outputs: Node[]): string {\n    const sortedInputs = inputs.map(node => node.name).sort();\n    const sortedOutputs = outputs.map(node => node.name).sort();\n    return sortedInputs.join(this.SEPERATOR) + '--' +\n        sortedOutputs.join(this.SEPERATOR);\n  }\n\n  /**\n   * Compiles the inference graph and returns the minimal set of nodes that are\n   * required for execution, in the correct execution order.\n   */\n  private compile(inputs: NamedTensorMap, outputs: Node[]): Node[] {\n    const executionInfo =\n        getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n    const {missingInputs, dynamicNode, syncInputs} = executionInfo;\n    if (dynamicNode != null) {\n      throw new Error(\n          `This execution contains the node '${dynamicNode.name}', which has ` +\n          `the dynamic op '${dynamicNode.op}'. Please use ` +\n          `model.executeAsync() instead. Alternatively, to avoid the ` +\n          `dynamic ops, specify the inputs [${syncInputs}]`);\n    }\n\n    if (missingInputs.length > 0) {\n      const outNames = outputs.map(n => n.name);\n      const inNames = Object.keys(inputs);\n      throw new Error(\n          `Cannot compute the outputs [${outNames}] from the provided inputs ` +\n          `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n    }\n\n    return getNodesInTopologicalOrder(\n        this.graph, this.weightMap, executionInfo);\n  }\n\n  /**\n   * Executes the inference for given input tensors.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model, if\n   * no outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   */\n  execute(inputs: NamedTensorMap, outputs?: string[]): Tensor[] {\n    inputs = this.mapInputs(inputs);\n    const names = Object.keys(inputs).sort();\n    this.checkInputs(inputs);\n    this.checkInputShapeAndType(inputs);\n    outputs = this.mapOutputs(outputs);\n    this.checkOutputs(outputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n    this.resetIntermediateTensors();\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n\n    // Do nothing if the compiled graph cache contains the input.\n    let orderedNodes = this.compiledMap.get(compilationKey);\n    if (orderedNodes == null) {\n      orderedNodes = this.compile(inputs, outputNodes);\n      this.compiledMap.set(compilationKey, orderedNodes);\n    }\n\n    const tensorArrayMap: TensorArrayMap = {};\n    const tensorListMap: TensorListMap = {};\n\n    return tidy(() => {\n      const context = new ExecutionContext(\n          this.weightMap, tensorArrayMap, tensorListMap,\n          this.functionExecutorMap);\n      const tensorsMap: NamedTensorsMap = {...this.weightMap};\n\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name);\n        const tensors: Tensor[] = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n      });\n\n      const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n      const intermediateTensorConsumerCount: {[key: number]: number} = {};\n      for (let i = 0; i < orderedNodes.length; i++) {\n        const node = orderedNodes[i];\n        if (!tensorsMap[node.name]) {\n          const tensors =\n              executeOp(node, tensorsMap, context, this._resourceManager) as\n              Tensor[];\n          if (util.isPromise(tensors)) {\n            throw new Error(\n                `The execution of the op '${node.op}' returned a promise. ` +\n                `Please use model.executeAsync() instead.`);\n          }\n          tensorsMap[node.name] = tensors;\n          this.checkTensorForDisposal(\n              node.name, node, tensorsMap, context, tensorsToKeep,\n              outputNodeNames, intermediateTensorConsumerCount);\n        }\n      }\n      // dispose the context for the root executor\n      if (this.parent == null) {\n        context.dispose(tensorsToKeep);\n      }\n      return outputs.map(name => getTensor(name, tensorsMap, context));\n    });\n  }\n\n  private getFrozenTensorIds(tensorMap: NamedTensorsMap): Set<number> {\n    const ids = [].concat.apply(\n        [],\n        Object.keys(tensorMap)\n            .map(key => tensorMap[key])\n            .map(tensors => tensors.map(tensor => tensor.id)));\n    return new Set(ids);\n  }\n  private checkTensorForDisposal(\n      nodeName: string, node: Node, tensorMap: NamedTensorsMap,\n      context: ExecutionContext, tensorsToKeep: Set<number>,\n      outputNames: string[],\n      intermediateTensorConsumerCount: {[key: string]: number}) {\n    // Skip output nodes and any control flow nodes, since its dependency is\n    // tricky to track correctly.\n    if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n      return;\n    }\n\n    tensorMap[nodeName].forEach(tensor => {\n      if (tensor != null) {\n        intermediateTensorConsumerCount[tensor.id] =\n            (intermediateTensorConsumerCount[tensor.id] || 0) +\n            node.children.length;\n      }\n    });\n    node.inputs.forEach(input => {\n      // Skip any control flow nodes, since its dependency is tricky to track\n      // correctly.\n      if (input.category !== 'control') {\n        const tensors =\n            getTensorsForCurrentContenxt(input.name, tensorMap, context);\n        if (tensors != null) {\n          tensors.forEach(tensor => {\n            if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n              const count = intermediateTensorConsumerCount[tensor.id];\n              if (count === 1) {\n                if (!this.keepTensorForDebug) {\n                  tensor.dispose();\n                } else {\n                  const [nodeName, index] =\n                      getNodeNameAndIndex(node.name, context);\n                  if (this.intermediateTensors[nodeName]) {\n                    this.intermediateTensors[nodeName][index] = tensor;\n                  } else {\n                    this.intermediateTensors[nodeName] = [];\n                    this.intermediateTensors[nodeName][index] = tensor;\n                  }\n                }\n                delete intermediateTensorConsumerCount[tensor.id];\n              } else if (count != null) {\n                // only intermediate nodes has count set, inputs and weights are\n                // not.\n                intermediateTensorConsumerCount[tensor.id]--;\n              }\n            }\n          });\n        }\n      }\n    });\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   */\n  async executeAsync(inputs: NamedTensorMap, outputs?: string[]):\n      Promise<Tensor[]> {\n    return this._executeAsync(inputs, outputs);\n  }\n\n  disposeIntermediateTensors() {\n    if (!this.intermediateTensors) {\n      return;\n    }\n    Object.keys(this.intermediateTensors)\n        .forEach(\n            key => this.intermediateTensors[key].forEach(\n                tensor => tensor.dispose()));\n    this.disposeTensorsMap();\n  }\n\n  private disposeTensorsMap() {\n    if (!this.tensorsMap) {\n      return;\n    }\n    Object.keys(this.tensorsMap).forEach(key => {\n      const tensorArray = this.tensorsMap[key];\n      tensorArray.forEach(tensor => {\n        if (tensor && !tensor.kept && !tensor.isDisposed &&\n            !this.keepIds.has(tensor.id)) {\n          tensor.dispose();\n        }\n      });\n    });\n  }\n\n  getIntermediateTensors(): NamedTensorsMap {\n    return this.tensorsMap;\n  }\n\n  private resetIntermediateTensors() {\n    for (const key in this.intermediateTensors) {\n      this.intermediateTensors[key].forEach(tensor => tensor.dispose());\n      delete this.intermediateTensors[key];\n    }\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Optional. Flag for executing a function.\n   * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n   * function execution.\n   * @param tensorArrayMap Optinal global TensorList map by id. Used for\n   * function execution.\n   */\n  private async _executeAsync(\n      inputs: NamedTensorMap, outputs?: string[], isFunctionExecution = false,\n      tensorArrayMap: TensorArrayMap = {},\n      tensorListMap: TensorListMap = {}): Promise<Tensor[]> {\n    if (!isFunctionExecution) {\n      inputs = this.mapInputs(inputs);\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n    }\n\n    // For model debug.\n    try {\n      this.keepTensorForDebug = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      console.warn(e.message);\n    }\n    this.resetIntermediateTensors();\n\n    const context = new ExecutionContext(\n        this.weightMap, tensorArrayMap, tensorListMap,\n        this.functionExecutorMap);\n\n    // Graph with control flow op requires runtime evaluation of the execution\n    // order, while without control flow the execution order is pre-determined\n    // in the compile method.\n    this.tensorsMap = await this.executeWithControlFlow(\n        inputs, context, outputs, isFunctionExecution);\n    const results =\n        outputs.map(name => getTensor(name, this.tensorsMap, context));\n\n    // dispose all the intermediate tensors\n    const outputIds = results.map(t => t.id);\n    const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n    this.keepIds =\n        new Set<number>([...outputIds, ...inputIds, ...this.weightIds]);\n    if (!this.keepTensorForDebug) {\n      this.disposeTensorsMap();\n    }\n\n    // dispose the context for the root executor\n    if (this.parent == null) {\n      context.dispose(this.keepIds);\n    }\n\n    return results;\n  }\n\n  async executeFunctionAsync(\n      inputs: Tensor[], tensorArrayMap: TensorArrayMap,\n      tensorListMap: TensorListMap): Promise<Tensor[]> {\n    const mappedInputs = inputs.reduce((map, tensor, index) => {\n      map[this.inputs[index].name] = tensor;\n      return map;\n    }, {} as NamedTensorMap);\n\n    return this._executeAsync(\n        mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n  }\n\n  /**\n   * When there are control flow nodes in the graph, the graph execution use\n   * ExecutionContext to keep track of the frames and loop iterators.\n   * @param inputs placeholder tensors for the graph.\n   * @param context the execution context object for current execution.\n   * @param outputNames Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   * @param isFunctionExecution Flag for executing a function.\n   */\n  private async executeWithControlFlow(\n      inputs: NamedTensorMap, context: ExecutionContext, outputNames?: string[],\n      isFunctionExecution?: boolean): Promise<NamedTensorsMap> {\n    const names = Object.keys(inputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const {usedNodes, missingInputs, dynamicNode, syncInputs} =\n        getExecutionSubgraph(\n            inputs, outputNodes, this.weightMap, this._initNodes);\n\n    // First nodes to execute include inputNodes, weights, and initNodes.\n    const stack: NodeWithContexts[] = [\n      ...inputNodes, ...this.graph.weights, ...(this._initNodes || [])\n    ].map(node => {\n      return {node, contexts: context.currentContext};\n    });\n    const tensorsMap: NamedTensorsMap = {...this.weightMap};\n    Object.keys(inputs).forEach(name => {\n      const [nodeName, index] = parseNodeName(name);\n      const tensors: Tensor[] = [];\n      tensors[index] = inputs[name];\n      tensorsMap[nodeName] = tensors;\n    });\n    const intermediateTensorConsumerCount: {[key: number]: number} = {};\n    const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n    const added: {[key: string]: boolean} = {};\n    while (stack.length > 0) {\n      const promises = this.processStack(\n          inputNodes, stack, context, tensorsMap, added, tensorsToKeep,\n          outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n      await Promise.all(promises);\n    }\n    if (dynamicNode == null && !isFunctionExecution) {\n      console.warn(\n          `This model execution did not contain any nodes with control flow ` +\n          `or dynamic output shapes. You can use model.execute() instead.`);\n    }\n    const missingOutputs =\n        outputNodes\n            .filter(\n                node => !isControlFlow(node) &&\n                    !getTensor(node.name, tensorsMap, context))\n            .map(node => node.name);\n    if (missingOutputs.length > 0) {\n      let alternativeMsg = '';\n      if (dynamicNode != null) {\n        alternativeMsg =\n            `Alternatively, to avoid the dynamic ops, use model.execute() ` +\n            `and specify the inputs [${syncInputs}]`;\n      }\n      throw new Error(\n          `Cannot compute the outputs [${missingOutputs}] from the provided ` +\n          `inputs [${names}]. Consider providing the following inputs: ` +\n          `[${missingInputs}]. ${alternativeMsg}`);\n    }\n    return tensorsMap;\n  }\n\n  private processStack(\n      inputNodes: Node[], stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      tensorsToKeep: Set<number>, outputNames: string[],\n      intermediateTensorConsumerCount: {[key: number]: number},\n      usedNodes: Set<string>) {\n    const promises: Array<Promise<Tensor[]>> = [];\n    while (stack.length > 0) {\n      const item = stack.pop();\n      context.currentContext = item.contexts;\n      let nodeName = '';\n      // The tensor of the Enter op with isConstant set should be set\n      // in the parent scope, so it will be available as constant for the\n      // whole loop.\n      if (item.node.op === 'Enter' &&\n          getParamValue('isConstant', item.node, tensorMap, context)) {\n        [nodeName] = getNodeNameAndIndex(item.node.name, context);\n      }\n\n      // only process nodes that are not in the tensorMap yet, this include\n      // inputNodes and internal initNodes.\n      if (tensorMap[item.node.name] == null) {\n        const tensors =\n            executeOp(item.node, tensorMap, context, this._resourceManager);\n        if (!nodeName) {\n          [nodeName] = getNodeNameAndIndex(item.node.name, context);\n        }\n        const currentContext = context.currentContext;\n        if (util.isPromise(tensors)) {\n          promises.push(tensors.then(t => {\n            tensorMap[nodeName] = t;\n            context.currentContext = currentContext;\n            this.checkTensorForDisposal(\n                nodeName, item.node, tensorMap, context, tensorsToKeep,\n                outputNames, intermediateTensorConsumerCount);\n            this.processChildNodes(\n                item.node, stack, context, tensorMap, added, usedNodes);\n            return t;\n          }));\n        } else {\n          tensorMap[nodeName] = tensors;\n          this.checkTensorForDisposal(\n              nodeName, item.node, tensorMap, context, tensorsToKeep,\n              outputNames, intermediateTensorConsumerCount);\n          this.processChildNodes(\n              item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      } else {\n        this.processChildNodes(\n            item.node, stack, context, tensorMap, added, usedNodes);\n      }\n    }\n    return promises;\n  }\n\n  private processChildNodes(\n      node: Node, stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      usedNodes: Set<string>) {\n    node.children.forEach((childNode) => {\n      const [nodeName, ] = getNodeNameAndIndex(childNode.name, context);\n      if (added[nodeName] || !usedNodes.has(childNode.name)) {\n        return;\n      }\n      // Merge op can be pushed if any of its inputs has value.\n      if (childNode.op === 'Merge') {\n        if (childNode.inputNames.some(name => {\n              return !!getTensor(name, tensorMap, context);\n            })) {\n          added[nodeName] = true;\n          stack.push({contexts: context.currentContext, node: childNode});\n        }\n      } else  // Otherwise all inputs must to have value.\n          if (childNode.inputNames.every(name => {\n                return !!getTensor(name, tensorMap, context);\n              })) {\n        added[nodeName] = true;\n        stack.push({contexts: context.currentContext, node: childNode});\n      }\n    });\n  }\n\n  /**\n   * Releases the memory used by the weight tensors.\n   */\n  dispose() {\n    Object.keys(this.weightMap)\n        .forEach(\n            key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n  }\n\n  private checkInputShapeAndType(inputs: NamedTensorMap) {\n    Object.keys(inputs).forEach(name => {\n      const input = inputs[name];\n      const [nodeName, ] = parseNodeName(name);\n      const node = this.graph.nodes[nodeName];\n      if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n        const shape = node.attrParams['shape'].value as number[];\n        const match = shape.length === input.shape.length &&\n            input.shape.every(\n                (dim, index) => shape[index] === -1 || shape[index] === dim);\n        util.assert(\n            match,\n            () => `The shape of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be [${shape}], but was ` +\n                `[${input.shape}]`);\n      }\n      if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n        util.assert(\n            input.dtype === node.attrParams['dtype'].value as string,\n            () => `The dtype of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be ` +\n                `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n      }\n    });\n  }\n\n  private mapInputs(inputs: NamedTensorMap) {\n    const result: NamedTensorMap = {};\n    for (const inputName in inputs) {\n      if (this._signature != null && this._signature.inputs != null &&\n          this._signature.inputs[inputName] != null) {\n        const tensor = this._signature.inputs[inputName];\n        result[tensor.name] = inputs[inputName];\n      } else {\n        result[inputName] = inputs[inputName];\n      }\n    }\n    return result;\n  }\n\n  private checkInputs(inputs: NamedTensorMap) {\n    const notInGraph = Object.keys(inputs).filter(name => {\n      const [nodeName] = parseNodeName(name);\n      return this.graph.nodes[nodeName] == null;\n    });\n    if (notInGraph.length > 0) {\n      throw new Error(\n          `The dict provided in model.execute(dict) has ` +\n          `keys: [${notInGraph}] that are not part of graph`);\n    }\n  }\n\n  private mapOutputs(outputs: string[]) {\n    return outputs.map(name => {\n      if (this._signature != null && this._signature.outputs != null &&\n          this._signature.outputs[name] != null) {\n        const tensor = this._signature.outputs[name];\n        return tensor.name;\n      }\n      return name;\n    }, {});\n  }\n\n  private checkOutputs(outputs: string[]): void {\n    outputs.forEach(name => {\n      const [normalizedName] = parseNodeName(name);\n      if (!this.graph.nodes[normalizedName]) {\n        throw new Error(`The output '${name}' is not found in the graph`);\n      }\n    });\n  }\n}\n"]},"metadata":{},"sourceType":"module"}