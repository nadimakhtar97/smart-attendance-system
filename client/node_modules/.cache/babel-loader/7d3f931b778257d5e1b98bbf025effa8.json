{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\n\nfunction fusedMatMul_(_ref) {\n  let {\n    a,\n    b,\n    transposeA = false,\n    transposeB = false,\n    bias,\n    activation = 'linear',\n    preluActivationWeights,\n    leakyreluAlpha\n  } = _ref;\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  let $a = convertToTensor(a, 'a', 'fused matMul');\n  let $b = convertToTensor(b, 'b', 'fused matMul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  const outerDimsA = $a.shape.slice(0, -2);\n  const outerDimsB = $b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape($a.shape.slice(0, -2), $b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  const a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  const b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    [$bias] = makeTypesMatch($bias, $a);\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  const grad = (dy, saved) => {\n    const [a3D, b3D, y, $bias] = saved; // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n\n    const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    let aDer;\n    let bDer;\n\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n\n  const inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    transposeA,\n    transposeB,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((a3D, b3D, save) => {\n      const res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n      const res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\n\nexport const matMul = op({\n  fusedMatMul_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,cAArB;AACA,SAAQC,UAAR,QAAyB,iBAAzB;AACA,SAAQC,YAAR,QAAkE,oBAAlE;AAIA,SAAQC,cAAR,QAA6B,mBAA7B;AACA,SAAQC,eAAR,QAA8B,uBAA9B;AAEA,OAAO,KAAKC,IAAZ,MAAsB,YAAtB;AAEA,SAAQC,GAAR,QAAkB,QAAlB;AACA,OAAO,KAAKC,cAAZ,MAAgC,mBAAhC;AAEA,SAAQC,eAAR,EAAyBC,oBAAzB,EAA+CC,oBAA/C,EAAqEC,UAArE,QAAsF,eAAtF;AACA,SAAQC,MAAM,IAAIC,aAAlB,QAAsC,YAAtC;AACA,SAAQC,EAAR,QAAiB,cAAjB;AACA,SAAQC,OAAR,QAAsB,YAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;AAqBA,SAASC,YAAT,OAkBC;AAAA,MAlBqB;AACpBC,KADoB;AAEpBC,KAFoB;AAGpBC,cAAU,GAAG,KAHO;AAIpBC,cAAU,GAAG,KAJO;AAKpBC,QALoB;AAMpBC,cAAU,GAAG,QANO;AAOpBC,0BAPoB;AAQpBC;AARoB,GAkBrB;;AACG,MAAIb,UAAU,CAACX,MAAM,CAACyB,KAAP,CAAaC,aAAd,EAA6BJ,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAIK,MAAM,GAAGd,aAAa,CAACI,CAAD,EAAIC,CAAJ,EAAOC,UAAP,EAAmBC,UAAnB,CAA1B;;AACA,QAAIC,IAAI,IAAI,IAAZ,EAAkB;AAChBM,YAAM,GAAGrB,GAAG,CAACqB,MAAD,EAASN,IAAT,CAAZ;AACD;;AAED,WAAOb,eAAe,CACXmB,MADW,EACHL,UADG,EACSC,sBADT,EACiCC,cADjC,CAAtB;AAED;;AAED,MAAII,EAAE,GAAGxB,eAAe,CAACa,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,MAAIY,EAAE,GAAGzB,eAAe,CAACc,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,GAACU,EAAD,EAAKC,EAAL,IAAW1B,cAAc,CAACyB,EAAD,EAAKC,EAAL,CAAzB;AAEA,QAAMC,WAAW,GACbX,UAAU,GAAGS,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAH,GAA2BJ,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAMC,WAAW,GACbb,UAAU,GAAGS,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAH,GAA2BH,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAME,WAAW,GACbf,UAAU,GAAGS,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CAAH,GAA2BJ,EAAE,CAACG,KAAH,CAASH,EAAE,CAACI,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAMG,WAAW,GACbf,UAAU,GAAGS,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CAAH,GAA2BH,EAAE,CAACE,KAAH,CAASF,EAAE,CAACG,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAMI,UAAU,GAAGR,EAAE,CAACG,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAMC,UAAU,GAAGT,EAAE,CAACE,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAME,SAAS,GAAGlC,IAAI,CAACmC,aAAL,CAAmBJ,UAAnB,CAAlB;AACA,QAAMK,SAAS,GAAGpC,IAAI,CAACmC,aAAL,CAAmBF,UAAnB,CAAlB;AAEAjC,MAAI,CAACqC,MAAL,CACIZ,WAAW,KAAKG,WADpB,EAEI,MAAM,wCAAwCH,WAAW,SAAnD,GACF,GAAGG,WAAW,4BAA4BL,EAAE,CAACG,KAAK,OADhD,GAEF,GAAGF,EAAE,CAACE,KAAK,mBAAmBZ,UAAU,EAFtC,GAGF,mBAAmBC,UAAU,cALrC;AAOA,QAAMuB,iBAAiB,GAAGpC,cAAc,CAACqC,0BAAf,CACtBhB,EAAE,CAACG,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CADsB,EACCR,EAAE,CAACE,KAAH,CAASM,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CADD,CAA1B;AAEA,QAAMQ,QAAQ,GAAGF,iBAAiB,CAACG,MAAlB,CAAyB,CAACZ,WAAD,EAAcC,WAAd,CAAzB,CAAjB;AAEA,QAAMY,GAAG,GAAa5B,UAAU,GAC5BJ,OAAO,CAACa,EAAD,EAAK,CAACW,SAAD,EAAYT,WAAZ,EAAyBI,WAAzB,CAAL,CADqB,GAE5BnB,OAAO,CAACa,EAAD,EAAK,CAACW,SAAD,EAAYL,WAAZ,EAAyBJ,WAAzB,CAAL,CAFX;AAGA,QAAMkB,GAAG,GAAa5B,UAAU,GAC5BL,OAAO,CAACc,EAAD,EAAK,CAACY,SAAD,EAAYN,WAAZ,EAAyBF,WAAzB,CAAL,CADqB,GAE5BlB,OAAO,CAACc,EAAD,EAAK,CAACY,SAAD,EAAYR,WAAZ,EAAyBE,WAAzB,CAAL,CAFX;AAIA,MAAIc,KAAJ;;AACA,MAAI5B,IAAI,IAAI,IAAZ,EAAkB;AAChB4B,SAAK,GAAG7C,eAAe,CAACiB,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC4B,KAAD,IAAU9C,cAAc,CAAC8C,KAAD,EAAQrB,EAAR,CAAxB;AAEArB,kBAAc,CAACqC,0BAAf,CAA0CC,QAA1C,EAAoDI,KAAK,CAAClB,KAA1D;AACD;;AAED,MAAImB,uBAAJ;;AACA,MAAI3B,sBAAsB,IAAI,IAA9B,EAAoC;AAClC2B,2BAAuB,GAAG9C,eAAe,CACrCmB,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAM4B,IAAI,GAAG,CAACC,EAAD,EAAeC,KAAf,KAAkC;AAC7C,UAAM,CAACN,GAAD,EAAMC,GAAN,EAAWM,CAAX,EAAcL,KAAd,IAAuBI,KAA7B,CAD6C,CAE7C;AACA;AACA;;AACA,UAAME,YAAY,GACd7C,oBAAoB,CAACK,OAAO,CAACqC,EAAD,EAAKE,CAAC,CAACvB,KAAP,CAAR,EAAuBuB,CAAvB,EAA0BhC,UAA1B,CADxB;AAEA,QAAIkC,IAAJ;AACA,QAAIC,IAAJ;;AAEA,QAAI,CAACtC,UAAD,IAAe,CAACC,UAApB,EAAgC;AAC9BoC,UAAI,GAAG3C,aAAa,CAAC0C,YAAD,EAAeP,GAAf,EAAoB,KAApB,EAA2B,IAA3B,CAApB;AACAS,UAAI,GAAG5C,aAAa,CAACkC,GAAD,EAAMQ,YAAN,EAAoB,IAApB,EAA0B,KAA1B,CAApB;AACD,KAHD,MAGO,IAAI,CAACpC,UAAD,IAAeC,UAAnB,EAA+B;AACpCoC,UAAI,GAAG3C,aAAa,CAAC0C,YAAD,EAAeP,GAAf,EAAoB,KAApB,EAA2B,KAA3B,CAApB;AACAS,UAAI,GAAG5C,aAAa,CAAC0C,YAAD,EAAeR,GAAf,EAAoB,IAApB,EAA0B,KAA1B,CAApB;AACD,KAHM,MAGA,IAAI5B,UAAU,IAAI,CAACC,UAAnB,EAA+B;AACpCoC,UAAI,GAAG3C,aAAa,CAACmC,GAAD,EAAMO,YAAN,EAAoB,KAApB,EAA2B,IAA3B,CAApB;AACAE,UAAI,GAAG5C,aAAa,CAACkC,GAAD,EAAMQ,YAAN,EAAoB,KAApB,EAA2B,KAA3B,CAApB;AACD,KAHM,MAGA;AACLC,UAAI,GAAG3C,aAAa,CAACmC,GAAD,EAAMO,YAAN,EAAoB,IAApB,EAA0B,IAA1B,CAApB;AACAE,UAAI,GAAG5C,aAAa,CAAC0C,YAAD,EAAeR,GAAf,EAAoB,IAApB,EAA0B,IAA1B,CAApB;AACD;;AAED,QAAI1B,IAAI,IAAI,IAAZ,EAAkB;AAChB,YAAMqC,OAAO,GAAGjD,oBAAoB,CAACwC,KAAD,EAAQM,YAAR,CAApC;AACA,aAAO,CAACC,IAAD,EAAOC,IAAP,EAAaC,OAAb,CAAP;AACD,KAHD,MAGO;AACL,aAAO,CAACF,IAAD,EAAOC,IAAP,CAAP;AACD;AACF,GA9BD;;AAgCA,QAAME,MAAM,GAAuB;AACjC1C,KAAC,EAAE8B,GAD8B;AAEjC7B,KAAC,EAAE8B,GAF8B;AAGjC3B,QAAI,EAAE4B,KAH2B;AAIjC1B,0BAAsB,EAAE2B;AAJS,GAAnC;AAMA,QAAMU,KAAK,GACP;AAACzC,cAAD;AAAaC,cAAb;AAAyBE,cAAzB;AAAqCE;AAArC,GADJ,CApGH,CAuGG;AACA;;AACA,MAAIH,IAAI,IAAI,IAAZ,EAAkB;AAChB,UAAMwC,QAAQ,GACV5D,UAAU,CAAC,CAAC8C,GAAD,EAAgBC,GAAhB,EAA+Bc,IAA/B,KAAqD;AAC9D,YAAMC,GAAG,GACL;AACA/D,YAAM,CAACgE,SAAP,CACI9D,YADJ,EACkByD,MADlB,EAEIC,KAFJ,CAFJ;AAMAE,UAAI,CAAC,CAACf,GAAD,EAAMC,GAAN,EAAWe,GAAX,CAAD,CAAJ;AAEA,aAAO;AAACE,aAAK,EAAElD,OAAO,CAACgD,GAAD,EAAMlB,QAAN,CAAf;AAAgCqB,gBAAQ,EAAEf;AAA1C,OAAP;AACD,KAVS,CADd;AAYA,WAAOU,QAAQ,CAACd,GAAD,EAAMC,GAAN,CAAf;AACD,GAdD,MAcO;AACL,UAAMmB,gBAAgB,GAAGlE,UAAU,CAC/B,CAAC8C,GAAD,EAAgBC,GAAhB,EAA+BC,KAA/B,EAA8Ca,IAA9C,KAAoE;AAClE,YAAMC,GAAG,GACL;AACA/D,YAAM,CAACgE,SAAP,CACI9D,YADJ,EACkByD,MADlB,EAEIC,KAFJ,CAFJ;AAMAE,UAAI,CAAC,CAACf,GAAD,EAAMC,GAAN,EAAWe,GAAX,EAAgBd,KAAhB,CAAD,CAAJ;AAEA,aAAO;AAACgB,aAAK,EAAElD,OAAO,CAACgD,GAAD,EAAMlB,QAAN,CAAf;AAAgCqB,gBAAQ,EAAEf;AAA1C,OAAP;AACD,KAX8B,CAAnC;AAaA,WAAOgB,gBAAgB,CAACpB,GAAD,EAAMC,GAAN,EAAWC,KAAX,CAAvB;AACD;AACF;;AAED,OAAO,MAAMrC,MAAM,GAAGE,EAAE,CAAC;AAACE;AAAD,CAAD,CAAjB","names":["ENGINE","customGrad","_FusedMatMul","makeTypesMatch","convertToTensor","util","add","broadcast_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","matMul","unfusedMatMul","op","reshape","fusedMatMul_","a","b","transposeA","transposeB","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","$a","$b","innerShapeA","shape","rank","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","assert","outShapeOuterDims","assertAndGetBroadcastShape","outShape","concat","a3D","b3D","$bias","$preluActivationWeights","grad","dy","saved","y","dyActivation","aDer","bDer","biasDer","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/fused/mat_mul.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\n\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {matMul as unfusedMatMul} from '../mat_mul';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha,\n}: {\n  a: Tensor|TensorLike,\n  b: Tensor|TensorLike,\n  transposeA?: boolean,\n  transposeB?: boolean,\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor\n  leakyreluAlpha?: number\n}): Tensor {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n      let result = unfusedMatMul(a, b, transposeA, transposeB);\n      if (bias != null) {\n        result = add(result, bias);\n      }\n\n      return applyActivation(\n                 result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n\n    const innerShapeA =\n        transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB =\n        transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n\n    const outerShapeA =\n        transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB =\n        transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n\n    util.assert(\n        innerShapeA === innerShapeB,\n        () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n            `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n            `${$b.shape} and transposeA=${transposeA}` +\n            ` and transposeB=${transposeB} must match.`);\n\n    const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n        $a.shape.slice(0, -2), $b.shape.slice(0, -2));\n    const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n    const a3D: Tensor3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D: Tensor3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n\n    let $bias: Tensor;\n    if (bias != null) {\n      $bias = convertToTensor(bias, 'bias', 'fused matMul');\n      [$bias] = makeTypesMatch($bias, $a);\n\n      broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n\n    let $preluActivationWeights: Tensor;\n    if (preluActivationWeights != null) {\n      $preluActivationWeights = convertToTensor(\n          preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n\n    const grad = (dy: Tensor3D, saved: Tensor[]) => {\n      const [a3D, b3D, y, $bias] = saved;\n      // we reshape dy because the result of the forward is not\n      // necessarily going to be a 3d tensor due to a reshape done at the end of\n      // the customOp.\n      const dyActivation =\n          getFusedDyActivation(reshape(dy, y.shape), y, activation);\n      let aDer: Tensor;\n      let bDer: Tensor;\n\n      if (!transposeA && !transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, true, false);\n      } else if (!transposeA && transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, false);\n        bDer = unfusedMatMul(dyActivation, a3D, true, false);\n      } else if (transposeA && !transposeB) {\n        aDer = unfusedMatMul(b3D, dyActivation, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, false, false);\n      } else {\n        aDer = unfusedMatMul(b3D, dyActivation, true, true);\n        bDer = unfusedMatMul(dyActivation, a3D, true, true);\n      }\n\n      if (bias != null) {\n        const biasDer = getFusedBiasGradient($bias, dyActivation);\n        return [aDer, bDer, biasDer];\n      } else {\n        return [aDer, bDer];\n      }\n    };\n\n    const inputs: _FusedMatMulInputs = {\n      a: a3D,\n      b: b3D,\n      bias: $bias,\n      preluActivationWeights: $preluActivationWeights\n    };\n    const attrs: _FusedMatMulAttrs =\n        {transposeA, transposeB, activation, leakyreluAlpha};\n\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n      const customOp =\n          customGrad((a3D: Tensor3D, b3D: Tensor3D, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as {} as NamedTensorMap,\n                    attrs as {} as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n      return customOp(a3D, b3D);\n    } else {\n      const customOpWithBias = customGrad(\n          (a3D: Tensor3D, b3D: Tensor3D, $bias: Tensor, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as {} as NamedTensorMap,\n                    attrs as {} as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res, $bias]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n\n      return customOpWithBias(a3D, b3D, $bias);\n    }\n  }\n\n  export const matMul = op({fusedMatMul_});\n"]},"metadata":{},"sourceType":"module"}