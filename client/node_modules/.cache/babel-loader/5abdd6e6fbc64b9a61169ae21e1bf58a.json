{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Softmax } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\n\nfunction softmax_(logits) {\n  let dim = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n\n  if (dim === -1) {\n    dim = $logits.rank - 1;\n  }\n\n  if (dim !== $logits.rank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + `Logits was rank ${$logits.rank} and dim was ${dim}`);\n  }\n\n  const inputs = {\n    logits: $logits\n  };\n  const attrs = {\n    dim\n  };\n  return ENGINE.runKernel(Softmax, inputs, attrs);\n}\n\nexport const softmax = op({\n  softmax_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,OAAR,QAAmD,iBAAnD;AAIA,SAAQC,eAAR,QAA8B,oBAA9B;AAGA,SAAQC,EAAR,QAAiB,aAAjB;AAEA;;;;;;;;;;;;;;;;;;;;;;AAqBA,SAASC,QAAT,CAAoCC,MAApC,EAAkE;AAAA,MAARC,GAAQ,uEAAF,CAAC,CAAC;AAChE,QAAMC,OAAO,GAAGL,eAAe,CAACG,MAAD,EAAS,QAAT,EAAmB,SAAnB,EAA8B,SAA9B,CAA/B;;AAEA,MAAIC,GAAG,KAAK,CAAC,CAAb,EAAgB;AACdA,OAAG,GAAGC,OAAO,CAACC,IAAR,GAAe,CAArB;AACD;;AACD,MAAIF,GAAG,KAAKC,OAAO,CAACC,IAAR,GAAe,CAA3B,EAA8B;AAC5B,UAAMC,KAAK,CACP,8DACA,mBAAmBF,OAAO,CAACC,IAAI,gBAAgBF,GAAG,EAF3C,CAAX;AAGD;;AAED,QAAMI,MAAM,GAAkB;AAACL,UAAM,EAAEE;AAAT,GAA9B;AACA,QAAMI,KAAK,GAAiB;AAACL;AAAD,GAA5B;AAEA,SAAON,MAAM,CAACY,SAAP,CACHX,OADG,EACMS,MADN,EACsCC,KADtC,CAAP;AAED;;AAED,OAAO,MAAME,OAAO,GAAGV,EAAE,CAAC;AAACC;AAAD,CAAD,CAAlB","names":["ENGINE","Softmax","convertToTensor","op","softmax_","logits","dim","$logits","rank","Error","inputs","attrs","runKernel","softmax"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/softmax.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {Softmax, SoftmaxAttrs, SoftmaxInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {op} from './operation';\n\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction softmax_<T extends Tensor>(logits: T|TensorLike, dim = -1): T {\n  const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n\n  if (dim === -1) {\n    dim = $logits.rank - 1;\n  }\n  if (dim !== $logits.rank - 1) {\n    throw Error(\n        'Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${$logits.rank} and dim was ${dim}`);\n  }\n\n  const inputs: SoftmaxInputs = {logits: $logits};\n  const attrs: SoftmaxAttrs = {dim};\n\n  return ENGINE.runKernel(\n      Softmax, inputs as {} as NamedTensorMap, attrs as {} as NamedAttrMap);\n}\n\nexport const softmax = op({softmax_});\n"]},"metadata":{},"sourceType":"module"}