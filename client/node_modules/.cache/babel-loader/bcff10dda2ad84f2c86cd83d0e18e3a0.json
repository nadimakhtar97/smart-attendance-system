{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\n\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  let epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  let out;\n\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` + `yet`);\n  }\n\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const targetShape = [];\n\n    for (const axis of math_utils.range(0, x.rank)) {\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n\n    const broadcastMean = reshape(mean, targetShape);\n    const broadcastVariance = reshape(variance, targetShape);\n    const broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    const broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport class BatchNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n\n    super(args);\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n    const dim = inputShape[axis];\n\n    if (dim == null) {\n      throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` + `the layer received an input with shape ` + `${JSON.stringify(inputShape)}.`);\n    }\n\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes: {\n        [axis]: dim\n      }\n    })];\n    const shape = [dim];\n\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n    }\n\n    if (this.center) {\n      this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n    }\n\n    this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : this.axis + ndim;\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean = reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance = reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta = this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma = this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n\n      const doMovingAverage = (variable, value, momentum) => {\n        tfc.tidy(() => {\n          const decay = 1 - momentum;\n          const origValue = variable.read();\n          const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n          variable.write(tfc.sub(origValue, updateDelta));\n        });\n      }; // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n\n\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n\n      updateMovingMeanAndVariance();\n      return normedTraining;\n    });\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n\n    super(args);\n    this.axis = args.axis == null ? -1 : args.axis;\n\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(`Expected axis to be an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(`Expected axis to be an integer or an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    this.supportsMasking = true;\n  }\n\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length; // Convert axis to array and resolve negatives.\n\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    } // Further validate axes.\n\n\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]);\n    const trainable = true;\n\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n\n    if (this.center) {\n      this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  call(inputs, kwargs) {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n    return tidy(() => {\n      const keepDims = true;\n      let {\n        mean,\n        variance\n      } = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n\n      for (const dim of this.axis) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = v => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = broadcast(this.gamma.read());\n      let offset = broadcast(this.beta.read()); // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n\n      const momentsTiling = [];\n      const scaleOffsetTiling = [];\n\n      for (let i = 0; i < nDims; ++i) {\n        if (this.axis.indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      scale = tfc.tile(scale, scaleOffsetTiling);\n      offset = tfc.tile(offset, scaleOffsetTiling);\n      return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n}\n/** @nocollapse */\n\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAZ,MAAqB,uBAArB;AACA,SAAQC,OAAR,EAAiBC,OAAjB,EAA0BC,aAA1B,EAAyFC,IAAzF,EAA+FC,IAA/F,QAA0G,uBAA1G;AAEA,SAA0CC,aAA1C,EAAyDC,mBAAzD,QAAmF,gBAAnF;AACA,SAAQC,SAAR,EAAmBC,KAAnB,QAA0C,oBAA1C;AACA,SAAQC,mBAAR,EAA6BC,UAA7B,QAA8C,WAA9C;AACA,SAAQC,cAAR,EAA4DC,oBAA5D,QAAuF,iBAAvF;AAEA,SAAQC,cAAR,EAA4DC,oBAA5D,QAAuF,iBAAvF;AAEA,OAAO,KAAKC,aAAZ,MAA+B,wBAA/B;AACA,OAAO,KAAKC,UAAZ,MAA4B,qBAA5B;AACA,SAAQC,kBAAR,EAA4BC,mBAA5B,QAAsD,sBAAtD;AAGA;;;;;;;;;;;;;;;AAcA,OAAM,SAAUC,kBAAV,CACFC,CADE,EACSC,IADT,EACuBC,QADvB,EACyCC,IADzC,EACwDC,KADxD,EAEY;AAAA,MAAdC,OAAc,uEAAJ,IAAI;AAChB,MAAIC,GAAJ;;AACA,MAAIN,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;AAChBD,OAAG,GAAG3B,GAAG,CAAC6B,WAAJ,CACFR,CADE,EACaC,IADb,EAEFC,QAFE,EAE+BC,IAF/B,EAGFC,KAHE,EAG4BC,OAH5B,CAAN;AAID,GALD,MAKO,IAAIL,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;AACvB;AACAD,OAAG,GAAG3B,GAAG,CAAC8B,WAAJ,CACFT,CADE,EACaC,IADb,EAEFC,QAFE,EAE+BC,IAF/B,EAGFC,KAHE,EAG4BC,OAH5B,CAAN;AAID,GANM,MAMA,IAAIL,CAAC,CAACO,IAAF,KAAW,CAAf,EAAkB;AACvBD,OAAG,GAAG3B,GAAG,CAAC+B,WAAJ,CACFV,CADE,EACaC,IADb,EAEFC,QAFE,EAE+BC,IAF/B,EAGFC,KAHE,EAG4BC,OAH5B,CAAN;AAID,GALM,MAKA;AACL,UAAM,IAAIhB,mBAAJ,CACF,2DAA2DW,CAAC,CAACO,IAAI,GAAjE,GACA,KAFE,CAAN;AAGD;;AACD,SAAOD,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;AAiBA,SAASK,+BAAT,CACIX,CADJ,EACeI,KADf,EAC8BD,IAD9B,EAC4CS,aAD5C,EAEkB;AAAA,MAAdP,OAAc,uEAAJ,IAAI;AAChB,SAAOtB,IAAI,CAAC,MAAK;AACR,UAAM8B,eAAe,GAAGlC,GAAG,CAACC,OAAJ,CAAYoB,CAAZ,EAAeY,aAAf,CAAxB;AACA,UAAMX,IAAI,GAAGY,eAAe,CAACZ,IAA7B;AACA,UAAMC,QAAQ,GAAGW,eAAe,CAACX,QAAjC;AACA,UAAMY,MAAM,GACRf,kBAAkB,CAACC,CAAD,EAAIC,IAAJ,EAAUC,QAAV,EAAoBC,IAApB,EAA0BC,KAA1B,EAAiCC,OAAjC,CADtB;AAEA,WAAO,CAACS,MAAD,EAASb,IAAT,EAAeC,QAAf,CAAP;AACD,GAPG,CAAX;AAQD;AAED;;;;;;;;;;;;;;;;;;;AAiBA,SAASa,iCAAT,CACIf,CADJ,EACeI,KADf,EAC8BD,IAD9B,EAC4CS,aAD5C,EAEkB;AAAA,MAAdP,OAAc,uEAAJ,IAAI;AAChB,SAAOtB,IAAI,CAAC,MAAK;AACR,UAAM8B,eAAe,GAAGlC,GAAG,CAACC,OAAJ,CAAYoB,CAAZ,EAAeY,aAAf,CAAxB;AACA,UAAMX,IAAI,GAAGY,eAAe,CAACZ,IAA7B;AACA,UAAMC,QAAQ,GAAGW,eAAe,CAACX,QAAjC;AACA,UAAMc,WAAW,GAAa,EAA9B;;AACA,SAAK,MAAMC,IAAX,IAAmBrB,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBlB,CAAC,CAACO,IAAtB,CAAnB,EAAgD;AAC9C,UAAIK,aAAa,CAACO,OAAd,CAAsBF,IAAtB,MAAgC,CAAC,CAArC,EAAwC;AACtCD,mBAAW,CAACI,IAAZ,CAAiB,CAAjB;AACD,OAFD,MAEO;AACLJ,mBAAW,CAACI,IAAZ,CAAiBpB,CAAC,CAACqB,KAAF,CAAQJ,IAAR,CAAjB;AACD;AACF;;AACD,UAAMK,aAAa,GAAGzC,OAAO,CAACoB,IAAD,EAAOe,WAAP,CAA7B;AACA,UAAMO,iBAAiB,GAAG1C,OAAO,CAACqB,QAAD,EAAWc,WAAX,CAAjC;AACA,UAAMQ,cAAc,GAChBpB,KAAK,IAAI,IAAT,GAAgB,IAAhB,GAAuBvB,OAAO,CAACuB,KAAD,EAAQY,WAAR,CADlC;AAEA,UAAMS,aAAa,GACftB,IAAI,IAAI,IAAR,GAAe,IAAf,GAAsBtB,OAAO,CAACsB,IAAD,EAAOa,WAAP,CADjC;AAEA,UAAMF,MAAM,GAAGf,kBAAkB,CAC7BC,CAD6B,EAC1BsB,aAD0B,EACXC,iBADW,EACQE,aADR,EAE7BD,cAF6B,EAEbnB,OAFa,CAAjC;AAGA,WAAO,CAACS,MAAD,EAASb,IAAT,EAAeC,QAAf,CAAP;AACD,GAtBG,CAAX;AAuBD;AAED;;;;;;;;;;;;;AAWA,OAAM,SAAUwB,wBAAV,CACF1B,CADE,EACSI,KADT,EACwBD,IADxB,EACsCS,aADtC,EAEY;AAAA,MAAdP,OAAc,uEAAJ,IAAI;;AAChB,MAAIrB,IAAI,CAAC2C,WAAL,CACIf,aAAa,CAACgB,KAAd,GAAsBC,IAAtB,EADJ,EACkCjC,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBlB,CAAC,CAACO,IAAF,GAAS,CAA7B,CADlC,CAAJ,EACwE;AACtE,WAAOI,+BAA+B,CAClCX,CADkC,EAC/BI,KAD+B,EACxBD,IADwB,EAClBS,aADkB,EACHP,OADG,CAAtC;AAED,GAJD,MAIO;AACL,WAAOU,iCAAiC,CACpCf,CADoC,EACjCI,KADiC,EAC1BD,IAD0B,EACpBS,aADoB,EACLP,OADK,CAAxC;AAED;AACF;AAoFD,OAAM,MAAOyB,kBAAP,SAAkC1C,KAAlC,CAAuC;AAqB3C2C,cAAYC,IAAZ,EAA8C;AAC5C,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AACD,UAAMA,IAAN;AAEA,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKhB,IAAL,GAAYe,IAAI,CAACf,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyBe,IAAI,CAACf,IAA1C;AACA,SAAKiB,QAAL,GAAgBF,IAAI,CAACE,QAAL,IAAiB,IAAjB,GAAwB,IAAxB,GAA+BF,IAAI,CAACE,QAApD;AACA,SAAK7B,OAAL,GAAe2B,IAAI,CAAC3B,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B2B,IAAI,CAAC3B,OAAlD;AACA,SAAK8B,MAAL,GAAcH,IAAI,CAACG,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6BH,IAAI,CAACG,MAAhD;AACA,SAAKC,KAAL,GAAaJ,IAAI,CAACI,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4BJ,IAAI,CAACI,KAA9C;AACA,SAAKC,eAAL,GAAuB9C,cAAc,CAACyC,IAAI,CAACK,eAAL,IAAwB,OAAzB,CAArC;AACA,SAAKC,gBAAL,GAAwB/C,cAAc,CAACyC,IAAI,CAACM,gBAAL,IAAyB,MAA1B,CAAtC;AACA,SAAKC,qBAAL,GACIhD,cAAc,CAACyC,IAAI,CAACO,qBAAL,IAA8B,OAA/B,CADlB;AAEA,SAAKC,yBAAL,GACIjD,cAAc,CAACyC,IAAI,CAACQ,yBAAL,IAAkC,MAAnC,CADlB;AAEA,SAAKC,cAAL,GAAsBxD,aAAa,CAAC+C,IAAI,CAACS,cAAN,CAAnC;AACA,SAAKC,eAAL,GAAuBzD,aAAa,CAAC+C,IAAI,CAACU,eAAN,CAApC;AACA,SAAKC,eAAL,GAAuBlD,cAAc,CAACuC,IAAI,CAACW,eAAN,CAArC;AACA,SAAKC,gBAAL,GAAwBnD,cAAc,CAACuC,IAAI,CAACY,gBAAN,CAAtC;AACD;;AAEMC,OAAK,CAACC,UAAD,EAA0B;AACpCA,cAAU,GAAGjD,kBAAkB,CAACiD,UAAD,CAA/B;AACA,UAAM7B,IAAI,GAAG,KAAKA,IAAL,IAAa,CAAb,GAAiB,KAAKA,IAAtB,GAA8B,KAAKA,IAAL,GAAY6B,UAAU,CAACC,MAAlE;AACA,UAAMC,GAAG,GAAGF,UAAU,CAAC7B,IAAD,CAAtB;;AACA,QAAI+B,GAAG,IAAI,IAAX,EAAiB;AACf,YAAM,IAAI1D,UAAJ,CACF,QAAQ2B,IAAI,uDAAZ,GACA,yCADA,GAEA,GAAGgC,IAAI,CAACC,SAAL,CAAeJ,UAAf,CAA0B,GAH3B,CAAN;AAID;;AACD,SAAKK,SAAL,GACI,CAAC,IAAIhE,SAAJ,CAAc;AAACiE,UAAI,EAAEN,UAAU,CAACC,MAAlB;AAA0BM,UAAI,EAAE;AAAC,SAACpC,IAAD,GAAQ+B;AAAT;AAAhC,KAAd,CAAD,CADJ;AAEA,UAAM3B,KAAK,GAAG,CAAC2B,GAAD,CAAd;;AACA,QAAI,KAAKZ,KAAT,EAAgB;AACd,WAAKhC,KAAL,GAAa,KAAKkD,SAAL,CACT,OADS,EACAjC,KADA,EACO,IADP,EACa,KAAKiB,gBADlB,EACoC,KAAKM,gBADzC,EAET,IAFS,EAEH,KAAKF,eAFF,CAAb;AAGD;;AACD,QAAI,KAAKP,MAAT,EAAiB;AACf,WAAKhC,IAAL,GAAY,KAAKmD,SAAL,CACR,MADQ,EACAjC,KADA,EACO,IADP,EACa,KAAKgB,eADlB,EACmC,KAAKM,eADxC,EACyD,IADzD,EAER,KAAKF,cAFG,CAAZ;AAGD;;AACD,SAAKc,UAAL,GAAkB,KAAKD,SAAL,CACd,aADc,EACCjC,KADD,EACQ,IADR,EACc,KAAKkB,qBADnB,EAC0C,IAD1C,EACgD,KADhD,CAAlB;AAEA,SAAKiB,cAAL,GAAsB,KAAKF,SAAL,CAClB,iBADkB,EACCjC,KADD,EACQ,IADR,EACc,KAAKmB,yBADnB,EAC8C,IAD9C,EAElB,KAFkB,CAAtB;AAGA,SAAKiB,KAAL,GAAa,IAAb;AACD;;AAEDC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAO7E,IAAI,CAAC,MAAK;AACf,YAAM8E,QAAQ,GAAGD,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqCA,MAAM,CAAC,UAAD,CAA5D;AACA,YAAME,KAAK,GAAGhE,mBAAmB,CAAC6D,MAAD,CAAjC;AACA,YAAMb,UAAU,GAAGgB,KAAK,CAACzC,KAAzB;AACA,YAAM+B,IAAI,GAAGN,UAAU,CAACC,MAAxB;AACA,YAAMnC,aAAa,GAAGhB,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBkC,IAApB,CAAtB;AACA,YAAMnC,IAAI,GAAG,KAAKA,IAAL,IAAa,CAAb,GAAiB,KAAKA,IAAtB,GAA8B,KAAKA,IAAL,GAAYmC,IAAvD;AACAxC,mBAAa,CAACmD,MAAd,CAAqB9C,IAArB,EAA2B,CAA3B;AACA,YAAM+C,cAAc,GAAGrE,aAAa,CAACsE,YAAd,CAA2B,CAA3B,EAA8Bb,IAA9B,CAAvB;AACAY,oBAAc,CAAC/C,IAAD,CAAd,GAAuB6B,UAAU,CAAC7B,IAAD,CAAjC;AAEA,YAAMiD,mBAAmB,GAAGtD,aAAa,CAACgB,KAAd,EAA5B;AACAsC,yBAAmB,CAACrC,IAApB;AACA,YAAMsC,iBAAiB,GAAG,CAACnF,IAAI,CAAC2C,WAAL,CACvBuC,mBADuB,EACFtE,UAAU,CAACsB,KAAX,CAAiB,CAAjB,EAAoBkC,IAApB,EAA0BxB,KAA1B,CAAgC,CAAhC,EAAmCwB,IAAI,GAAG,CAA1C,CADE,CAA3B;;AAGA,YAAMgB,kBAAkB,GAAiB,MAAK;AAC5C,YAAID,iBAAJ,EAAuB;AACrB,gBAAME,mBAAmB,GACrBxF,OAAO,CAAC,KAAK0E,UAAL,CAAgBe,IAAhB,EAAD,EAAyBN,cAAzB,CADX;AAEA,gBAAMO,uBAAuB,GACzB1F,OAAO,CAAC,KAAK2E,cAAL,CAAoBc,IAApB,EAAD,EAA6BN,cAA7B,CADX;AAEA,gBAAMvC,aAAa,GACf,KAAKU,MAAL,GAActD,OAAO,CAAC,KAAKsB,IAAL,CAAUmE,IAAV,EAAD,EAAmBN,cAAnB,CAArB,GAA0D,IAD9D;AAEA,gBAAMxC,cAAc,GAChB,KAAKY,KAAL,GAAavD,OAAO,CAAC,KAAKuB,KAAL,CAAWkE,IAAX,EAAD,EAAoBN,cAApB,CAApB,GAA0D,IAD9D;AAEA,iBAAOjE,kBAAkB,CACrB+D,KADqB,EACdO,mBADc,EACOE,uBADP,EAErB9C,aAFqB,EAEND,cAFM,EAEU,KAAKnB,OAFf,CAAzB;AAGD,SAZD,MAYO;AACL,iBAAON,kBAAkB,CACrB+D,KADqB,EACd,KAAKP,UAAL,CAAgBe,IAAhB,EADc,EACU,KAAKd,cAAL,CAAoBc,IAApB,EADV,EAErB,KAAKnE,IAAL,IAAa,IAAb,GAAoB,IAApB,GAA2B,KAAKA,IAAL,CAAUmE,IAAV,EAFN,EAGrB,KAAKlE,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,KAAKA,KAAL,CAAWkE,IAAX,EAHP,EAG0B,KAAKjE,OAH/B,CAAzB;AAID;AACF,OAnBD;;AAqBA,UAAI,CAACwD,QAAL,EAAe;AACb,eAAOO,kBAAkB,EAAzB;AACD;;AAED,YAAM,CAACI,cAAD,EAAiBvE,IAAjB,EAAuBC,QAAvB,IAAmCwB,wBAAwB,CAC7DoC,KAD6D,EACtD,KAAK1D,KAAL,CAAWkE,IAAX,EADsD,EACnC,KAAKnE,IAAL,CAAUmE,IAAV,EADmC,EACjB1D,aADiB,EAE7D,KAAKP,OAFwD,CAAjE;;AAIA,YAAMoE,eAAe,GACjB,CAACC,QAAD,EAA0BC,KAA1B,EAAyCzC,QAAzC,KAAmE;AACjEvD,WAAG,CAACI,IAAJ,CAAS,MAAK;AACZ,gBAAM6F,KAAK,GAAG,IAAI1C,QAAlB;AACA,gBAAM2C,SAAS,GAAGH,QAAQ,CAACJ,IAAT,EAAlB;AACA,gBAAMQ,WAAW,GAAGnG,GAAG,CAACoG,GAAJ,CAAQpG,GAAG,CAACqG,GAAJ,CAAQH,SAAR,EAAmBF,KAAnB,CAAR,EAAmCC,KAAnC,CAApB;AACAF,kBAAQ,CAACO,KAAT,CAAetG,GAAG,CAACqG,GAAJ,CAAQH,SAAR,EAAmBC,WAAnB,CAAf;AACD,SALD;AAMD,OARL,CA7Ce,CAuDf;AACA;AACA;AACA;AACA;AACA;;;AACA,YAAMI,2BAA2B,GAAG,MAAK;AACvCT,uBAAe,CAAC,KAAKlB,UAAN,EAAkBtD,IAAlB,EAAwB,KAAKiC,QAA7B,CAAf;AACAuC,uBAAe,CAAC,KAAKjB,cAAN,EAAsBtD,QAAtB,EAAgC,KAAKgC,QAArC,CAAf;AACD,OAHD;;AAIAgD,iCAA2B;AAE3B,aAAOV,cAAP;AACD,KApEU,CAAX;AAqED;;AAEDW,WAAS;AACP,UAAMC,MAAM,GAA6B;AACvCnE,UAAI,EAAE,KAAKA,IAD4B;AAEvCiB,cAAQ,EAAE,KAAKA,QAFwB;AAGvC7B,aAAO,EAAE,KAAKA,OAHyB;AAIvC8B,YAAM,EAAE,KAAKA,MAJ0B;AAKvCC,WAAK,EAAE,KAAKA,KAL2B;AAMvCC,qBAAe,EAAE7C,oBAAoB,CAAC,KAAK6C,eAAN,CANE;AAOvCC,sBAAgB,EAAE9C,oBAAoB,CAAC,KAAK8C,gBAAN,CAPC;AAQvCC,2BAAqB,EAAE/C,oBAAoB,CAAC,KAAK+C,qBAAN,CARJ;AASvCC,+BAAyB,EACrBhD,oBAAoB,CAAC,KAAKgD,yBAAN,CAVe;AAWvCG,qBAAe,EAAEjD,oBAAoB,CAAC,KAAKiD,eAAN,CAXE;AAYvCC,sBAAgB,EAAElD,oBAAoB,CAAC,KAAKkD,gBAAN,CAZC;AAavCH,oBAAc,EAAEvD,mBAAmB,CAAC,KAAKuD,cAAN,CAbI;AAcvCC,qBAAe,EAAExD,mBAAmB,CAAC,KAAKwD,eAAN;AAdG,KAAzC;AAgBA,UAAM2C,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AAxK0C;AAC3C;;AACOtD,+BAAY,oBAAZ;AAwKThD,aAAa,CAAC0G,aAAd,CAA4B1D,kBAA5B;AAkDA,OAAM,MAAO2D,kBAAP,SAAkCrG,KAAlC,CAAuC;AAgB3C2C,cAAYC,IAAZ,EAA8C;AAC5C,QAAIA,IAAI,IAAI,IAAZ,EAAkB;AAChBA,UAAI,GAAG,EAAP;AACD;;AACD,UAAMA,IAAN;AAEA,SAAKf,IAAL,GAAYe,IAAI,CAACf,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyBe,IAAI,CAACf,IAA1C;;AACA,QAAI,OAAO,KAAKA,IAAZ,KAAqB,QAAzB,EAAmC;AACjC,UAAI,CAACyE,MAAM,CAACC,SAAP,CAAiB,KAAK1E,IAAtB,CAAL,EAAkC;AAChC,cAAM,IAAI2E,KAAJ,CACF,gDAAgD,KAAK3E,IAAI,EADvD,CAAN;AAED;AACF,KALD,MAKO,IAAI4E,KAAK,CAACC,OAAN,CAAc,KAAK7E,IAAnB,CAAJ,EAA8B;AACnC,WAAK,MAAMA,IAAX,IAAmB,KAAKA,IAAxB,EAA8B;AAC5B,YAAI,CAACyE,MAAM,CAACC,SAAP,CAAiB1E,IAAjB,CAAL,EAA6B;AAC3B,gBAAM,IAAI2E,KAAJ,CACF,+CACA,gBAAgB3C,IAAI,CAACC,SAAL,CAAe,KAAKjC,IAApB,CAAyB,EAFvC,CAAN;AAGD;AACF;AACF,KARM,MAQA;AACL,YAAM,IAAI2E,KAAJ,CACF,6DACA,gBAAgB3C,IAAI,CAACC,SAAL,CAAe,KAAKjC,IAApB,CAAyB,EAFvC,CAAN;AAGD;;AAED,SAAKZ,OAAL,GAAe2B,IAAI,CAAC3B,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B2B,IAAI,CAAC3B,OAAlD;AACA,SAAK8B,MAAL,GAAcH,IAAI,CAACG,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6BH,IAAI,CAACG,MAAhD;AACA,SAAKC,KAAL,GAAaJ,IAAI,CAACI,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4BJ,IAAI,CAACI,KAA9C;AACA,SAAKC,eAAL,GAAuB9C,cAAc,CAACyC,IAAI,CAACK,eAAL,IAAwB,OAAzB,CAArC;AACA,SAAKC,gBAAL,GAAwB/C,cAAc,CAACyC,IAAI,CAACM,gBAAL,IAAyB,MAA1B,CAAtC;AACA,SAAKK,eAAL,GAAuBlD,cAAc,CAACuC,IAAI,CAACW,eAAN,CAArC;AACA,SAAKC,gBAAL,GAAwBnD,cAAc,CAACuC,IAAI,CAACY,gBAAN,CAAtC;AAEA,SAAKX,eAAL,GAAuB,IAAvB;AACD;;AAEMY,OAAK,CAACC,UAAD,EAA0B;AACpCA,cAAU,GAAGjD,kBAAkB,CAACiD,UAAD,CAA/B;AACA,UAAMiD,KAAK,GAAGjD,UAAU,CAACC,MAAzB,CAFoC,CAIpC;;AACA,QAAI,OAAO,KAAK9B,IAAZ,KAAqB,QAAzB,EAAmC;AACjC,WAAKA,IAAL,GAAY,CAAC,KAAKA,IAAN,CAAZ;AACD;;AACD,SAAK,IAAI+E,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAAK/E,IAAL,CAAU8B,MAA9B,EAAsC,EAAEiD,CAAxC,EAA2C;AACzC,UAAI,KAAK/E,IAAL,CAAU+E,CAAV,IAAe,CAAnB,EAAsB;AACpB,aAAK/E,IAAL,CAAU+E,CAAV,KAAgBD,KAAhB;AACD;AACF,KAZmC,CAcpC;;;AACA,SAAK,MAAM9E,IAAX,IAAmB,KAAKA,IAAxB,EAA8B;AAC5B,UAAIA,IAAI,GAAG,CAAP,IAAYA,IAAI,IAAI8E,KAAxB,EAA+B;AAC7B,cAAM,IAAIH,KAAJ,CAAU,iBAAiB3E,IAAI,EAA/B,CAAN;AACD;AACF;;AACD,QAAI,KAAKA,IAAL,CAAU8B,MAAV,KAAqBpD,aAAa,CAACsG,MAAd,CAAqB,KAAKhF,IAA1B,EAAgC8B,MAAzD,EAAiE;AAC/D,YAAM,IAAI6C,KAAJ,CAAU,4BAA4B,KAAK3E,IAAI,EAA/C,CAAN;AACD;;AAED,UAAMiF,UAAU,GAAG,KAAKjF,IAAL,CAAUkF,GAAV,CAAclF,IAAI,IAAI6B,UAAU,CAAC7B,IAAD,CAAhC,CAAnB;AAEA,UAAMmF,SAAS,GAAG,IAAlB;;AACA,QAAI,KAAKhE,KAAT,EAAgB;AACd,WAAKhC,KAAL,GAAa,KAAKkD,SAAL,CACT,OADS,EACA4C,UADA,EACY,SADZ,EACuB,KAAK5D,gBAD5B,EAET,KAAKM,gBAFI,EAEcwD,SAFd,CAAb;AAGD,KAJD,MAIO;AACL,WAAKhG,KAAL,GAAa,IAAb;AACD;;AACD,QAAI,KAAK+B,MAAT,EAAiB;AACf,WAAKhC,IAAL,GAAY,KAAKmD,SAAL,CACR,MADQ,EACA4C,UADA,EACY,SADZ,EACuB,KAAK7D,eAD5B,EAER,KAAKM,eAFG,EAEcyD,SAFd,CAAZ;AAGD,KAJD,MAIO;AACL,WAAKjG,IAAL,GAAY,IAAZ;AACD;;AAED,SAAKsD,KAAL,GAAa,IAAb;AACD;;AAEDC,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,UAAME,KAAK,GAAGhE,mBAAmB,CAAC6D,MAAD,CAAjC;AACA,UAAMb,UAAU,GAAGgB,KAAK,CAACzC,KAAzB;AACA,UAAM0E,KAAK,GAAGjD,UAAU,CAACC,MAAzB;AAEA,WAAOhE,IAAI,CAAC,MAAK;AACf,YAAMsH,QAAQ,GAAG,IAAjB;AACA,UAAI;AAACpG,YAAD;AAAOC;AAAP,UAAmBtB,OAAO,CAACkF,KAAD,EAAQ,KAAK7C,IAAb,EAAmBoF,QAAnB,CAA9B;AACA,YAAMrC,cAAc,GAAGrE,aAAa,CAACsE,YAAd,CAA2B,CAA3B,EAA8B8B,KAA9B,CAAvB;;AACA,WAAK,MAAM/C,GAAX,IAAkB,KAAK/B,IAAvB,EAAyC;AACvC+C,sBAAc,CAAChB,GAAD,CAAd,GAAsBF,UAAU,CAACE,GAAD,CAAhC;AACD;;AAED,YAAMsD,SAAS,GAAIC,CAAD,IAAc;AAC9B,YAAIA,CAAC,IAAI,IAAL,IAAaA,CAAC,CAAClF,KAAF,CAAQ0B,MAAR,KAAmBgD,KAApC,EAA2C;AACzC,iBAAOpH,GAAG,CAACE,OAAJ,CAAY0H,CAAZ,EAAevC,cAAf,CAAP;AACD,SAFD,MAEO;AACL,iBAAOuC,CAAP;AACD;AACF,OAND;;AAQA,UAAInE,KAAK,GAAGkE,SAAS,CAAC,KAAKlG,KAAL,CAAWkE,IAAX,EAAD,CAArB;AACA,UAAIkC,MAAM,GAAGF,SAAS,CAAC,KAAKnG,IAAL,CAAUmE,IAAV,EAAD,CAAtB,CAjBe,CAmBf;AACA;AACA;AACA;AACA;AACA;;AACA,YAAMmC,aAAa,GAAa,EAAhC;AACA,YAAMC,iBAAiB,GAAa,EAApC;;AACA,WAAK,IAAIV,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGD,KAApB,EAA2B,EAAEC,CAA7B,EAAgC;AAC9B,YAAK,KAAK/E,IAAL,CAAuBE,OAAvB,CAA+B6E,CAA/B,MAAsC,CAAC,CAA5C,EAA+C;AAC7CS,uBAAa,CAACrF,IAAd,CAAmB0B,UAAU,CAACkD,CAAD,CAA7B;AACAU,2BAAiB,CAACtF,IAAlB,CAAuB,CAAvB;AACD,SAHD,MAGO;AACLqF,uBAAa,CAACrF,IAAd,CAAmB,CAAnB;AACAsF,2BAAiB,CAACtF,IAAlB,CAAuB0B,UAAU,CAACkD,CAAD,CAAjC;AACD;AACF;;AACD/F,UAAI,GAAGtB,GAAG,CAACgI,IAAJ,CAAS1G,IAAT,EAAewG,aAAf,CAAP;AACAvG,cAAQ,GAAGvB,GAAG,CAACgI,IAAJ,CAASzG,QAAT,EAAmBuG,aAAnB,CAAX;AACArE,WAAK,GAAGzD,GAAG,CAACgI,IAAJ,CAASvE,KAAT,EAAgBsE,iBAAhB,CAAR;AACAF,YAAM,GAAG7H,GAAG,CAACgI,IAAJ,CAASH,MAAT,EAAiBE,iBAAjB,CAAT;AAEA,aAAO3G,kBAAkB,CACrB+D,KADqB,EACd7D,IADc,EACRC,QADQ,EACEsG,MADF,EACUpE,KADV,EACiB,KAAK/B,OADtB,CAAzB;AAED,KA3CU,CAAX;AA4CD;;AAED8E,WAAS;AACP,UAAMC,MAAM,GAA6B;AACvCnE,UAAI,EAAE,KAAKA,IAD4B;AAEvCZ,aAAO,EAAE,KAAKA,OAFyB;AAGvC8B,YAAM,EAAE,KAAKA,MAH0B;AAIvCC,WAAK,EAAE,KAAKA,KAJ2B;AAKvCC,qBAAe,EAAE7C,oBAAoB,CAAC,KAAK6C,eAAN,CALE;AAMvCC,sBAAgB,EAAE9C,oBAAoB,CAAC,KAAK8C,gBAAN,CANC;AAOvCK,qBAAe,EAAEjD,oBAAoB,CAAC,KAAKiD,eAAN,CAPE;AAQvCC,sBAAgB,EAAElD,oBAAoB,CAAC,KAAKkD,gBAAN;AARC,KAAzC;AAUA,UAAMyC,UAAU,GAAG,MAAMF,SAAN,EAAnB;AACAG,UAAM,CAACC,MAAP,CAAcH,MAAd,EAAsBC,UAAtB;AACA,WAAOD,MAAP;AACD;;AAnK0C;AAC3C;;AACOK,+BAAY,oBAAZ;AAmKT3G,aAAa,CAAC0G,aAAd,CAA4BC,kBAA5B","names":["tfc","moments","reshape","serialization","tidy","util","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","generic_utils","math_utils","getExactlyOneShape","getExactlyOneTensor","batchNormalization","x","mean","variance","beta","gamma","epsilon","out","rank","batchNorm2d","batchNorm3d","batchNorm4d","regularNormalizeBatchInTraining","reductionAxes","meanAndVariance","normed","broadcastNormalizeBatchInTraining","targetShape","axis","range","indexOf","push","shape","broadcastMean","broadcastVariance","broadcastGamma","broadcastBeta","normalizeBatchInTraining","arraysEqual","slice","sort","BatchNormalization","constructor","args","supportsMasking","momentum","center","scale","betaInitializer","gammaInitializer","movingMeanInitializer","movingVarianceInitializer","betaConstraint","gammaConstraint","betaRegularizer","gammaRegularizer","build","inputShape","length","dim","JSON","stringify","inputSpec","ndim","axes","addWeight","movingMean","movingVariance","built","call","inputs","kwargs","training","input","splice","broadcastShape","pyListRepeat","sortedReductionAxes","needsBroadcasting","normalizeInference","broadcastMovingMean","read","broadcastMovingVariance","normedTraining","doMovingAverage","variable","value","decay","origValue","updateDelta","mul","sub","write","updateMovingMeanAndVariance","getConfig","config","baseConfig","Object","assign","registerClass","LayerNormalization","Number","isInteger","Error","Array","isArray","nDims","i","unique","paramShape","map","trainable","keepDims","broadcast","v","offset","momentsTiling","scaleOffsetTiling","tile"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/layers/normalization.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {moments, reshape, serialization, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(\n    x: Tensor, mean: Tensor, variance: Tensor, beta?: Tensor, gamma?: Tensor,\n    epsilon = 1e-3): Tensor {\n  let out: Tensor;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(\n        x as Tensor2D, mean as Tensor2D | Tensor1D,\n        variance as Tensor2D | Tensor1D, beta as Tensor2D | Tensor1D,\n        gamma as Tensor2D | Tensor1D, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(\n        x as Tensor3D, mean as Tensor3D | Tensor1D,\n        variance as Tensor3D | Tensor1D, beta as Tensor3D | Tensor1D,\n        gamma as Tensor3D | Tensor1D, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(\n        x as Tensor4D, mean as Tensor4D | Tensor1D,\n        variance as Tensor4D | Tensor1D, beta as Tensor4D | Tensor1D,\n        gamma as Tensor4D | Tensor1D, epsilon);\n  } else {\n    throw new NotImplementedError(\n        `batchNormalization is not implemented for array of rank ${x.rank} ` +\n        `yet`);\n  }\n  return out;\n}\n\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const normed =\n               batchNormalization(x, mean, variance, beta, gamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const targetShape: number[] = [];\n           for (const axis of math_utils.range(0, x.rank)) {\n             if (reductionAxes.indexOf(axis) !== -1) {\n               targetShape.push(1);\n             } else {\n               targetShape.push(x.shape[axis]);\n             }\n           }\n           const broadcastMean = reshape(mean, targetShape);\n           const broadcastVariance = reshape(variance, targetShape);\n           const broadcastGamma =\n               gamma == null ? null : reshape(gamma, targetShape);\n           const broadcastBeta =\n               beta == null ? null : reshape(beta, targetShape);\n           const normed = batchNormalization(\n               x, broadcastMean, broadcastVariance, broadcastBeta,\n               broadcastGamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  if (util.arraysEqual(\n          reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexport declare interface BatchNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The integer axis that should be normalized (typically the features axis).\n   * Defaults to -1.\n   *\n   * For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`,\n   * set `axis=1` in `batchNormalization`.\n   */\n  axis?: number;\n\n  /**\n   * Momentum of the moving average. Defaults to 0.99.\n   */\n  momentum?: number;\n\n  /**\n   * Small float added to the variance to avoid dividing by zero. Defaults to\n   * 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Defaults to `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear (also e.g. `nn.relu`),\n   * this can be disabled since the scaling will be done by the next layer.\n   * Defaults to `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   *  Defaults to 'zeros'.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   *  Defaults to `ones`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving mean.\n   * Defaults to `zeros`\n   */\n  movingMeanInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving variance.\n   *  Defaults to 'Ones'.\n   */\n  movingVarianceInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Constraint for the beta weight.\n   */\n  betaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for gamma weight.\n   */\n  gammaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer for the beta weight.\n   */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer for the gamma weight.\n   */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class BatchNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'BatchNormalization';\n  private readonly axis: number;\n  private readonly momentum: number;\n  private readonly epsilon: number;\n  private readonly center: boolean;\n  private readonly scale: boolean;\n  private readonly betaInitializer: Initializer;\n  private readonly gammaInitializer: Initializer;\n  private readonly movingMeanInitializer: Initializer;\n  private readonly movingVarianceInitializer: Initializer;\n  private readonly betaConstraint: Constraint;\n  private readonly gammaConstraint: Constraint;\n  private readonly betaRegularizer: Regularizer;\n  private readonly gammaRegularizer: Regularizer;\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n  private movingMean: LayerVariable;\n  private movingVariance: LayerVariable;\n\n  constructor(args?: BatchNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer =\n        getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer =\n        getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(\n          `Axis ${axis} of input tensor should have a defined dimension but ` +\n          `the layer received an input with shape ` +\n          `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec =\n        [new InputSpec({ndim: inputShape.length, axes: {[axis]: dim}})];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', shape, null, this.gammaInitializer, this.gammaRegularizer,\n          true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', shape, null, this.betaInitializer, this.betaRegularizer, true,\n          this.betaConstraint);\n    }\n    this.movingMean = this.addWeight(\n        'moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight(\n        'moving_variance', shape, null, this.movingVarianceInitializer, null,\n        false);\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(\n          sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference: () => Tensor = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean =\n              reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance =\n              reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta =\n              this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma =\n              this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(\n              input, broadcastMovingMean, broadcastMovingVariance,\n              broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(\n              input, this.movingMean.read(), this.movingVariance.read(),\n              this.beta == null ? null : this.beta.read(),\n              this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(\n          input, this.gamma.read(), this.beta.read(), reductionAxes,\n          this.epsilon);\n\n      const doMovingAverage =\n          (variable: LayerVariable, value: Tensor, momentum: number): void => {\n            tfc.tidy(() => {\n              const decay = 1 - momentum;\n              const origValue = variable.read();\n              const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n              variable.write(tfc.sub(origValue, updateDelta));\n            });\n          };\n\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n\n      return normedTraining;\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer:\n          serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(BatchNormalization);\n\nexport interface LayerNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The axis or axes that should be normalized (typically, the feature axis.)\n   * Defaults to -1 (the last axis.)\n   */\n  axis?: number|number[];\n\n  /**\n   * A small positive float added to variance to avoid divison by zero.\n   * Defaults to 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Default: `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply output by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear, this can be disabled since scaling will\n   * be done by the next layer.\n   * Default: `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   * Default: `'zeros'`.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   * Default: `'ones'`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /** Regularizer for the beta weight. */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /** Regularizer for the gamma weight. */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class LayerNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'LayerNormalization';\n\n  private axis: number|number[];\n  readonly epsilon: number;\n  readonly center: boolean;\n  readonly scale: boolean;\n  readonly betaInitializer: Initializer;\n  readonly gammaInitializer: Initializer;\n  readonly betaRegularizer: Regularizer;\n  readonly gammaRegularizer: Regularizer;\n\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n\n  constructor(args?: LayerNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(\n            `Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(\n              `Expected axis to be an array of integers, ` +\n              `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(\n          `Expected axis to be an integer or an array of integers, ` +\n          `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n\n    this.supportsMasking = true;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]) as number[];\n\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', paramShape, 'float32', this.gammaInitializer,\n          this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', paramShape, 'float32', this.betaInitializer,\n          this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n\n    return tidy(() => {\n      const keepDims = true;\n      let {mean, variance} = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis as number[]) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = (v: Tensor) => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = broadcast(this.gamma.read());\n      let offset = broadcast(this.beta.read());\n\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling: number[] = [];\n      const scaleOffsetTiling: number[] = [];\n      for (let i = 0; i < nDims; ++i) {\n        if ((this.axis as number[]).indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      scale = tfc.tile(scale, scaleOffsetTiling);\n      offset = tfc.tile(offset, scaleOffsetTiling);\n\n      return batchNormalization(\n          input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LayerNormalization);\n"]},"metadata":{},"sourceType":"module"}