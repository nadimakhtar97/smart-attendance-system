{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class SGDOptimizer extends Optimizer {\n  constructor(learningRate) {\n    super();\n    this.learningRate = learningRate;\n    this.setLearningRate(learningRate);\n  }\n\n  applyGradients(variableGradients) {\n    const varNames = Array.isArray(variableGradients) ? variableGradients.map(v => v.name) : Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n\n\n  setLearningRate(learningRate) {\n    this.learningRate = learningRate;\n\n    if (this.c != null) {\n      this.c.dispose();\n    }\n\n    this.c = keep(scalar(-learningRate));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  async getWeights() {\n    return [await this.saveIterations()];\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate']);\n  }\n\n}\n/** @nocollapse */\n\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\n\nregisterClass(SGDOptimizer);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,WAArB;AACA,SAAQC,IAAR,EAAcC,IAAd,QAAyB,YAAzB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,GAAR,QAAkB,YAAlB;AACA,SAAQC,MAAR,QAAqB,eAArB;AACA,SAAoBC,aAApB,QAA+E,kBAA/E;AAIA,SAAQC,SAAR,QAAwB,aAAxB;AAEA;;AACA,OAAM,MAAOC,YAAP,SAA4BD,SAA5B,CAAqC;AAKzCE,cAAsBC,YAAtB,EAA0C;AACxC;AADoB;AAEpB,SAAKC,eAAL,CAAqBD,YAArB;AACD;;AAEDE,gBAAc,CAACC,iBAAD,EAAgD;AAC5D,UAAMC,QAAQ,GAAGC,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACI,GAAlB,CAAsBC,CAAC,IAAIA,CAAC,CAACC,IAA7B,CADa,GAEbC,MAAM,CAACC,IAAP,CAAYR,iBAAZ,CAFJ;AAGAC,YAAQ,CAACQ,OAAT,CAAiB,CAACH,IAAD,EAAOI,CAAP,KAAY;AAC3B,YAAMC,QAAQ,GAAGT,KAAK,CAACC,OAAN,CAAcH,iBAAd,IACbA,iBAAiB,CAACU,CAAD,CAAjB,CAAqBE,MADR,GAEbZ,iBAAiB,CAACM,IAAD,CAFrB;;AAGA,UAAIK,QAAQ,IAAI,IAAhB,EAAsB;AACpB;AACD;;AACD,YAAME,KAAK,GAAG1B,MAAM,CAAC2B,mBAAP,CAA2BR,IAA3B,CAAd;AACAjB,UAAI,CAAC,MAAK;AACR,cAAM0B,QAAQ,GAAGzB,GAAG,CAACC,GAAG,CAAC,KAAKyB,CAAN,EAASL,QAAT,CAAJ,EAAwBE,KAAxB,CAApB;AACAA,aAAK,CAACI,MAAN,CAAaF,QAAb;AACD,OAHG,CAAJ;AAID,KAZD;AAaA,SAAKG,mBAAL;AACD;AAED;;;;;AAGApB,iBAAe,CAACD,YAAD,EAAqB;AAClC,SAAKA,YAAL,GAAoBA,YAApB;;AACA,QAAI,KAAKmB,CAAL,IAAU,IAAd,EAAoB;AAClB,WAAKA,CAAL,CAAOG,OAAP;AACD;;AACD,SAAKH,CAAL,GAAS5B,IAAI,CAACI,MAAM,CAAC,CAACK,YAAF,CAAP,CAAb;AACD;;AAEDsB,SAAO;AACL,SAAKH,CAAL,CAAOG,OAAP;AACD;;AAEe,QAAVC,UAAU;AACd,WAAO,CAAC,MAAM,KAAKC,cAAL,EAAP,CAAP;AACD;;AAEe,QAAVC,UAAU,CAACC,YAAD,EAA4B;AAC1CA,gBAAY,GAAG,MAAM,KAAKC,iBAAL,CAAuBD,YAAvB,CAArB;;AACA,QAAIA,YAAY,CAACE,MAAb,KAAwB,CAA5B,EAA+B;AAC7B,YAAM,IAAIC,KAAJ,CAAU,+CAAV,CAAN;AACD;AACF;;AAEDC,WAAS;AACP,WAAO;AAAC,sBAAgB,KAAK9B;AAAtB,KAAP;AACD;AAED;;;AACiB,SAAV+B,UAAU,CACbC,GADa,EACoBC,MADpB,EACsC;AACrD,WAAO,IAAID,GAAJ,CAAQC,MAAM,CAAC,cAAD,CAAd,CAAP;AACD;;AAhEwC;AACzC;;AACOnC,yBAAY,KAAZ,C,CAAoB;;AAgE7BF,aAAa,CAACE,YAAD,CAAb","names":["ENGINE","keep","tidy","add","mul","scalar","registerClass","Optimizer","SGDOptimizer","constructor","learningRate","setLearningRate","applyGradients","variableGradients","varNames","Array","isArray","map","v","name","Object","keys","forEach","i","gradient","tensor","value","registeredVariables","newValue","c","assign","incrementIterations","dispose","getWeights","saveIterations","setWeights","weightValues","extractIterations","length","Error","getConfig","fromConfig","cls","config"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/optimizers/sgd_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {keep, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {ConfigDict, registerClass, Serializable, SerializableConstructor} from '../serialization';\nimport {Scalar} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\nimport {Optimizer} from './optimizer';\n\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n  /** @nocollapse */\n  static className = 'SGD';  // Note: Name matters for Python compatibility.\n  protected c: Scalar;\n\n  constructor(protected learningRate: number) {\n    super();\n    this.setLearningRate(learningRate);\n  }\n\n  applyGradients(variableGradients: NamedTensorMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n  setLearningRate(learningRate: number) {\n    this.learningRate = learningRate;\n    if (this.c != null) {\n      this.c.dispose();\n    }\n    this.c = keep(scalar(-learningRate));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    return [await this.saveIterations()];\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n\n  getConfig(): ConfigDict {\n    return {'learningRate': this.learningRate};\n  }\n\n  /** @nocollapse */\n  static fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate']);\n  }\n}\nregisterClass(SGDOptimizer);\n"]},"metadata":{},"sourceType":"module"}