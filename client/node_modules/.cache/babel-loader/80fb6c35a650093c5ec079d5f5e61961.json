{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\n\nfunction fusedConv2d_(_ref) {\n  let {\n    x,\n    filter,\n    strides,\n    pad,\n    dataFormat = 'NHWC',\n    dilations = [1, 1],\n    dimRoundingMode,\n    bias,\n    activation = 'linear',\n    preluActivationWeights,\n    leakyreluAlpha\n  } = _ref;\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` + `${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` + `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` + `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' + `Got strides ${strides} and dilations '${dilations}'`);\n  util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy, saved) => {\n    const [$filter, x4D, y, $bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' + `dilation rates greater than 1 ` + `are not yet supported in gradients. Got dilations '${dilations}'`);\n    const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n\n    return der;\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      let res = // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport const conv2d = op({\n  fusedConv2d_\n});","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAR,QAAqB,cAArB;AACA,SAAQC,UAAR,QAAyB,iBAAzB;AACA,SAAQC,WAAR,QAA+D,oBAA/D;AAIA,SAAQC,cAAR,QAA6B,mBAA7B;AACA,SAAQC,eAAR,QAA8B,uBAA9B;AAEA,OAAO,KAAKC,IAAZ,MAAsB,YAAtB;AACA,SAAQC,GAAR,QAAkB,QAAlB;AACA,OAAO,KAAKC,cAAZ,MAAgC,mBAAhC;AACA,SAAQC,MAAM,IAAIC,aAAlB,QAAsC,WAAtC;AACA,SAAQC,oBAAR,QAAmC,2BAAnC;AACA,SAAQC,mBAAR,QAAkC,0BAAlC;AACA,OAAO,KAAKC,SAAZ,MAA2B,cAA3B;AAEA,SAAQC,eAAR,EAAyBC,oBAAzB,EAA+CC,oBAA/C,EAAqEC,UAArE,QAAsF,eAAtF;AACA,SAAQC,EAAR,QAAiB,cAAjB;AACA,SAAQC,OAAR,QAAsB,YAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyDA,SAASC,YAAT,OAwBC;AAAA,MAxBkD;AACjDC,KADiD;AAEjDC,UAFiD;AAGjDC,WAHiD;AAIjDC,OAJiD;AAKjDC,cAAU,GAAG,MALoC;AAMjDC,aAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CANqC;AAOjDC,mBAPiD;AAQjDC,QARiD;AASjDC,cAAU,GAAG,QAToC;AAUjDC,0BAViD;AAWjDC;AAXiD,GAwBlD;AACCF,YAAU,GAAGA,UAAU,IAAI,QAA3B;;AAEA,MAAIZ,UAAU,CAAChB,MAAM,CAAC+B,KAAP,CAAaC,aAAd,EAA6BJ,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAIK,MAAM,GAAGxB,aAAa,CACtBW,CADsB,EACnBC,MADmB,EACXC,OADW,EACFC,GADE,EACGC,UADH,EACeC,SADf,EAC0BC,eAD1B,CAA1B;;AAEA,QAAIC,IAAI,IAAI,IAAZ,EAAkB;AAChBM,YAAM,GAAG3B,GAAG,CAAC2B,MAAD,EAASN,IAAT,CAAZ;AACD;;AAED,WAAOd,eAAe,CACXoB,MADW,EACHL,UADG,EACSC,sBADT,EACiCC,cADjC,CAAtB;AAED;;AAED,QAAMI,EAAE,GAAG9B,eAAe,CAACgB,CAAD,EAAI,GAAJ,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACA,QAAMe,OAAO,GAAG/B,eAAe,CAACiB,MAAD,EAAS,QAAT,EAAmB,QAAnB,EAA6B,SAA7B,CAA/B;AAEA,MAAIe,GAAG,GAAGF,EAAV;AACA,MAAIG,YAAY,GAAG,KAAnB;;AAEA,MAAIH,EAAE,CAACI,IAAH,KAAY,CAAhB,EAAmB;AACjBD,gBAAY,GAAG,IAAf;AACAD,OAAG,GAAGlB,OAAO,CAACgB,EAAD,EAAK,CAAC,CAAD,EAAIA,EAAE,CAACK,KAAH,CAAS,CAAT,CAAJ,EAAiBL,EAAE,CAACK,KAAH,CAAS,CAAT,CAAjB,EAA8BL,EAAE,CAACK,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;AACD;;AACDlC,MAAI,CAACmC,MAAL,CACIJ,GAAG,CAACE,IAAJ,KAAa,CADjB,EAEI,MAAM,+DACF,GAAGF,GAAG,CAACE,IAAI,GAHnB;AAIAjC,MAAI,CAACmC,MAAL,CACIL,OAAO,CAACG,IAAR,KAAiB,CADrB,EAEI,MAAM,gEACF,GAAGH,OAAO,CAACG,IAAI,GAHvB;AAIA1B,WAAS,CAAC6B,yBAAV,CAAoC,cAApC,EAAoDlB,GAApD,EAAyDG,eAAzD;AACArB,MAAI,CAACmC,MAAL,CACIJ,GAAG,CAACG,KAAJ,CAAU,CAAV,MAAiBJ,OAAO,CAACI,KAAR,CAAc,CAAd,CADrB,EAEI,MAAM,oCAAoCH,GAAG,CAACG,KAAJ,CAAU,CAAV,CAAY,eAAhD,GACF,0BAA0BJ,OAAO,CAACI,KAAR,CAAc,CAAd,CAAgB,GAHlD;AAIAlC,MAAI,CAACmC,MAAL,CACI5B,SAAS,CAAC8B,8BAAV,CAAyCpB,OAAzC,EAAkDG,SAAlD,CADJ,EAEI,MAAM,6DACF,eAAeH,OAAO,mBAAmBG,SAAS,GAH1D;AAIApB,MAAI,CAACmC,MAAL,CACIhB,UAAU,KAAK,MADnB,EAEI,MAAM,sCACFA,UAAU,wCAHlB;AAKA,QAAMmB,QAAQ,GAAG/B,SAAS,CAACgC,iBAAV,CACbR,GAAG,CAACG,KADS,EACFJ,OAAO,CAACI,KADN,EACajB,OADb,EACsBG,SADtB,EACiCF,GADjC,EACsCG,eADtC,CAAjB;AAGA,MAAImB,KAAJ;;AACA,MAAIlB,IAAI,IAAI,IAAZ,EAAkB;AAChBkB,SAAK,GAAGzC,eAAe,CAACuB,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAACkB,KAAD,IAAU1C,cAAc,CAAC0C,KAAD,EAAQX,EAAR,CAAxB;AAEA3B,kBAAc,CAACuC,0BAAf,CAA0CH,QAAQ,CAACI,QAAnD,EAA6DF,KAAK,CAACN,KAAnE;AACD;;AAED,MAAIS,uBAAJ;;AACA,MAAInB,sBAAsB,IAAI,IAA9B,EAAoC;AAClCmB,2BAAuB,GAAG5C,eAAe,CACrCyB,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAMoB,IAAI,GAAG,CAACC,EAAD,EAAeC,KAAf,KAAkC;AAC7C,UAAM,CAAChB,OAAD,EAAUC,GAAV,EAAegB,CAAf,EAAkBP,KAAlB,IACFM,KADJ;AAGA,UAAME,YAAY,GAAGtC,oBAAoB,CAACmC,EAAD,EAAKE,CAAL,EAAQxB,UAAR,CAAzC;AAEAvB,QAAI,CAACmC,MAAL,CACI5B,SAAS,CAAC0C,iBAAV,CAA4B7B,SAA5B,CADJ,EAEI,MAAM,wCACF,gCADE,GAEF,sDAAsDA,SAAS,GAJvE;AAMA,UAAM8B,IAAI,GACN5C,mBAAmB,CAACyB,GAAG,CAACG,KAAL,EAAYc,YAAZ,EAA0BlB,OAA1B,EAAmCb,OAAnC,EAA4CC,GAA5C,CADvB;AAEA,UAAMiC,SAAS,GACX9C,oBAAoB,CAAC0B,GAAD,EAAMiB,YAAN,EAAoBlB,OAAO,CAACI,KAA5B,EAAmCjB,OAAnC,EAA4CC,GAA5C,CADxB;AAEA,UAAMkC,GAAG,GAAa,CAACF,IAAD,EAAOC,SAAP,CAAtB;;AAEA,QAAIX,KAAK,IAAI,IAAb,EAAmB;AACjB,YAAMa,OAAO,GAAG5C,oBAAoB,CAAC+B,KAAD,EAAQQ,YAAR,CAApC;AACAI,SAAG,CAACE,IAAJ,CAASD,OAAT;AACD;;AACD,WAAOD,GAAP;AACD,GAvBD;;AAyBA,QAAMG,MAAM,GAAsB;AAChCxC,KAAC,EAAEgB,GAD6B;AAEhCf,UAAM,EAAEc,OAFwB;AAGhCR,QAAI,EAAEkB,KAH0B;AAIhChB,0BAAsB,EAAEmB;AAJQ,GAAlC;AAOA,QAAMa,KAAK,GAAqB;AAC9BvC,WAD8B;AAE9BC,OAF8B;AAG9BC,cAH8B;AAI9BC,aAJ8B;AAK9BC,mBAL8B;AAM9BE,cAN8B;AAO9BE;AAP8B,GAAhC,CA/FD,CAyGC;AACA;;AACA,MAAIH,IAAI,IAAI,IAAZ,EAAkB;AAChB,UAAMmC,QAAQ,GACV7D,UAAU,CAAC,CAACmC,GAAD,EAAgBf,MAAhB,EAAkC0C,IAAlC,KAAwD;AACjE,UAAIC,GAAG,GACH;AACAhE,YAAM,CAACiE,SAAP,CACI/D,WADJ,EACiB0D,MADjB,EAEIC,KAFJ,CAFJ;AAMAE,UAAI,CAAC,CAAC1C,MAAD,EAASe,GAAT,EAAc4B,GAAd,CAAD,CAAJ;;AAEA,UAAI3B,YAAJ,EAAkB;AAChB;AACA2B,WAAG,GAAG9C,OAAO,CAAC8C,GAAD,EAAM,CAACA,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAAD,EAAeyB,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAAf,EAA6ByB,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;AAED;;AAED,aAAO;AAAC2B,aAAK,EAAEF,GAAR;AAAaG,gBAAQ,EAAElB;AAAvB,OAAP;AACD,KAhBS,CADd;AAkBA,WAAOa,QAAQ,CAAC1B,GAAD,EAAMD,OAAN,CAAf;AACD,GApBD,MAoBO;AACL,UAAMiC,gBAAgB,GAAGnE,UAAU,CAC/B,CAACmC,GAAD,EAAgBf,MAAhB,EAAkCM,IAAlC,EAAgDoC,IAAhD,KAAsE;AACpE,UAAIC,GAAG,GAAsBhE,MAAM,CAACiE,SAAP,CACzB/D,WADyB,EACZ0D,MADY,EAEzBC,KAFyB,CAA7B;AAIAE,UAAI,CAAC,CAAC1C,MAAD,EAASe,GAAT,EAAc4B,GAAd,EAAmBrC,IAAnB,CAAD,CAAJ;;AAEA,UAAIU,YAAJ,EAAkB;AAChB;AACA2B,WAAG,GAAG9C,OAAO,CAAC8C,GAAD,EAAM,CAACA,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAAD,EAAeyB,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAAf,EAA6ByB,GAAG,CAACzB,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;AAED;;AAED,aAAO;AAAC2B,aAAK,EAAEF,GAAR;AAAaG,gBAAQ,EAAElB;AAAvB,OAAP;AACD,KAf8B,CAAnC;AAiBA,WAAOmB,gBAAgB,CAAChC,GAAD,EAAMD,OAAN,EAAeU,KAAf,CAAvB;AACD;AACF;;AACD,OAAO,MAAMrC,MAAM,GAAGS,EAAE,CAAC;AAACE;AAAD,CAAD,CAAjB","names":["ENGINE","customGrad","FusedConv2D","makeTypesMatch","convertToTensor","util","add","broadcast_util","conv2d","unfusedConv2d","conv2DBackpropFilter","conv2DBackpropInput","conv_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","op","reshape","fusedConv2d_","x","filter","strides","pad","dataFormat","dilations","dimRoundingMode","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","$x","$filter","x4D","reshapedTo4D","rank","shape","assert","checkPadOnDimRoundingMode","eitherStridesOrDilationsAreOne","convInfo","computeConv2DInfo","$bias","assertAndGetBroadcastShape","outShape","$preluActivationWeights","grad","dy","saved","y","dyActivation","tupleValuesAreOne","xDer","filterDer","der","biasDer","push","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-core/src/ops/fused/conv2d.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedConv2D, FusedConv2DAttrs, FusedConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {conv2d as unfusedConv2d} from '../conv2d';\nimport {conv2DBackpropFilter} from '../conv2d_backprop_filter';\nimport {conv2DBackpropInput} from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number|conv_util.ExplicitPadding,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n          `${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n          `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  util.assert(\n      x4D.shape[3] === $filter.shape[2],\n      () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n          `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n          `Got strides ${strides} and dilations '${dilations}'`);\n  util.assert(\n      dataFormat === 'NHWC',\n      () => `Error in conv2d: got dataFormat of ${\n          dataFormat} but only NHWC is currently supported.`);\n\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    const [$filter, x4D, y, $bias] =\n        saved as [Tensor4D, Tensor4D, Tensor4D, Tensor];\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n\n    const xDer =\n        conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer =\n        conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der: Tensor[] = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n    return der;\n  };\n\n  const inputs: FusedConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n\n  const attrs: FusedConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D =\n              // tslint:disable-next-line: no-unnecessary-type-assertion\n              ENGINE.runKernel(\n                  FusedConv2D, inputs as {} as NamedTensorMap,\n                  attrs as {} as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedConv2D, inputs as {} as NamedTensorMap,\n              attrs as {} as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const conv2d = op({fusedConv2d_});\n"]},"metadata":{},"sourceType":"module"}