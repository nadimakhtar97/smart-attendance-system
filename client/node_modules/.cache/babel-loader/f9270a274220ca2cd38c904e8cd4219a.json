{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      stddev: this.stddev\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n\n      const noised = () => add(K.randomNormal(input.shape, 0, this.stddev), input);\n\n      const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n\n}\n/** @nocollapse */\n\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\n        };\n\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n\n      return input;\n    });\n  }\n\n}\n/** @nocollapse */\n\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\n\nexport class AlphaDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n\n  _getNoiseShape(inputs) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n          const alphaP = -alpha * scale;\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n          keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n          // Get affine transformation params.\n\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate; // Apply mask.\n\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n          return add(mul(x, a), b);\n        };\n\n        return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n      }\n\n      return inputs;\n    });\n  }\n\n}\n/** @nocollapse */\n\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"mappings":"AAAA;;;;;;;;;;AAUA;;;AAIA,SAAQA,GAAR,EAAaC,YAAb,EAA2BC,GAA3B,EAAgCC,aAAhC,EAA+CC,aAA/C,EAAsEC,IAAtE,QAAiF,uBAAjF;AAEA,OAAO,KAAKC,CAAZ,MAAmB,yBAAnB;AACA,SAAQC,KAAR,QAA+B,oBAA/B;AAGA,SAAQC,mBAAR,QAAkC,sBAAlC;AAOA,OAAM,MAAOC,aAAP,SAA6BF,KAA7B,CAAkC;AAKtCG,cAAYC,IAAZ,EAAmC;AACjC,UAAMA,IAAN;AACA,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKC,MAAL,GAAcF,IAAI,CAACE,MAAnB;AACD;;AAEDC,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AACA,UAAME,MAAM,GAAG;AAACL,YAAM,EAAE,KAAKA;AAAd,KAAf;AACAM,UAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;AACA,WAAOC,MAAP;AACD;;AAEDG,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAOlB,IAAI,CAAC,MAAK;AACf,WAAKmB,cAAL,CAAoBF,MAApB,EAA4BC,MAA5B;AACA,YAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;;AACA,YAAMI,MAAM,GAAG,MACX1B,GAAG,CAACM,CAAC,CAACqB,YAAF,CAAeF,KAAK,CAACG,KAArB,EAA4B,CAA5B,EAA+B,KAAKf,MAApC,CAAD,EAA8CY,KAA9C,CADP;;AAEA,YAAMI,MAAM,GACRvB,CAAC,CAACwB,YAAF,CAAeJ,MAAf,EAAuB,MAAMD,KAA7B,EAAoCF,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CADJ;AAEA,aAAOM,MAAP;AACD,KARU,CAAX;AASD;;AAhCqC;AACtC;;AACOpB,0BAAY,eAAZ;AAgCTL,aAAa,CAAC2B,aAAd,CAA4BtB,aAA5B;AAOA,OAAM,MAAOuB,eAAP,SAA+BzB,KAA/B,CAAoC;AAKxCG,cAAYC,IAAZ,EAAqC;AACnC,UAAMA,IAAN;AACA,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKqB,IAAL,GAAYtB,IAAI,CAACsB,IAAjB;AACD;;AAEDnB,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AACA,UAAME,MAAM,GAAG;AAACe,UAAI,EAAE,KAAKA;AAAZ,KAAf;AACAd,UAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;AACA,WAAOC,MAAP;AACD;;AAEDG,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAOlB,IAAI,CAAC,MAAK;AACf,WAAKmB,cAAL,CAAoBF,MAApB,EAA4BC,MAA5B;AACA,YAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;;AACA,UAAI,KAAKW,IAAL,GAAY,CAAZ,IAAiB,KAAKA,IAAL,GAAY,CAAjC,EAAoC;AAClC,cAAMP,MAAM,GAAG,MAAK;AAClB,gBAAMb,MAAM,GAAGqB,IAAI,CAACC,IAAL,CAAU,KAAKF,IAAL,IAAa,IAAI,KAAKA,IAAtB,CAAV,CAAf;AACA,iBAAO/B,GAAG,CAACuB,KAAD,EAAQnB,CAAC,CAACqB,YAAF,CAAeF,KAAK,CAACG,KAArB,EAA4B,CAA5B,EAA+Bf,MAA/B,CAAR,CAAV;AACD,SAHD;;AAIA,eAAOP,CAAC,CAACwB,YAAF,CAAeJ,MAAf,EAAuB,MAAMD,KAA7B,EAAoCF,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CAAP;AACD;;AACD,aAAOE,KAAP;AACD,KAXU,CAAX;AAYD;;AAnCuC;AACxC;;AACOO,4BAAY,iBAAZ;AAmCT5B,aAAa,CAAC2B,aAAd,CAA4BC,eAA5B;AAYA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BA,OAAM,MAAOI,YAAP,SAA4B7B,KAA5B,CAAiC;AAMrCG,cAAYC,IAAZ,EAAkC;AAChC,UAAMA,IAAN;AACA,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKqB,IAAL,GAAYtB,IAAI,CAACsB,IAAjB;AACA,SAAKI,UAAL,GAAkB1B,IAAI,CAAC0B,UAAvB;AACD;;AAEDC,gBAAc,CAAChB,MAAD,EAAwB;AACpC,WAAO,KAAKe,UAAL,IAAmB7B,mBAAmB,CAACc,MAAD,CAAnB,CAA4BM,KAAtD;AACD;;AAEDd,oBAAkB,CAACC,UAAD,EAA0B;AAC1C,WAAOA,UAAP;AACD;;AAEDC,WAAS;AACP,UAAMC,UAAU,GAAG,MAAMD,SAAN,EAAnB;AACA,UAAME,MAAM,GAAG;AAACe,UAAI,EAAE,KAAKA;AAAZ,KAAf;AACAd,UAAM,CAACC,MAAP,CAAcF,MAAd,EAAsBD,UAAtB;AACA,WAAOC,MAAP;AACD;;AAEDG,MAAI,CAACC,MAAD,EAA0BC,MAA1B,EAAwC;AAC1C,WAAOlB,IAAI,CAAC,MAAK;AACf,UAAI,KAAK4B,IAAL,GAAY,CAAZ,IAAiB,KAAKA,IAAL,GAAY,CAAjC,EAAoC;AAClC,cAAMI,UAAU,GAAG,KAAKC,cAAL,CAAoBhB,MAApB,CAAnB;;AAEA,cAAMiB,aAAa,GAAG,MAAK;AACzB,gBAAMd,KAAK,GAAGjB,mBAAmB,CAACc,MAAD,CAAjC;AAEA,gBAAMkB,KAAK,GAAG,iCAAd;AACA,gBAAMC,KAAK,GAAG,iCAAd;AAEA,gBAAMC,MAAM,GAAG,CAACF,KAAD,GAASC,KAAxB;AAEA,cAAIE,OAAO,GAAG1C,YAAY,CAACE,aAAa,CAACkC,UAAD,CAAd,EAA4B,KAAKJ,IAAjC,CAA1B;AAEAU,iBAAO,GAAGrC,CAAC,CAACsC,IAAF,CAAOD,OAAP,EAAgB,SAAhB,CAAV,CAVyB,CAUc;AAEvC;;AACA,gBAAME,CAAC,GAAG,CAAC,CAAC,IAAI,KAAKZ,IAAV,KAAmB,IAAI,KAAKA,IAAL,GAAYS,MAAM,IAAI,CAA7C,CAAD,KAAqD,CAAC,GAAhE;AACA,gBAAMI,CAAC,GAAG,CAACD,CAAD,GAAKH,MAAL,GAAc,KAAKT,IAA7B,CAdyB,CAgBzB;;AACA,gBAAMc,CAAC,GAAG/C,GAAG,CAACE,GAAG,CAACuB,KAAD,EAAQkB,OAAR,CAAJ,EAAsBzC,GAAG,CAACF,GAAG,CAAC2C,OAAD,EAAU,CAAC,CAAX,CAAJ,EAAmBD,MAAnB,CAAzB,CAAb;AAEA,iBAAO1C,GAAG,CAACE,GAAG,CAAC6C,CAAD,EAAIF,CAAJ,CAAJ,EAAYC,CAAZ,CAAV;AACD,SApBD;;AAqBA,eAAOxC,CAAC,CAACwB,YAAF,CACHS,aADG,EACY,MAAM/B,mBAAmB,CAACc,MAAD,CADrC,EAEHC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAFnB,CAAP;AAGD;;AACD,aAAOD,MAAP;AACD,KA9BU,CAAX;AA+BD;;AA5DoC;AACrC;;AACOc,yBAAY,cAAZ;AA4DThC,aAAa,CAAC2B,aAAd,CAA4BK,YAA5B","names":["add","greaterEqual","mul","randomUniform","serialization","tidy","K","Layer","getExactlyOneTensor","GaussianNoise","constructor","args","supportsMasking","stddev","computeOutputShape","inputShape","getConfig","baseConfig","config","Object","assign","call","inputs","kwargs","invokeCallHook","input","noised","randomNormal","shape","output","inTrainPhase","registerClass","GaussianDropout","rate","Math","sqrt","AlphaDropout","noiseShape","_getNoiseShape","droppedInputs","alpha","scale","alphaP","keptIdx","cast","a","b","x"],"sources":["/home/nadimakhtar97/smart-attendance-system/tfjs-layers/src/layers/noise.ts"],"sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\r\n\r\n/**\r\n * TensorFlow.js Layers: Noise Layers.\r\n */\r\n\r\nimport {add, greaterEqual, mul, randomUniform, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\r\n\r\nimport * as K from '../backend/tfjs_backend';\r\nimport {Layer, LayerArgs} from '../engine/topology';\r\nimport {Shape} from '../keras_format/common';\r\nimport {Kwargs} from '../types';\r\nimport {getExactlyOneTensor} from '../utils/types_utils';\r\n\r\nexport declare interface GaussianNoiseArgs extends LayerArgs {\r\n  /** Standard Deviation.  */\r\n  stddev: number;\r\n}\r\n\r\nexport class GaussianNoise extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianNoise';\r\n  readonly stddev: number;\r\n\r\n  constructor(args: GaussianNoiseArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.stddev = args.stddev;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {stddev: this.stddev};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      const noised = () =>\r\n          add(K.randomNormal(input.shape, 0, this.stddev), input);\r\n      const output =\r\n          K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      return output;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianNoise);\r\n\r\nexport declare interface GaussianDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n}\r\n\r\nexport class GaussianDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianDropout';\r\n  readonly rate: number;\r\n\r\n  constructor(args: GaussianDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      if (this.rate > 0 && this.rate < 1) {\r\n        const noised = () => {\r\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\r\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\r\n        };\r\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      }\r\n      return input;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianDropout);\r\n\r\nexport declare interface AlphaDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n  /**\r\n   * A 1-D `Tensor` of type `int32`, representing the\r\n   * shape for randomly generated keep/drop flags.\r\n   */\r\n  noiseShape?: Shape;\r\n}\r\n\r\n/**\r\n * Applies Alpha Dropout to the input.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\r\n * to their original values, in order to ensure the self-normalizing property\r\n * even after this dropout.\r\n * Alpha Dropout fits well to Scaled Exponential Linear Units\r\n * by randomly setting activations to the negative saturation value.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\r\n *     shape for randomly generated keep/drop flags.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n */\r\nexport class AlphaDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'AlphaDropout';\r\n  readonly rate: number;\r\n  readonly noiseShape: Shape;\r\n\r\n  constructor(args: AlphaDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n    this.noiseShape = args.noiseShape;\r\n  }\r\n\r\n  _getNoiseShape(inputs: Tensor|Tensor[]) {\r\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      if (this.rate < 1 && this.rate > 0) {\r\n        const noiseShape = this._getNoiseShape(inputs);\r\n\r\n        const droppedInputs = () => {\r\n          const input = getExactlyOneTensor(inputs);\r\n\r\n          const alpha = 1.6732632423543772848170429916717;\r\n          const scale = 1.0507009873554804934193349852946;\r\n\r\n          const alphaP = -alpha * scale;\r\n\r\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\r\n\r\n          keptIdx = K.cast(keptIdx, 'float32');  // get default dtype.\r\n\r\n          // Get affine transformation params.\r\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\r\n          const b = -a * alphaP * this.rate;\r\n\r\n          // Apply mask.\r\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\r\n\r\n          return add(mul(x, a), b);\r\n        };\r\n        return K.inTrainPhase(\r\n            droppedInputs, () => getExactlyOneTensor(inputs),\r\n            kwargs['training'] || false);\r\n      }\r\n      return inputs;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(AlphaDropout);\r\n"]},"metadata":{},"sourceType":"module"}